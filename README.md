# ğŸ“š LLMs vs. SweSAT â€“ Andon Labs Hackathon 2025
This repo contains our work from the [Andon Labs Hackathon at LinkÃ¶ping University](https://www.facebook.com/events/947993350302617/), where we evaluated LLMs on SweSAT (HÃ¶gskoleprovet) question types.

## ğŸ§  Models Evaluated
- `openai/gpt-4o-mini`
- `openai/gpt-4o`
- `anthropic/claude-3-5-haiku-latest`
- `anthropic/claude-3-5-sonnet-latest`
- `o1-mini`

## ğŸ“„ Dataset
Questions sourced from:  ğŸ‘‰ [github.com/ViktorAlm/HP](https://github.com/ViktorAlm/HP)

Covers:
- Reading Comprehension (RC) â€“ SV & EN  
- Sentence Completion (MEK) â€“ SV & EN  
- Vocabulary (Words) â€“ SV only

## ğŸ“Š Results
**Detailed Performance**
![Model Performance](assets/performance.png)

**Aggregated Accuracy**
![Aggregated Results](assets/aggregated.png)

## ğŸ† Findings
- GPT-4o and Claude 3.5 Sonnet consistently outperformed others.
- LLMs are nearing human-level performance on standardized tests.
- Prompt quality had a notable impact on accuracy.

## ğŸ“Œ Summary
Can an LLM pass the SweSAT? For some question types, yesâ€”especially with the right prompt and a top-tier model.
