[
    {
        "question": "What are we told in relation to SheTaxis?",
        "options": [
            "It is about time women should be given a chance to drive yellow cabs. ",
            "Its drivers have been treated with scepticism by their male colleagues.",
            "It is doubtful whether male passengers will accept female cab drivers.",
            "Its business idea has met with unexpected success among passengers. "
        ],
        "article": "Women-only Cabs Women have been driving yellow cabs in New York since the 1940s, but 99% of drivers are male. Even among drivers of cars booked by phone or online, only 4% are women. That may change with the launch of SheTaxis, an app that lets female passengers insist on female drivers, and vice versa. But SheTaxis faces two speed bumps. One is practical. Demand has been so great that the firm has had to decelerate its launch until it can recruit 500 drivers. The other obstacle is legal. By employing only female drivers, SheTaxis is obviously discriminating against men. Since anti-discrimination law is not always applied with common sense, that may be illegal.",
        "id": 31,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What does the text imply about the writer’s attitude to SheTaxis? The writer …",
        "options": [
            "is concerned about current prejudice against New York’s female drivers.",
            "seems to think that the firm’s policy conforms to the spirit of the law. ",
            "is upset about its discriminating effect on the majority of cab drivers.",
            "believes that the law will be changed in favour of the new taxi company."
        ],
        "article": "Women-only Cabs Women have been driving yellow cabs in New York since the 1940s, but 99% of drivers are male. Even among drivers of cars booked by phone or online, only 4% are women. That may change with the launch of SheTaxis, an app that lets female passengers insist on female drivers, and vice versa. But SheTaxis faces two speed bumps. One is practical. Demand has been so great that the firm has had to decelerate its launch until it can recruit 500 drivers. The other obstacle is legal. By employing only female drivers, SheTaxis is obviously discriminating against men. Since anti-discrimination law is not always applied with common sense, that may be illegal.",
        "id": 32,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What is implied in relation to religious belief in the first two paragraphs?",
        "options": [
            "Atheists are on their way to outnumbering reli gious people in America.",
            "Research indicates that religiosity is the single most important factor with regard to well-being. ",
            "Religious faith as a social force is losing ground in most parts of the world.",
            "Fewer people than before in the U.S. now consider themselves actively involved in religion. "
        ],
        "article": "Religion and Well-being A large body of research suggests that, as compared with religious individuals, people who lack a creed are less likely to be happy and healthy – surely the two most important earthly concerns – and tend to lose out on at least seven years of life, some estimates suggest. Several large-scale population studies have reinforced a single premise: the more you engage with religious activities, the better off you are. Yet many people are less certain about their beliefs than ever before. Non-believers number between 500 million and 750 million worldwide, according to one analysis. In the past two decades, the percentage of the U.S. population that proclaims its religious affiliation as “none” has more than doubled, to about 15 percent. Many of these individuals do not identify as atheists; even so, many of them conduct their life outside the religious establishment. Although research on this population is only now emerging, a recent study found that people who were less committed to their religious creed were actually less happy than avowed atheists. If the shifting beliefs of a growing number of people portend lower health and a less contented outlook, knowing exactly how religion benefits its followers becomes a matter of public concern. Several recent studies have sought to answer this question by examining it with greater resolution. For the first time, researchers have collected a representative sample of the world that measures religiosity and happiness. They found that the positive effects of religion depend enormously on where you live. Religious people may be happier than their godless counterparts, but only if the society they belong to values religion highly, which not all societies do. For atheists and the growing ranks of unaffiliated individuals, these findings bode well. Although many questions remain about how nonbelievers can acquire the health benefits of religion, scientists are now finding that secular communities of like-minded people can offer similar social support. To examine how the U.S., where being religious is the cultural norm, compares with the rest of the world, psychologist Ed Diener of the University of Illinois and his colleagues conducted a global survey, including a state-by-state sample in the U.S. In line with earlier findings, two thirds of those surveyed regarded religion as important to their life. The states, however, varied by a factor of two: in Vermont, 44 percent rated religion as important, compared with 88 percent in Mississippi. In the states where religion was very important, people were much more likely to be living in difficult circumstances. They also had lower subjective well-being than people living in less religious parts of the country. Did religion make them happier, as previous studies had shown? Absolutely, according to the data – but they were worse off than the contented residents of more affluent states, where religion mattered less. Globally, people in 154 countries were surveyed, with an average of 3,000 people per country. Overall, three out of four reported that religion was important in their life. Again, the devil was in the details. The countries in the sample varied in religiosity by a factor of six, ranging from 16 percent of respondents in Sweden agreeing that religion was an important part of their life to 99 percent affirmation in Bangladesh, Egypt, Sri Lanka and Somaliland. In this global poll, a tough environment also tended to coincide with greater national religiosity. If you live in a nation where daily existence is difficult, your life satisfaction is generally lower. In those countries, being more religious appears to grant you a premium on happiness that your less religious neighbours do not enjoy. If the living is easy, however, both nonbelievers and religious people have similar, relatively high subjective well-being. This effect held true for all religions represented in the sample – Buddhism, Christianity, Hinduism and Islam. More important, whether a person was struggling or thriving was nowhere near as predictive of religiosity as a society’s conditions and norms. “Do you have individual choice about what you believe? Yes, but there are strong influences in society – whether everyone around you is religious,” Ed Diener says. “That’s the eye-opener for me, that societal forces led to religiosity rather than individual forces.” For nonbelievers, these are heartening data. They bolster the idea that believers will accrue more psychological benefits only in places that value religion more, and vice versa. It means atheists are not permanently shut off from some fountain of happiness – although they may want to find a like-minded community to live in. Sandra Upson, Scientific American Mind",
        "id": 33,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What is said about recent research addressing the relationship between religion and well-being?",
        "options": [
            "It has so far confirmed the intuitive views on the topic that already existed.",
            "main finding has been the importance of social factors for a correlation to appear. ",
            "Methodological problems have made results more controversial than expected.",
            "It suggests that a secular society is less likely than a religious one to provide social contentment."
        ],
        "article": "Religion and Well-being A large body of research suggests that, as compared with religious individuals, people who lack a creed are less likely to be happy and healthy – surely the two most important earthly concerns – and tend to lose out on at least seven years of life, some estimates suggest. Several large-scale population studies have reinforced a single premise: the more you engage with religious activities, the better off you are. Yet many people are less certain about their beliefs than ever before. Non-believers number between 500 million and 750 million worldwide, according to one analysis. In the past two decades, the percentage of the U.S. population that proclaims its religious affiliation as “none” has more than doubled, to about 15 percent. Many of these individuals do not identify as atheists; even so, many of them conduct their life outside the religious establishment. Although research on this population is only now emerging, a recent study found that people who were less committed to their religious creed were actually less happy than avowed atheists. If the shifting beliefs of a growing number of people portend lower health and a less contented outlook, knowing exactly how religion benefits its followers becomes a matter of public concern. Several recent studies have sought to answer this question by examining it with greater resolution. For the first time, researchers have collected a representative sample of the world that measures religiosity and happiness. They found that the positive effects of religion depend enormously on where you live. Religious people may be happier than their godless counterparts, but only if the society they belong to values religion highly, which not all societies do. For atheists and the growing ranks of unaffiliated individuals, these findings bode well. Although many questions remain about how nonbelievers can acquire the health benefits of religion, scientists are now finding that secular communities of like-minded people can offer similar social support. To examine how the U.S., where being religious is the cultural norm, compares with the rest of the world, psychologist Ed Diener of the University of Illinois and his colleagues conducted a global survey, including a state-by-state sample in the U.S. In line with earlier findings, two thirds of those surveyed regarded religion as important to their life. The states, however, varied by a factor of two: in Vermont, 44 percent rated religion as important, compared with 88 percent in Mississippi. In the states where religion was very important, people were much more likely to be living in difficult circumstances. They also had lower subjective well-being than people living in less religious parts of the country. Did religion make them happier, as previous studies had shown? Absolutely, according to the data – but they were worse off than the contented residents of more affluent states, where religion mattered less. Globally, people in 154 countries were surveyed, with an average of 3,000 people per country. Overall, three out of four reported that religion was important in their life. Again, the devil was in the details. The countries in the sample varied in religiosity by a factor of six, ranging from 16 percent of respondents in Sweden agreeing that religion was an important part of their life to 99 percent affirmation in Bangladesh, Egypt, Sri Lanka and Somaliland. In this global poll, a tough environment also tended to coincide with greater national religiosity. If you live in a nation where daily existence is difficult, your life satisfaction is generally lower. In those countries, being more religious appears to grant you a premium on happiness that your less religious neighbours do not enjoy. If the living is easy, however, both nonbelievers and religious people have similar, relatively high subjective well-being. This effect held true for all religions represented in the sample – Buddhism, Christianity, Hinduism and Islam. More important, whether a person was struggling or thriving was nowhere near as predictive of religiosity as a society’s conditions and norms. “Do you have individual choice about what you believe? Yes, but there are strong influences in society – whether everyone around you is religious,” Ed Diener says. “That’s the eye-opener for me, that societal forces led to religiosity rather than individual forces.” For nonbelievers, these are heartening data. They bolster the idea that believers will accrue more psychological benefits only in places that value religion more, and vice versa. It means atheists are not permanently shut off from some fountain of happiness – although they may want to find a like-minded community to live in. Sandra Upson, Scientific American Mind",
        "id": 34,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What, according to Ed Diener’s research, are the relations between religiosity, well being and relative wealth in the United States?",
        "options": [
            "People in relatively wealthy states are less religious but more contented than religious people in poorer states.",
            "Religious people are in general more contented than non-religious people, regardless of material status",
            "People in relatively poor states are both more religious and more contented than less religious people in richer states",
            "Non-religious people are in general more contented than religious people, regardless of material status"
        ],
        "article": "Religion and Well-being A large body of research suggests that, as compared with religious individuals, people who lack a creed are less likely to be happy and healthy – surely the two most important earthly concerns – and tend to lose out on at least seven years of life, some estimates suggest. Several large-scale population studies have reinforced a single premise: the more you engage with religious activities, the better off you are. Yet many people are less certain about their beliefs than ever before. Non-believers number between 500 million and 750 million worldwide, according to one analysis. In the past two decades, the percentage of the U.S. population that proclaims its religious affiliation as “none” has more than doubled, to about 15 percent. Many of these individuals do not identify as atheists; even so, many of them conduct their life outside the religious establishment. Although research on this population is only now emerging, a recent study found that people who were less committed to their religious creed were actually less happy than avowed atheists. If the shifting beliefs of a growing number of people portend lower health and a less contented outlook, knowing exactly how religion benefits its followers becomes a matter of public concern. Several recent studies have sought to answer this question by examining it with greater resolution. For the first time, researchers have collected a representative sample of the world that measures religiosity and happiness. They found that the positive effects of religion depend enormously on where you live. Religious people may be happier than their godless counterparts, but only if the society they belong to values religion highly, which not all societies do. For atheists and the growing ranks of unaffiliated individuals, these findings bode well. Although many questions remain about how nonbelievers can acquire the health benefits of religion, scientists are now finding that secular communities of like-minded people can offer similar social support. To examine how the U.S., where being religious is the cultural norm, compares with the rest of the world, psychologist Ed Diener of the University of Illinois and his colleagues conducted a global survey, including a state-by-state sample in the U.S. In line with earlier findings, two thirds of those surveyed regarded religion as important to their life. The states, however, varied by a factor of two: in Vermont, 44 percent rated religion as important, compared with 88 percent in Mississippi. In the states where religion was very important, people were much more likely to be living in difficult circumstances. They also had lower subjective well-being than people living in less religious parts of the country. Did religion make them happier, as previous studies had shown? Absolutely, according to the data – but they were worse off than the contented residents of more affluent states, where religion mattered less. Globally, people in 154 countries were surveyed, with an average of 3,000 people per country. Overall, three out of four reported that religion was important in their life. Again, the devil was in the details. The countries in the sample varied in religiosity by a factor of six, ranging from 16 percent of respondents in Sweden agreeing that religion was an important part of their life to 99 percent affirmation in Bangladesh, Egypt, Sri Lanka and Somaliland. In this global poll, a tough environment also tended to coincide with greater national religiosity. If you live in a nation where daily existence is difficult, your life satisfaction is generally lower. In those countries, being more religious appears to grant you a premium on happiness that your less religious neighbours do not enjoy. If the living is easy, however, both nonbelievers and religious people have similar, relatively high subjective well-being. This effect held true for all religions represented in the sample – Buddhism, Christianity, Hinduism and Islam. More important, whether a person was struggling or thriving was nowhere near as predictive of religiosity as a society’s conditions and norms. “Do you have individual choice about what you believe? Yes, but there are strong influences in society – whether everyone around you is religious,” Ed Diener says. “That’s the eye-opener for me, that societal forces led to religiosity rather than individual forces.” For nonbelievers, these are heartening data. They bolster the idea that believers will accrue more psychological benefits only in places that value religion more, and vice versa. It means atheists are not permanently shut off from some fountain of happiness – although they may want to find a like-minded community to live in. Sandra Upson, Scientific American Mind",
        "id": 35,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "Which of the following conclusions about Ed Diener’s global study is true?",
        "options": [
            "Whatever the circumstances, being religious or non-religious makes little difference to a person’s happiness.",
            "In poor countries, religious individuals enjoy a greater sense of well-being than non-religious ",
            "In certain conditions, all major religions occa sionally fail to promote a sense of national well-being. ",
            "Regardless of country, relative happiness is generally boosted by a strong religious belief."
        ],
        "article": "Religion and Well-being A large body of research suggests that, as compared with religious individuals, people who lack a creed are less likely to be happy and healthy – surely the two most important earthly concerns – and tend to lose out on at least seven years of life, some estimates suggest. Several large-scale population studies have reinforced a single premise: the more you engage with religious activities, the better off you are. Yet many people are less certain about their beliefs than ever before. Non-believers number between 500 million and 750 million worldwide, according to one analysis. In the past two decades, the percentage of the U.S. population that proclaims its religious affiliation as “none” has more than doubled, to about 15 percent. Many of these individuals do not identify as atheists; even so, many of them conduct their life outside the religious establishment. Although research on this population is only now emerging, a recent study found that people who were less committed to their religious creed were actually less happy than avowed atheists. If the shifting beliefs of a growing number of people portend lower health and a less contented outlook, knowing exactly how religion benefits its followers becomes a matter of public concern. Several recent studies have sought to answer this question by examining it with greater resolution. For the first time, researchers have collected a representative sample of the world that measures religiosity and happiness. They found that the positive effects of religion depend enormously on where you live. Religious people may be happier than their godless counterparts, but only if the society they belong to values religion highly, which not all societies do. For atheists and the growing ranks of unaffiliated individuals, these findings bode well. Although many questions remain about how nonbelievers can acquire the health benefits of religion, scientists are now finding that secular communities of like-minded people can offer similar social support. To examine how the U.S., where being religious is the cultural norm, compares with the rest of the world, psychologist Ed Diener of the University of Illinois and his colleagues conducted a global survey, including a state-by-state sample in the U.S. In line with earlier findings, two thirds of those surveyed regarded religion as important to their life. The states, however, varied by a factor of two: in Vermont, 44 percent rated religion as important, compared with 88 percent in Mississippi. In the states where religion was very important, people were much more likely to be living in difficult circumstances. They also had lower subjective well-being than people living in less religious parts of the country. Did religion make them happier, as previous studies had shown? Absolutely, according to the data – but they were worse off than the contented residents of more affluent states, where religion mattered less. Globally, people in 154 countries were surveyed, with an average of 3,000 people per country. Overall, three out of four reported that religion was important in their life. Again, the devil was in the details. The countries in the sample varied in religiosity by a factor of six, ranging from 16 percent of respondents in Sweden agreeing that religion was an important part of their life to 99 percent affirmation in Bangladesh, Egypt, Sri Lanka and Somaliland. In this global poll, a tough environment also tended to coincide with greater national religiosity. If you live in a nation where daily existence is difficult, your life satisfaction is generally lower. In those countries, being more religious appears to grant you a premium on happiness that your less religious neighbours do not enjoy. If the living is easy, however, both nonbelievers and religious people have similar, relatively high subjective well-being. This effect held true for all religions represented in the sample – Buddhism, Christianity, Hinduism and Islam. More important, whether a person was struggling or thriving was nowhere near as predictive of religiosity as a society’s conditions and norms. “Do you have individual choice about what you believe? Yes, but there are strong influences in society – whether everyone around you is religious,” Ed Diener says. “That’s the eye-opener for me, that societal forces led to religiosity rather than individual forces.” For nonbelievers, these are heartening data. They bolster the idea that believers will accrue more psychological benefits only in places that value religion more, and vice versa. It means atheists are not permanently shut off from some fountain of happiness – although they may want to find a like-minded community to live in. Sandra Upson, Scientific American Mind",
        "id": 36,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What can be concluded from the last two paragraphs?",
        "options": [
            "Well-being is unrelated to religiosity.",
            "Atheism can be seen as a special form of religion.",
            "High social status may enhance religiosity.",
            "Religion in itself is no guarantee of happiness."
        ],
        "article": "Religion and Well-being A large body of research suggests that, as compared with religious individuals, people who lack a creed are less likely to be happy and healthy – surely the two most important earthly concerns – and tend to lose out on at least seven years of life, some estimates suggest. Several large-scale population studies have reinforced a single premise: the more you engage with religious activities, the better off you are. Yet many people are less certain about their beliefs than ever before. Non-believers number between 500 million and 750 million worldwide, according to one analysis. In the past two decades, the percentage of the U.S. population that proclaims its religious affiliation as “none” has more than doubled, to about 15 percent. Many of these individuals do not identify as atheists; even so, many of them conduct their life outside the religious establishment. Although research on this population is only now emerging, a recent study found that people who were less committed to their religious creed were actually less happy than avowed atheists. If the shifting beliefs of a growing number of people portend lower health and a less contented outlook, knowing exactly how religion benefits its followers becomes a matter of public concern. Several recent studies have sought to answer this question by examining it with greater resolution. For the first time, researchers have collected a representative sample of the world that measures religiosity and happiness. They found that the positive effects of religion depend enormously on where you live. Religious people may be happier than their godless counterparts, but only if the society they belong to values religion highly, which not all societies do. For atheists and the growing ranks of unaffiliated individuals, these findings bode well. Although many questions remain about how nonbelievers can acquire the health benefits of religion, scientists are now finding that secular communities of like-minded people can offer similar social support. To examine how the U.S., where being religious is the cultural norm, compares with the rest of the world, psychologist Ed Diener of the University of Illinois and his colleagues conducted a global survey, including a state-by-state sample in the U.S. In line with earlier findings, two thirds of those surveyed regarded religion as important to their life. The states, however, varied by a factor of two: in Vermont, 44 percent rated religion as important, compared with 88 percent in Mississippi. In the states where religion was very important, people were much more likely to be living in difficult circumstances. They also had lower subjective well-being than people living in less religious parts of the country. Did religion make them happier, as previous studies had shown? Absolutely, according to the data – but they were worse off than the contented residents of more affluent states, where religion mattered less. Globally, people in 154 countries were surveyed, with an average of 3,000 people per country. Overall, three out of four reported that religion was important in their life. Again, the devil was in the details. The countries in the sample varied in religiosity by a factor of six, ranging from 16 percent of respondents in Sweden agreeing that religion was an important part of their life to 99 percent affirmation in Bangladesh, Egypt, Sri Lanka and Somaliland. In this global poll, a tough environment also tended to coincide with greater national religiosity. If you live in a nation where daily existence is difficult, your life satisfaction is generally lower. In those countries, being more religious appears to grant you a premium on happiness that your less religious neighbours do not enjoy. If the living is easy, however, both nonbelievers and religious people have similar, relatively high subjective well-being. This effect held true for all religions represented in the sample – Buddhism, Christianity, Hinduism and Islam. More important, whether a person was struggling or thriving was nowhere near as predictive of religiosity as a society’s conditions and norms. “Do you have individual choice about what you believe? Yes, but there are strong influences in society – whether everyone around you is religious,” Ed Diener says. “That’s the eye-opener for me, that societal forces led to religiosity rather than individual forces.” For nonbelievers, these are heartening data. They bolster the idea that believers will accrue more psychological benefits only in places that value religion more, and vice versa. It means atheists are not permanently shut off from some fountain of happiness – although they may want to find a like-minded community to live in. Sandra Upson, Scientific American Mind",
        "id": 37,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "How could the tone in this text best be described?",
        "options": [
            "Sentimental",
            "Prosaic",
            "Sarcastic ",
            "Moving Question"
        ],
        "article": "Cooking The film Julie & Julia will, I hope, knock cookery off the pedestal it has acquired of being a pastime, a hobby or primarily an enjoyable activity. Cooking is, in fact, characterised by duty, boredom, bossiness and chaos; it is life in microcosm. Julie Powell, who set herself the task of cooking all 524 recipes in Julia Child’s Mastering the Art of French Cooking, had the right idea about food preparation: set yourself up for tightly scheduled failures on a regimented basis. This ensures that any passing pleasure one might find in the sensuality of cooking or the refinement of one’s talents will be overwhelmed by tedium and obligation. As it should be.",
        "id": 38,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "Which of the following statements is true, according to the text, with regard to calorie restriction in people?",
        "options": [
            "Eating less does not appear to affect life expectancy. ",
            "There are indications that eating less leads to a longer life.",
            "The new study confirms previous conclusions by using a different research method.",
            "Results based on monkeys cannot automatically be transferred to humans.",
            "Health Issue Studies have shown that reducing typical calorie con-"
        ],
        "article": "A Health Issue Studies have shown that reducing typical calorie consumption, usually by 30 to 40 percent, extends life span by a third or more in many animals, including nematodes, fruit flies and rodents. When it comes to calorie restriction in primates and people, however, the jury is still out. Although some studies have suggested that monkeys that eat less live longer, a new 25-year-long primate study concluded that calorie restriction does not extend average life span in rhesus monkeys. Even if calorie restriction does not help anyone live longer, a large portion of the data supports the idea that limiting food intake reduces the risks of diseases common in old age and lengthens the period of life spent in good health.",
        "id": 39,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What is the main point argued in this text?",
        "options": [
            "The benefits of education should be spread more equally among the members of a democratic society.",
            "From a global perspective, democratic ideals are no longer seen as self-evident in education.",
            "In education, steps should be taken to make young people realize the importance of democratic ideas.",
            "Results based on monkeys cannot automatically be transferred to humans.",
            "The spread of democracy has tended to devalue the quality of education in many parts of the world."
        ],
        "article": "Democracy and Education During the era in which people began to demand democratic self-governance, education all over the world was remodelled to produce the sort of student who could function well in this demanding form of government: not a cultivated gentleman, stuffed with the wisdom of the ages, but an active, critical, reflective, and empathetic member of a community of equals, capable of exchanging ideas on a basis of respect and understanding with people from C many different backgrounds. Today we still maintain that we like democracy and self-governance, and we also think that we like freedom of speech, respect for difference, and understanding of others. We give these values lip service, but we think far too little about what we need to do in order to transmit them to the next generation and ensure their survival.",
        "id": 40,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What is the most accurate description of women’s role in the British Foreign Office in the period between the two world wars?",
        "options": [
            "They were generally allowed to work in offices in Britain but rarely abroad.",
            "They worked with background tasks but were not permitted to perform any public duties. ",
            "They were usually not allowed to be involved in any type of work in the Foreign Office. ",
            "They played active roles in diplomatic negotiations but this was not officially recognized."
        ],
        "article": "Opening the Doors of Diplomacy In her interwar classic, Three Guineas, Virginia Woolf imagined the following scene. The daughter of an educated man, in the course of conversation with a brother or male acquaintance, raises the possibility that newly enfranchised women should now be admitted to all professions, including those still reserved for men: We on our side of the table become aware at once of some ‘strong emotion’ on your side ‘arising from some motive below the level of conscious thought’...; a glance at the private psychometer shows that the emotional temperature has risen from ten to twenty degrees above normal. Woolf was not present at the proceedings of the departmental committee convened by the British Foreign Office in 1934 to consider women’s suitability for diplomatic careers. Had she been, she would have felt little need to reconsider her analysis of the masculine instinct to preserve its professional privileges. The strength of feeling on the part of men in the Foreign Office stemmed from the perceived threat posed by women diplomats to the social and sexual order underpinning the modern diplomatic profession. In the 19th century, diplomacy evolved into a well-defined career for elite men accompanied by dutiful and loyal spouses. By the interwar years, women were employed as typists and in lower-grade clerical roles in embassies, but never as the professional equals of men. This state of affairs clearly suited Foreign Office authorities, who only reluctantly agreed to consider the possibility of appointing female diplomats following pressure from feminists and professional women’s societies. The head of the Foreign Office, Sir Robert Vansittart, made his position clear. Such a move ‘would not be in the interests of the women themselves, nor in the interest of His Majesty’s Service; and it would be damaging to the prestige of His Majesty’s Government in foreign countries’. In 1933, Vansittart handed the task of preparing the Foreign Office case to an assistant under-secretary, Charles Howard Smith, who swiftly pledged to gather ‘all the ammunition I can get’. This did not prove difficult. Of 51 ambassadors canvassed for their opinions, only three were in favour of employing women. The heads of the Consular Service, whose officers worked in less salubrious surroundings in remote outposts or port towns, were even more emphatically opposed. Their combined evidence advanced three main arguments. The first dwelt on the practical difficulties of posting women to countries where their status was low. In Vansittart’s words: ‘It is a false argument to say that, because women are treated as equals in this country, an Englishwoman abroad will be so treated by foreigners. She will not.’ The Ambassador to Berlin, Sir Eric Phipps, reinforced the point, noting that Nazi officials ‘would probably feel that they themselves were not being taken sufficiently seriously for being asked to receive her’. The second argument centred on the rough and tumble of ports, where consular officers could be called upon to placate violent and inebriated sailors, deal with outbreaks of venereal disease on merchant ships, or investigate allegations of ‘homosexual crime’ at sea. Such tasks were ‘obviously’ unsuitable for women. The third argument focused on marital status. A single woman, it was suggested, was of dramatically lower value than a male diplomat, who came with a spouse in tow. The Ambassador to Buenos Aires, Sir Henry Chilton, speculated that ‘a femme sole as Ambassadress would do no more than 50 per cent of the work done by my wife and myself ’. Conversely, a married woman presented the thorny problem of the diplomatic husband: if he shunned the part of trailing spouse, then her resignation became inevitable but, if he accompanied her en poste, the chances were he would quickly make himself a nuisance to the work of the embassy. In 1934 the committee found in favour of maintaining the status quo, yet just over a decade later women were representing Britain as diplomats in their own right. What happened to effect this dramatic reversal? Put simply, the answer was the war. Manpower shortages forced the Foreign Office, alongside other Whitehall departments, to post women overseas to carry out responsible political work. Their professional competence destroyed much of the Foreign Office case against women, while the sacrifices of ordinary women on the home front made it impossible to deny renewed feminist demands for equality in all areas of public life, including that of diplomacy. Woolf ’s psychometer, however, still ran high even after 1946. Alongside a marriage bar, which stayed in place until 1973, postwar female recruits encountered hostile department heads in London and were denied overseas postings by sceptical ambassadors. The Foreign Office today is a far more welcoming place, yet women remain outnumbered by men in the most senior grades to the level of three to one. The battle for the Foreign Office may be over, but it is not yet history. Helen McCarthy, History Today",
        "id": 36,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "How can the views of Sir Robert Vansittart and Charles Howard Smith best be described?",
        "options": [
            "Vansittart more than Smith often strove to take into account women’s own interests.",
            "Their views on women were most likely similar and reflected those of most ambassadors.",
            "Smith was much more radical than Vansittart in his lack of faith in women’s capabilities. ",
            "Their views on female diplomats were very different from those of the British ambassadors."
        ],
        "article": "Opening the Doors of Diplomacy In her interwar classic, Three Guineas, Virginia Woolf imagined the following scene. The daughter of an educated man, in the course of conversation with a brother or male acquaintance, raises the possibility that newly enfranchised women should now be admitted to all professions, including those still reserved for men: We on our side of the table become aware at once of some ‘strong emotion’ on your side ‘arising from some motive below the level of conscious thought’...; a glance at the private psychometer shows that the emotional temperature has risen from ten to twenty degrees above normal. Woolf was not present at the proceedings of the departmental committee convened by the British Foreign Office in 1934 to consider women’s suitability for diplomatic careers. Had she been, she would have felt little need to reconsider her analysis of the masculine instinct to preserve its professional privileges. The strength of feeling on the part of men in the Foreign Office stemmed from the perceived threat posed by women diplomats to the social and sexual order underpinning the modern diplomatic profession. In the 19th century, diplomacy evolved into a well-defined career for elite men accompanied by dutiful and loyal spouses. By the interwar years, women were employed as typists and in lower-grade clerical roles in embassies, but never as the professional equals of men. This state of affairs clearly suited Foreign Office authorities, who only reluctantly agreed to consider the possibility of appointing female diplomats following pressure from feminists and professional women’s societies. The head of the Foreign Office, Sir Robert Vansittart, made his position clear. Such a move ‘would not be in the interests of the women themselves, nor in the interest of His Majesty’s Service; and it would be damaging to the prestige of His Majesty’s Government in foreign countries’. In 1933, Vansittart handed the task of preparing the Foreign Office case to an assistant under-secretary, Charles Howard Smith, who swiftly pledged to gather ‘all the ammunition I can get’. This did not prove difficult. Of 51 ambassadors canvassed for their opinions, only three were in favour of employing women. The heads of the Consular Service, whose officers worked in less salubrious surroundings in remote outposts or port towns, were even more emphatically opposed. Their combined evidence advanced three main arguments. The first dwelt on the practical difficulties of posting women to countries where their status was low. In Vansittart’s words: ‘It is a false argument to say that, because women are treated as equals in this country, an Englishwoman abroad will be so treated by foreigners. She will not.’ The Ambassador to Berlin, Sir Eric Phipps, reinforced the point, noting that Nazi officials ‘would probably feel that they themselves were not being taken sufficiently seriously for being asked to receive her’. The second argument centred on the rough and tumble of ports, where consular officers could be called upon to placate violent and inebriated sailors, deal with outbreaks of venereal disease on merchant ships, or investigate allegations of ‘homosexual crime’ at sea. Such tasks were ‘obviously’ unsuitable for women. The third argument focused on marital status. A single woman, it was suggested, was of dramatically lower value than a male diplomat, who came with a spouse in tow. The Ambassador to Buenos Aires, Sir Henry Chilton, speculated that ‘a femme sole as Ambassadress would do no more than 50 per cent of the work done by my wife and myself ’. Conversely, a married woman presented the thorny problem of the diplomatic husband: if he shunned the part of trailing spouse, then her resignation became inevitable but, if he accompanied her en poste, the chances were he would quickly make himself a nuisance to the work of the embassy. In 1934 the committee found in favour of maintaining the status quo, yet just over a decade later women were representing Britain as diplomats in their own right. What happened to effect this dramatic reversal? Put simply, the answer was the war. Manpower shortages forced the Foreign Office, alongside other Whitehall departments, to post women overseas to carry out responsible political work. Their professional competence destroyed much of the Foreign Office case against women, while the sacrifices of ordinary women on the home front made it impossible to deny renewed feminist demands for equality in all areas of public life, including that of diplomacy. Woolf ’s psychometer, however, still ran high even after 1946. Alongside a marriage bar, which stayed in place until 1973, postwar female recruits encountered hostile department heads in London and were denied overseas postings by sceptical ambassadors. The Foreign Office today is a far more welcoming place, yet women remain outnumbered by men in the most senior grades to the level of three to one. The battle for the Foreign Office may be over, but it is not yet history. Helen McCarthy, History Today",
        "id": 37,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What did Vansittart claim about the status of men and women in 1930s Britain?",
        "options": [
            "There was no difference in status between British women and British men. ",
            "Women had lower status than men but higher than people in other countries.",
            "It was important to make sure that the status of British women was promoted.",
            "Men and women were so different that their status was impossible to compare."
        ],
        "article": "Opening the Doors of Diplomacy In her interwar classic, Three Guineas, Virginia Woolf imagined the following scene. The daughter of an educated man, in the course of conversation with a brother or male acquaintance, raises the possibility that newly enfranchised women should now be admitted to all professions, including those still reserved for men: We on our side of the table become aware at once of some ‘strong emotion’ on your side ‘arising from some motive below the level of conscious thought’...; a glance at the private psychometer shows that the emotional temperature has risen from ten to twenty degrees above normal. Woolf was not present at the proceedings of the departmental committee convened by the British Foreign Office in 1934 to consider women’s suitability for diplomatic careers. Had she been, she would have felt little need to reconsider her analysis of the masculine instinct to preserve its professional privileges. The strength of feeling on the part of men in the Foreign Office stemmed from the perceived threat posed by women diplomats to the social and sexual order underpinning the modern diplomatic profession. In the 19th century, diplomacy evolved into a well-defined career for elite men accompanied by dutiful and loyal spouses. By the interwar years, women were employed as typists and in lower-grade clerical roles in embassies, but never as the professional equals of men. This state of affairs clearly suited Foreign Office authorities, who only reluctantly agreed to consider the possibility of appointing female diplomats following pressure from feminists and professional women’s societies. The head of the Foreign Office, Sir Robert Vansittart, made his position clear. Such a move ‘would not be in the interests of the women themselves, nor in the interest of His Majesty’s Service; and it would be damaging to the prestige of His Majesty’s Government in foreign countries’. In 1933, Vansittart handed the task of preparing the Foreign Office case to an assistant under-secretary, Charles Howard Smith, who swiftly pledged to gather ‘all the ammunition I can get’. This did not prove difficult. Of 51 ambassadors canvassed for their opinions, only three were in favour of employing women. The heads of the Consular Service, whose officers worked in less salubrious surroundings in remote outposts or port towns, were even more emphatically opposed. Their combined evidence advanced three main arguments. The first dwelt on the practical difficulties of posting women to countries where their status was low. In Vansittart’s words: ‘It is a false argument to say that, because women are treated as equals in this country, an Englishwoman abroad will be so treated by foreigners. She will not.’ The Ambassador to Berlin, Sir Eric Phipps, reinforced the point, noting that Nazi officials ‘would probably feel that they themselves were not being taken sufficiently seriously for being asked to receive her’. The second argument centred on the rough and tumble of ports, where consular officers could be called upon to placate violent and inebriated sailors, deal with outbreaks of venereal disease on merchant ships, or investigate allegations of ‘homosexual crime’ at sea. Such tasks were ‘obviously’ unsuitable for women. The third argument focused on marital status. A single woman, it was suggested, was of dramatically lower value than a male diplomat, who came with a spouse in tow. The Ambassador to Buenos Aires, Sir Henry Chilton, speculated that ‘a femme sole as Ambassadress would do no more than 50 per cent of the work done by my wife and myself ’. Conversely, a married woman presented the thorny problem of the diplomatic husband: if he shunned the part of trailing spouse, then her resignation became inevitable but, if he accompanied her en poste, the chances were he would quickly make himself a nuisance to the work of the embassy. In 1934 the committee found in favour of maintaining the status quo, yet just over a decade later women were representing Britain as diplomats in their own right. What happened to effect this dramatic reversal? Put simply, the answer was the war. Manpower shortages forced the Foreign Office, alongside other Whitehall departments, to post women overseas to carry out responsible political work. Their professional competence destroyed much of the Foreign Office case against women, while the sacrifices of ordinary women on the home front made it impossible to deny renewed feminist demands for equality in all areas of public life, including that of diplomacy. Woolf ’s psychometer, however, still ran high even after 1946. Alongside a marriage bar, which stayed in place until 1973, postwar female recruits encountered hostile department heads in London and were denied overseas postings by sceptical ambassadors. The Foreign Office today is a far more welcoming place, yet women remain outnumbered by men in the most senior grades to the level of three to one. The battle for the Foreign Office may be over, but it is not yet history. Helen McCarthy, History Today",
        "id": 38,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "How did the Second World War affect arguments against women entering the diplomatic profession?",
        "options": [
            "The work of women appointed during the war showed that the arguments were flawed. ",
            "The experience of war united the British people and made men less narrow-minded.",
            "The war completely changed traditionally existing roles of men and women in Britain.",
            "The arguments lost their force since women proved to be the best wartime diplomats of all."
        ],
        "article": "Opening the Doors of Diplomacy In her interwar classic, Three Guineas, Virginia Woolf imagined the following scene. The daughter of an educated man, in the course of conversation with a brother or male acquaintance, raises the possibility that newly enfranchised women should now be admitted to all professions, including those still reserved for men: We on our side of the table become aware at once of some ‘strong emotion’ on your side ‘arising from some motive below the level of conscious thought’...; a glance at the private psychometer shows that the emotional temperature has risen from ten to twenty degrees above normal. Woolf was not present at the proceedings of the departmental committee convened by the British Foreign Office in 1934 to consider women’s suitability for diplomatic careers. Had she been, she would have felt little need to reconsider her analysis of the masculine instinct to preserve its professional privileges. The strength of feeling on the part of men in the Foreign Office stemmed from the perceived threat posed by women diplomats to the social and sexual order underpinning the modern diplomatic profession. In the 19th century, diplomacy evolved into a well-defined career for elite men accompanied by dutiful and loyal spouses. By the interwar years, women were employed as typists and in lower-grade clerical roles in embassies, but never as the professional equals of men. This state of affairs clearly suited Foreign Office authorities, who only reluctantly agreed to consider the possibility of appointing female diplomats following pressure from feminists and professional women’s societies. The head of the Foreign Office, Sir Robert Vansittart, made his position clear. Such a move ‘would not be in the interests of the women themselves, nor in the interest of His Majesty’s Service; and it would be damaging to the prestige of His Majesty’s Government in foreign countries’. In 1933, Vansittart handed the task of preparing the Foreign Office case to an assistant under-secretary, Charles Howard Smith, who swiftly pledged to gather ‘all the ammunition I can get’. This did not prove difficult. Of 51 ambassadors canvassed for their opinions, only three were in favour of employing women. The heads of the Consular Service, whose officers worked in less salubrious surroundings in remote outposts or port towns, were even more emphatically opposed. Their combined evidence advanced three main arguments. The first dwelt on the practical difficulties of posting women to countries where their status was low. In Vansittart’s words: ‘It is a false argument to say that, because women are treated as equals in this country, an Englishwoman abroad will be so treated by foreigners. She will not.’ The Ambassador to Berlin, Sir Eric Phipps, reinforced the point, noting that Nazi officials ‘would probably feel that they themselves were not being taken sufficiently seriously for being asked to receive her’. The second argument centred on the rough and tumble of ports, where consular officers could be called upon to placate violent and inebriated sailors, deal with outbreaks of venereal disease on merchant ships, or investigate allegations of ‘homosexual crime’ at sea. Such tasks were ‘obviously’ unsuitable for women. The third argument focused on marital status. A single woman, it was suggested, was of dramatically lower value than a male diplomat, who came with a spouse in tow. The Ambassador to Buenos Aires, Sir Henry Chilton, speculated that ‘a femme sole as Ambassadress would do no more than 50 per cent of the work done by my wife and myself ’. Conversely, a married woman presented the thorny problem of the diplomatic husband: if he shunned the part of trailing spouse, then her resignation became inevitable but, if he accompanied her en poste, the chances were he would quickly make himself a nuisance to the work of the embassy. In 1934 the committee found in favour of maintaining the status quo, yet just over a decade later women were representing Britain as diplomats in their own right. What happened to effect this dramatic reversal? Put simply, the answer was the war. Manpower shortages forced the Foreign Office, alongside other Whitehall departments, to post women overseas to carry out responsible political work. Their professional competence destroyed much of the Foreign Office case against women, while the sacrifices of ordinary women on the home front made it impossible to deny renewed feminist demands for equality in all areas of public life, including that of diplomacy. Woolf ’s psychometer, however, still ran high even after 1946. Alongside a marriage bar, which stayed in place until 1973, postwar female recruits encountered hostile department heads in London and were denied overseas postings by sceptical ambassadors. The Foreign Office today is a far more welcoming place, yet women remain outnumbered by men in the most senior grades to the level of three to one. The battle for the Foreign Office may be over, but it is not yet history. Helen McCarthy, History Today",
        "id": 39,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What is meant by the statement that after the Second World War ‘Woolf ’s psychometer still ran high’?",
        "options": [
            "Generally only women with certain personality traits were allowed to work in diplomacy.",
            "There was very little change in the opportunities available for women to become diplomats.",
            "Many women were furious about the fact that the change was taking place so slowly.",
            "Many men continued to be suspicious of women’s involvement in diplomatic affairs."
        ],
        "article": "Opening the Doors of Diplomacy In her interwar classic, Three Guineas, Virginia Woolf imagined the following scene. The daughter of an educated man, in the course of conversation with a brother or male acquaintance, raises the possibility that newly enfranchised women should now be admitted to all professions, including those still reserved for men: We on our side of the table become aware at once of some ‘strong emotion’ on your side ‘arising from some motive below the level of conscious thought... a glance at the private psychometer shows that the emotional temperature has risen from ten to twenty degrees above normal. Woolf was not present at the proceedings of the departmental committee convened by the British Foreign Office in 1934 to consider women’s suitability for diplomatic careers. Had she been, she would have felt little need to reconsider her analysis of the masculine instinct to preserve its professional privileges. The strength of feeling on the part of men in the Foreign Office stemmed from the perceived threat posed by women diplomats to the social and sexual order underpinning the modern diplomatic profession. In the 19th century, diplomacy evolved into a well-defined career for elite men accompanied by dutiful and loyal spouses. By the interwar years, women were employed as typists and in lower-grade clerical roles in embassies, but never as the professional equals of men. This state of affairs clearly suited Foreign Office authorities, who only reluctantly agreed to consider the possibility of appointing female diplomats following pressure from feminists and professional women’s societies. The head of the Foreign Office, Sir Robert Vansittart, made his position clear. Such a move ‘would not be in the interests of the women themselves, nor in the interest of His Majesty’s Service; and it would be damaging to the prestige of His Majesty’s Government in foreign countries’. In 1933, Vansittart handed the task of preparing the Foreign Office case to an assistant under-secretary, Charles Howard Smith, who swiftly pledged to gather ‘all the ammunition I can get’. This did not prove difficult. Of 51 ambassadors canvassed for their opinions, only three were in favour of employing women. The heads of the Consular Service, whose officers worked in less salubrious surroundings in remote outposts or port towns, were even more emphatically opposed. Their combined evidence advanced three main arguments. The first dwelt on the practical difficulties of posting women to countries where their status was low. In Vansittart’s words: ‘It is a false argument to say that, because women are treated as equals in this country, an Englishwoman abroad will be so treated by foreigners. She will not.’ The Ambassador to Berlin, Sir Eric Phipps, reinforced the point, noting that Nazi officials ‘would probably feel that they themselves were not being taken sufficiently seriously for being asked to receive her’. The second argument centred on the rough and tumble of ports, where consular officers could be called upon to placate violent and inebriated sailors, deal with outbreaks of venereal disease on merchant ships, or investigate allegations of ‘homosexual crime’ at sea. Such tasks were ‘obviously’ unsuitable for women. The third argument focused on marital status. A single woman, it was suggested, was of dramatically lower value than a male diplomat, who came with a spouse in tow. The Ambassador to Buenos Aires, Sir Henry Chilton, speculated that ‘a femme sole as Ambassadress would do no more than 50 per cent of the work done by my wife and myself ’. Conversely, a married woman presented the thorny problem of the diplomatic husband: if he shunned the part of trailing spouse, then her resignation became inevitable but, if he accompanied her en poste, the chances were he would quickly make himself a nuisance to the work of the embassy. In 1934 the committee found in favour of maintaining the status quo, yet just over a decade later women were representing Britain as diplomats in their own right. What happened to effect this dramatic reversal? Put simply, the answer was the war. Manpower shortages forced the Foreign Office, alongside other Whitehall departments, to post women overseas to carry out responsible political work. Their professional competence destroyed much of the Foreign Office case against women, while the sacrifices of ordinary women on the home front made it impossible to deny renewed feminist demands for equality in all areas of public life, including that of diplomacy. Woolf ’s psychometer, however, still ran high even after 1946. Alongside a marriage bar, which stayed in place until 1973, postwar female recruits encountered hostile department heads in London and were denied overseas postings by sceptical ambassadors. The Foreign Office today is a far more welcoming place, yet women remain outnumbered by men in the most senior grades to the level of three to one. The battle for the Foreign Office may be over, but it is not yet history. Helen McCarthy, History Today",
        "id": 40,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2017"
    },
    {
        "question": "What is implied here?",
        "options": [
            "Recycling has been found to be the best way to achieve a sustainable society. ",
            "There is an urgent need to set up global popu­ lation targets.",
            "More funding has now been allocated to mod­ ernize the food industry.",
            "There is a risk of a world collapse unless more effort is put into energy production."
        ],
        "article": "Sustainability We can talk all we like about recycling and sustainable agriculture, but population is the issue that really matters. Yet it is the one on which so many people are silent. We have made the human right to reproduce unchallengeable: to do so is either to be eugenicist or – as with China’s one-child policy – repressively authoritarian. But sooner or later we have to do something. No matter how much recycling we do, how much renewable energy we create and how much better we become at producing food, there has to come a time when the world’s population makes the planet unsustainable. The pressure on resources is being maintained at both ends of the spectrum: not only are more babies being born, people are living longer and longer.",
        "id": 31,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What are we told about recessions?",
        "options": [
            "Confident professionals seem to be less affected by them than those who are less sure.",
            "They involve both risk­taking and gambling with other people’s money. ",
            "They are related to lack of trust and are one of the ways in which capitalism works. ",
            "They are a result of people’s unethical drive to make more money than they need. "
        ],
        "article": "Recession Economists usually try to solve problems by tinkering with things, and making them more complicated. That is because it often works – until, suddenly, it doesn’t. The Nobel Prize winner Paul Krugman, who is a specialist in recessions, takes us through the history of why they happen. It is always because people devise an ingenious way to make what appears to be free money and nobody understands the consequences until it is too late. There is, it turns out, no such thing as a free lunch. The entire edifice of capitalism is based on capital – which is really just another word for confidence. If people believe your confidence to be authentic, the risk you take is likely to be small. But as soon as people think you are bluffing, they panic – and panic destroys wealth faster than confidence can ever increase it.",
        "id": 32,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What is said in relation to the way New Orleans has presented itself to visitors?",
        "options": [
            "There is little reason why the city should see itself as a tourist magnet.",
            "It has played down those aspects of its history that do not fit in with its tourist image. ",
            "The city has failed to exploit the full commercial potential of modern tourism.",
            "Its promotion of itself as a major tourist attraction is mainly the result of its glorious past."
        ],
        "article": "New Orleans When Hurricane Katrina hit New Orleans in late August 2005, the city and its people suffered wounds that are still healing. While the rebuilding efforts continue apace, everyone seems to agree that the city that eventually emerges from the wreckage will be very different to the “old” New Orleans. But it was not just the physical city that was devastated by Katrina. Equally damaged was its carefully cultivated image as the city that care forgot. Shocked by the indelible images of destruction wrought by the hurricane and its aftermath, the world was presented with an unsettling and unfamiliar vision of a city that it thought it knew well. For many years, New Orleans had traded on a painstakingly constructed sense of itself as a tourist town. Before Katrina, the city was famous the world over as one of America’s most beloved playgrounds, known for its food, its music, for Mardi Gras and the hedonism of Bourbon Street and for letting the good times roll. It was also a city that capitalised on its history – or, at least, selected highlights from its past. Whatever else it did, Hurricane Katrina tore away the city’s elaborate carnival mask and demonstrated that life in the Big Easy has not always been so. In stark contrast to its laissez-faire reputation, this 21st-century disaster evoked parts of the city’s hidden history, exposing a landscape that had long been scarred by repeated traumas. Indeed, though Katrina may have seemed a uniquely terrible event to modern eyes, it was a relatively characteristic moment in the history of a city used to extraordinary reversals of fortune. It was a reminder that catastrophe, adversity and rebirth had been part of the fabric of life in New Orleans since its foundation in 1718. Flood, fire, famine and war – New Orleans has seen it all. Beginning life as a small colonial backwater on the banks of the Mississippi River, traded between France and Spain and valued largely for its strategic location, New Orleans was acquired by the United States in 1803 as part of the Louisiana Purchase. It soon became one of the richest and most powerful cities in the US, made prosperous by the cotton trade and its status as the home of the largest slave market in the South. By the mid-19th century, New Orleans was internationally famous as the imperious, opulent, cosmopolitan Queen of the South. That success, however, came at significant cost, since the city’s rise took place against a backdrop of repeated disasters and an almost constant sense of flux. Partly this was a result of its location. Or, as New Orleans Mayor Marc Morial put it in 2001: ‘This was a lousy place to put a city.’ Unsurprisingly, therefore, the city’s particular relationship with catastrophe has long been proverbial. As bad as hurricanes and conflagrations and potential slave rebellions could be, perhaps no single threat was more pressing and more characteristic of 19th-century New Orleans than disease. In the years before the Civil War, New Orleans had the highest mortality rate in the United States. Cholera carried away nearly 5,000 people in 1832 alone. More terrifying still were the periodic visitations of yellow fever. The city suffered multiple outbreaks of the disease from the late 18th to the early 20th century, but it was in the middle of the 19th century that the worst incidents took place. In countless ways, the previous traumas in the life of New Orleans prefigured the ways in which the evils of Katrina and its aftermath unfolded. Indeed, what the events of 2005 also revealed was the degree to which, even before Katrina, this was a city marked by disaster. While 21st-century New Orleans may not have been bothered by repeated yellow fever epidemics, it was certainly plagued by the highest per-capita murder rate in America – an accolade, unfortunately, that the city currently looks set on reclaiming. Yet, if this modern moment of devastation evokes past disasters, it also brings to mind previous moments of rebirth for the city. The continued existence of New Orleans is a story of survival against the odds and repeated renewals. Of course, this new beginning is tinged with tragedy. That there have already been profound problems with the rebuilding effort is clear for all to see. That those problems have been unfairly spread is equally evident; a recent study showed that the city’s neighbourhoods are now more segregated than they were before Katrina. The seemingly inevitable cycles of death and rebirth might be woven into the fabric of New Orleans’ history and, as an incident like the BP oil spill highlighted, there is little doubt that a city in such a ‘lousy place’ is likely to face hard times again. But as the process of rebuilding continues, this might at least be the moment to lay some ghosts to rest. Thomas Ruys Smith, History Today",
        "id": 33,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What are we told in relation to New Orleans’ “particular relationship with catastrophe”?",
        "options": [
            "The city’s geographical position may in some ways be described as a recipe for disaster. ",
            "People in New Orleans have long realized that the city’s prosperity is dependent on ",
            "It is only in recent years that natural disasters have been a recurring feature of the city’s life.",
            "New Orleans tends to see itself as the constant victim of mysterious forces beyond its control."
        ],
        "article": "New Orleans When Hurricane Katrina hit New Orleans in late August 2005, the city and its people suffered wounds that are still healing. While the rebuilding efforts continue apace, everyone seems to agree that the city that eventually emerges from the wreckage will be very different to the “old” New Orleans. But it was not just the physical city that was devastated by Katrina. Equally damaged was its carefully cultivated image as the city that care forgot. Shocked by the indelible images of destruction wrought by the hurricane and its aftermath, the world was presented with an unsettling and unfamiliar vision of a city that it thought it knew well. For many years, New Orleans had traded on a painstakingly constructed sense of itself as a tourist town. Before Katrina, the city was famous the world over as one of America’s most beloved playgrounds, known for its food, its music, for Mardi Gras and the hedonism of Bourbon Street and for letting the good times roll. It was also a city that capitalised on its history – or, at least, selected highlights from its past. Whatever else it did, Hurricane Katrina tore away the city’s elaborate carnival mask and demonstrated that life in the Big Easy has not always been so. In stark contrast to its laissez-faire reputation, this 21st-century disaster evoked parts of the city’s hidden history, exposing a landscape that had long been scarred by repeated traumas. Indeed, though Katrina may have seemed a uniquely terrible event to modern eyes, it was a relatively characteristic moment in the history of a city used to extraordinary reversals of fortune. It was a reminder that catastrophe, adversity and rebirth had been part of the fabric of life in New Orleans since its foundation in 1718. Flood, fire, famine and war – New Orleans has seen it all. Beginning life as a small colonial backwater on the banks of the Mississippi River, traded between France and Spain and valued largely for its strategic location, New Orleans was acquired by the United States in 1803 as part of the Louisiana Purchase. It soon became one of the richest and most powerful cities in the US, made prosperous by the cotton trade and its status as the home of the largest slave market in the South. By the mid-19th century, New Orleans was internationally famous as the imperious, opulent, cosmopolitan Queen of the South. That success, however, came at significant cost, since the city’s rise took place against a backdrop of repeated disasters and an almost constant sense of flux. Partly this was a result of its location. Or, as New Orleans Mayor Marc Morial put it in 2001: ‘This was a lousy place to put a city.’ Unsurprisingly, therefore, the city’s particular relationship with catastrophe has long been proverbial. As bad as hurricanes and conflagrations and potential slave rebellions could be, perhaps no single threat was more pressing and more characteristic of 19th-century New Orleans than disease. In the years before the Civil War, New Orleans had the highest mortality rate in the United States. Cholera carried away nearly 5,000 people in 1832 alone. More terrifying still were the periodic visitations of yellow fever. The city suffered multiple outbreaks of the disease from the late 18th to the early 20th century, but it was in the middle of the 19th century that the worst incidents took place. In countless ways, the previous traumas in the life of New Orleans prefigured the ways in which the evils of Katrina and its aftermath unfolded. Indeed, what the events of 2005 also revealed was the degree to which, even before Katrina, this was a city marked by disaster. While 21st-century New Orleans may not have been bothered by repeated yellow fever epidemics, it was certainly plagued by the highest per-capita murder rate in America – an accolade, unfortunately, that the city currently looks set on reclaiming. Yet, if this modern moment of devastation evokes past disasters, it also brings to mind previous moments of rebirth for the city. The continued existence of New Orleans is a story of survival against the odds and repeated renewals. Of course, this new beginning is tinged with tragedy. That there have already been profound problems with the rebuilding effort is clear for all to see. That those problems have been unfairly spread is equally evident; a recent study showed that the city’s neighbourhoods are now more segregated than they were before Katrina. The seemingly inevitable cycles of death and rebirth might be woven into the fabric of New Orleans’ history and, as an incident like the BP oil spill highlighted, there is little doubt that a city in such a ‘lousy place’ is likely to face hard times again. But as the process of rebuilding continues, this might at least be the moment to lay some ghosts to rest. Thomas Ruys Smith, History Today",
        "id": 34,
        "answer": "A",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What is implied about different kinds of disasters in 19th-century New Orleans?",
        "options": [
            "Then, as now, hurricanes were regarded as the biggest threat to life and property.",
            "The consequences of racial uprisings were looked upon as more serious than those of hurricanes and flooding.",
            "Epidemics of yellow fever seem to have been feared much more than natural disasters. ",
            "Repeated outbreaks of cholera caused more deaths than any other disease."
        ],
        "article": "New Orleans When Hurricane Katrina hit New Orleans in late August 2005, the city and its people suffered wounds that are still healing. While the rebuilding efforts continue apace, everyone seems to agree that the city that eventually emerges from the wreckage will be very different to the “old” New Orleans. But it was not just the physical city that was devastated by Katrina. Equally damaged was its carefully cultivated image as the city that care forgot. Shocked by the indelible images of destruction wrought by the hurricane and its aftermath, the world was presented with an unsettling and unfamiliar vision of a city that it thought it knew well. For many years, New Orleans had traded on a painstakingly constructed sense of itself as a tourist town. Before Katrina, the city was famous the world over as one of America’s most beloved playgrounds, known for its food, its music, for Mardi Gras and the hedonism of Bourbon Street and for letting the good times roll. It was also a city that capitalised on its history – or, at least, selected highlights from its past. Whatever else it did, Hurricane Katrina tore away the city’s elaborate carnival mask and demonstrated that life in the Big Easy has not always been so. In stark contrast to its laissez-faire reputation, this 21st-century disaster evoked parts of the city’s hidden history, exposing a landscape that had long been scarred by repeated traumas. Indeed, though Katrina may have seemed a uniquely terrible event to modern eyes, it was a relatively characteristic moment in the history of a city used to extraordinary reversals of fortune. It was a reminder that catastrophe, adversity and rebirth had been part of the fabric of life in New Orleans since its foundation in 1718. Flood, fire, famine and war – New Orleans has seen it all. Beginning life as a small colonial backwater on the banks of the Mississippi River, traded between France and Spain and valued largely for its strategic location, New Orleans was acquired by the United States in 1803 as part of the Louisiana Purchase. It soon became one of the richest and most powerful cities in the US, made prosperous by the cotton trade and its status as the home of the largest slave market in the South. By the mid-19th century, New Orleans was internationally famous as the imperious, opulent, cosmopolitan Queen of the South. That success, however, came at significant cost, since the city’s rise took place against a backdrop of repeated disasters and an almost constant sense of flux. Partly this was a result of its location. Or, as New Orleans Mayor Marc Morial put it in 2001: ‘This was a lousy place to put a city.’ Unsurprisingly, therefore, the city’s particular relationship with catastrophe has long been proverbial. As bad as hurricanes and conflagrations and potential slave rebellions could be, perhaps no single threat was more pressing and more characteristic of 19th-century New Orleans than disease. In the years before the Civil War, New Orleans had the highest mortality rate in the United States. Cholera carried away nearly 5,000 people in 1832 alone. More terrifying still were the periodic visitations of yellow fever. The city suffered multiple outbreaks of the disease from the late 18th to the early 20th century, but it was in the middle of the 19th century that the worst incidents took place. In countless ways, the previous traumas in the life of New Orleans prefigured the ways in which the evils of Katrina and its aftermath unfolded. Indeed, what the events of 2005 also revealed was the degree to which, even before Katrina, this was a city marked by disaster. While 21st-century New Orleans may not have been bothered by repeated yellow fever epidemics, it was certainly plagued by the highest per-capita murder rate in America – an accolade, unfortunately, that the city currently looks set on reclaiming. Yet, if this modern moment of devastation evokes past disasters, it also brings to mind previous moments of rebirth for the city. The continued existence of New Orleans is a story of survival against the odds and repeated renewals. Of course, this new beginning is tinged with tragedy. That there have already been profound problems with the rebuilding effort is clear for all to see. That those problems have been unfairly spread is equally evident; a recent study showed that the city’s neighbourhoods are now more segregated than they were before Katrina. The seemingly inevitable cycles of death and rebirth might be woven into the fabric of New Orleans’ history and, as an incident like the BP oil spill highlighted, there is little doubt that a city in such a ‘lousy place’ is likely to face hard times again. But as the process of rebuilding continues, this might at least be the moment to lay some ghosts to rest. Thomas Ruys Smith, History Today",
        "id": 35,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "Which of the following statements about the per-capita murder rate in New Orleans is most in agreement with the text?",
        "options": [
            "It has been rising for decades. ",
            "After the hurricane, no reliable statistics are as yet available. ",
            "It has been falling for decades.",
            "After a temporary drop, it seems to be rising again."
        ],
        "article": "New Orleans When Hurricane Katrina hit New Orleans in late August 2005, the city and its people suffered wounds that are still healing. While the rebuilding efforts continue apace, everyone seems to agree that the city that eventually emerges from the wreckage will be very different to the “old” New Orleans. But it was not just the physical city that was devastated by Katrina. Equally damaged was its carefully cultivated image as the city that care forgot. Shocked by the indelible images of destruction wrought by the hurricane and its aftermath, the world was presented with an unsettling and unfamiliar vision of a city that it thought it knew well. For many years, New Orleans had traded on a painstakingly constructed sense of itself as a tourist town. Before Katrina, the city was famous the world over as one of America’s most beloved playgrounds, known for its food, its music, for Mardi Gras and the hedonism of Bourbon Street and for letting the good times roll. It was also a city that capitalised on its history – or, at least, selected highlights from its past. Whatever else it did, Hurricane Katrina tore away the city’s elaborate carnival mask and demonstrated that life in the Big Easy has not always been so. In stark contrast to its laissez-faire reputation, this 21st-century disaster evoked parts of the city’s hidden history, exposing a landscape that had long been scarred by repeated traumas. Indeed, though Katrina may have seemed a uniquely terrible event to modern eyes, it was a relatively characteristic moment in the history of a city used to extraordinary reversals of fortune. It was a reminder that catastrophe, adversity and rebirth had been part of the fabric of life in New Orleans since its foundation in 1718. Flood, fire, famine and war – New Orleans has seen it all. Beginning life as a small colonial backwater on the banks of the Mississippi River, traded between France and Spain and valued largely for its strategic location, New Orleans was acquired by the United States in 1803 as part of the Louisiana Purchase. It soon became one of the richest and most powerful cities in the US, made prosperous by the cotton trade and its status as the home of the largest slave market in the South. By the mid-19th century, New Orleans was internationally famous as the imperious, opulent, cosmopolitan Queen of the South. That success, however, came at significant cost, since the city’s rise took place against a backdrop of repeated disasters and an almost constant sense of flux. Partly this was a result of its location. Or, as New Orleans Mayor Marc Morial put it in 2001: ‘This was a lousy place to put a city.’ Unsurprisingly, therefore, the city’s particular relationship with catastrophe has long been proverbial. As bad as hurricanes and conflagrations and potential slave rebellions could be, perhaps no single threat was more pressing and more characteristic of 19th-century New Orleans than disease. In the years before the Civil War, New Orleans had the highest mortality rate in the United States. Cholera carried away nearly 5,000 people in 1832 alone. More terrifying still were the periodic visitations of yellow fever. The city suffered multiple outbreaks of the disease from the late 18th to the early 20th century, but it was in the middle of the 19th century that the worst incidents took place. In countless ways, the previous traumas in the life of New Orleans prefigured the ways in which the evils of Katrina and its aftermath unfolded. Indeed, what the events of 2005 also revealed was the degree to which, even before Katrina, this was a city marked by disaster. While 21st-century New Orleans may not have been bothered by repeated yellow fever epidemics, it was certainly plagued by the highest per-capita murder rate in America – an accolade, unfortunately, that the city currently looks set on reclaiming. Yet, if this modern moment of devastation evokes past disasters, it also brings to mind previous moments of rebirth for the city. The continued existence of New Orleans is a story of survival against the odds and repeated renewals. Of course, this new beginning is tinged with tragedy. That there have already been profound problems with the rebuilding effort is clear for all to see. That those problems have been unfairly spread is equally evident; a recent study showed that the city’s neighbourhoods are now more segregated than they were before Katrina. The seemingly inevitable cycles of death and rebirth might be woven into the fabric of New Orleans’ history and, as an incident like the BP oil spill highlighted, there is little doubt that a city in such a ‘lousy place’ is likely to face hard times again. But as the process of rebuilding continues, this might at least be the moment to lay some ghosts to rest. Thomas Ruys Smith, History Today",
        "id": 36,
        "answer": "D",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What is the writer’s main point in connection with New Orleans and Hurricane Katrina?",
        "options": [
            "The Katrina disaster has taught New Orleans a lesson that will help to prevent future catastrophes",
            "Hurricane Katrina was only the latest in a long series of disasters. ",
            "The Katrina tragedy was a unique historical event, unlikely to be repeated. ",
            "Though devastating, Hurricane Katrina’s impact will be forgotten sooner than we now think."
        ],
        "article": "New Orleans When Hurricane Katrina hit New Orleans in late August 2005, the city and its people suffered wounds that are still healing. While the rebuilding efforts continue apace, everyone seems to agree that the city that eventually emerges from the wreckage will be very different to the “old” New Orleans. But it was not just the physical city that was devastated by Katrina. Equally damaged was its carefully cultivated image as the city that care forgot. Shocked by the indelible images of destruction wrought by the hurricane and its aftermath, the world was presented with an unsettling and unfamiliar vision of a city that it thought it knew well. For many years, New Orleans had traded on a painstakingly constructed sense of itself as a tourist town. Before Katrina, the city was famous the world over as one of America’s most beloved playgrounds, known for its food, its music, for Mardi Gras and the hedonism of Bourbon Street and for letting the good times roll. It was also a city that capitalised on its history – or, at least, selected highlights from its past. Whatever else it did, Hurricane Katrina tore away the city’s elaborate carnival mask and demonstrated that life in the Big Easy has not always been so. In stark contrast to its laissez-faire reputation, this 21st-century disaster evoked parts of the city’s hidden history, exposing a landscape that had long been scarred by repeated traumas. Indeed, though Katrina may have seemed a uniquely terrible event to modern eyes, it was a relatively characteristic moment in the history of a city used to extraordinary reversals of fortune. It was a reminder that catastrophe, adversity and rebirth had been part of the fabric of life in New Orleans since its foundation in 1718. Flood, fire, famine and war – New Orleans has seen it all. Beginning life as a small colonial backwater on the banks of the Mississippi River, traded between France and Spain and valued largely for its strategic location, New Orleans was acquired by the United States in 1803 as part of the Louisiana Purchase. It soon became one of the richest and most powerful cities in the US, made prosperous by the cotton trade and its status as the home of the largest slave market in the South. By the mid-19th century, New Orleans was internationally famous as the imperious, opulent, cosmopolitan Queen of the South. That success, however, came at significant cost, since the city’s rise took place against a backdrop of repeated disasters and an almost constant sense of flux. Partly this was a result of its location. Or, as New Orleans Mayor Marc Morial put it in 2001: ‘This was a lousy place to put a city.’ Unsurprisingly, therefore, the city’s particular relationship with catastrophe has long been proverbial. As bad as hurricanes and conflagrations and potential slave rebellions could be, perhaps no single threat was more pressing and more characteristic of 19th-century New Orleans than disease. In the years before the Civil War, New Orleans had the highest mortality rate in the United States. Cholera carried away nearly 5,000 people in 1832 alone. More terrifying still were the periodic visitations of yellow fever. The city suffered multiple outbreaks of the disease from the late 18th to the early 20th century, but it was in the middle of the 19th century that the worst incidents took place. In countless ways, the previous traumas in the life of New Orleans prefigured the ways in which the evils of Katrina and its aftermath unfolded. Indeed, what the events of 2005 also revealed was the degree to which, even before Katrina, this was a city marked by disaster. While 21st-century New Orleans may not have been bothered by repeated yellow fever epidemics, it was certainly plagued by the highest per-capita murder rate in America – an accolade, unfortunately, that the city currently looks set on reclaiming. Yet, if this modern moment of devastation evokes past disasters, it also brings to mind previous moments of rebirth for the city. The continued existence of New Orleans is a story of survival against the odds and repeated renewals. Of course, this new beginning is tinged with tragedy. That there have already been profound problems with the rebuilding effort is clear for all to see. That those problems have been unfairly spread is equally evident; a recent study showed that the city’s neighbourhoods are now more segregated than they were before Katrina. The seemingly inevitable cycles of death and rebirth might be woven into the fabric of New Orleans’ history and, as an incident like the BP oil spill highlighted, there is little doubt that a city in such a ‘lousy place’ is likely to face hard times again. But as the process of rebuilding continues, this might at least be the moment to lay some ghosts to rest. Thomas Ruys Smith, History Today",
        "id": 37,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What are we told about the “Mozart effect”?",
        "options": [
            "It is a great help to ambitious parents.",
            "It is an exaggeration of results from research on music. ",
            "Its importance has increased ever since it was established.",
            "Its negative sides have now started to appear."
        ],
        "article": "Music and Mind Nearly 20 years ago a small study advanced the notion that listening to Mozart’s Sonata for Two Pianos in D Major could boost mental functioning. It was not long before trademarked “Mozart effect” products appealed to neurotic parents aiming to put toddlers on the fast track to the Ivy League. Georgia’s governor even proposed giving every newborn there a classical CD or cassette. The evidence for Mozart therapy turned out to be flimsy, perhaps nonexist- ent, although the original study never claimed anything more than a temporary and limited effect. In recent years, however, neuroscientists have examined the benefits of a concerted effort to study and practice music, as opposed to playing a Mozart CD or a computer-based “brain fitness” game once in a while. Advanced monitoring tech- niques have enabled scientists to see what happens inside your head when you listen to your mother and actually practice the violin for an hour every afternoon. And they have found that music lessons can produce profound and lasting changes that enhance the general ability to learn. These results should disabuse public officials of the idea that music classes are a mere frill, ripe for discarding in the budget crises that constantly beset public schools.",
        "id": 38,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What is said about music and learning?",
        "options": [
            "Music should be part of regular classes instead of being a separate subject.",
            "Babies who listen to music every day seem to have more developed brains.",
            "Playing an instrument seems to have positive effects on learning.",
            "A connection between the two was established a long time ago."
        ],
        "article": "Music and Mind Nearly 20 years ago a small study advanced the notion that listening to Mozart’s Sonata for Two Pianos in D Major could boost mental functioning. It was not long before trademarked “Mozart effect” products appealed to neurotic parents aiming to put toddlers on the fast track to the Ivy League. Georgia’s governor even proposed giving every newborn there a classical CD or cassette. The evidence for Mozart therapy turned out to be flimsy, perhaps nonexist- ent, although the original study never claimed anything more than a temporary and limited effect. In recent years, however, neuroscientists have examined the benefits of a concerted effort to study and practice music, as opposed to playing a Mozart CD or a computer-based “brain fitness” game once in a while. Advanced monitoring tech- niques have enabled scientists to see what happens inside your head when you listen to your mother and actually practice the violin for an hour every afternoon. And they have found that music lessons can produce profound and lasting changes that enhance the general ability to learn. These results should disabuse public officials of the idea that music classes are a mere frill, ripe for discarding in the budget crises that constantly beset public schools.",
        "id": 39,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What is said about the people described in relation to the impostor phenomenon?",
        "options": [
            "Cheating is one of their most prominent characteristics.",
            "They do not experience the world as it actually is. ",
            "Deceptive behavior is a common trait among such people.",
            "Their dubious behavior seems to be prompted by research."
        ],
        "article": "The Impostor Phenomenon The impostor phenomenon is defined as the mistaken feeling that one’s successes are unearned and that at any moment the charade could end. The emphasis is on “mistaken” – these people are not really frauds. Or are they? Researchers asked this question recently. They studied how often self-defined impostors engaged in plagiarism and other types of dishonest behavior. Their results supported the original idea that the fraudulence is all in their head; the supposed impostors reported on a survey that they cheated less often than control subjects who did not have impostor feelings.",
        "id": 40,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What is implied about the Shona people in the first two paragraphs?",
        "options": [
            "The majority of them now work and live in the big cities of Zimbabwe. ",
            "They used to make a living as miners in ancient times.",
            "Their brilliant sculpture is a relatively modern phenomenon.",
            "Few of them can now remember their old way of life."
        ],
        "article": "Shona Sculpture In Zimbabwe, the Shona people were traditionally known as “the People of the Mist”, since they inhabited the mistshrouded Inyanga Mountains, from whose stone their descendants have been creating extraordinary sculpture over the past 55 years or so. The Shona are the oldest and also the largest ethnic group in the country, and the legendary guardians of King Solomon’s mines. Despite the fact that many of them are now urbanised, the essence of their misty land and its spiritual world is deep within their psyche and still relevant. In fact, the belief is that each rock on the Inyanga slopes, wonderfully varied in colour, contains a spirit unique to that stone. As an exhibition review in a London newspaper put it recently: “Shona sculpture is an art movement which emerged in Zimbabwe in the 1960s and has now been hailed on the international art scene. It is sculpture of world quality, extracting the individual spirit of the stone.” Potent traditions of spiritual connection and myth may well link contemporary Zimbabwean sculpture to steatite carvings, particularly of birds, created at Great Zimbabwe, the imposing city structure built of stone circa 1500 AD, and surrounded by massive walls. From long past, the Shona have been a gentle, deeply reflective people, mystically inclined and armed with enduring patience and resourcefulness. Even in warfare, they relied on traditional religious systems for power, defeating Portuguese invaders in 1600. The key to Shona life is the family – women, men and their children, as well as their ancestors. In the past a member of the family was a muvesi or carver, empowered by a Mudzimu, a special ancestral spirit who manifested in dreams. The muvesi created household items with great artistry in wood and clay, though not usually stone, as a way of recounting what he had seen in a dream. This heritage is embedded in the work of modern Shona sculptors, though theirs is clearly not for functional purposes. The present art movement surged into public consciousness in the early 1960s with the encouragement of Frank McEwen, the first director of the National Gallery in Harare. He quickly realised the inherent sculptural brilliance of the Shona, enabling sculptors to form a workshop at the gallery, helping to sell their work there, and fairly soon to exhibit it abroad. Many exhibitions at global venues have followed, as far afield as Japan. In 1966, a second European, Tom Blomefield, made available land on which he had been farming, which had a vast deposit of serpentine stone. This enabled a group of sculptural stars to shine. Initially, Blomefield encouraged them by supplying them with tools, shelter and food. All the sculpture produced was transported 100 miles to Harare where it was successfully sold, enabling the artists to become independent. Now, a second generation of urban artists has gained recognition, based in Harare or establishing themselves in Europe. Most Shona artists have no formal art training, but were taught by relatives or a master sculptor. Brothers Gedion and Collen were instructed by their father, Claud Nyanhongo. Gedion says: “I started making sculptures before I knew my name.” He has now achieved international success, with studios in Zimbabwe and the USA. His work explores social issues such as unemployment, as well as human love and the spiritual power it can provide. “I want to make people happy and to promote peace,” he adds. Juliet Highet, New African",
        "id": 36,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What, according to the text, is the key motivation for Shona sculpture?",
        "options": [
            "The commercial potential of African sculpture in the western art world. ",
            "The religious idea that rocks are inhabited by specific spiritual beings. ",
            "The striking colours and shapes of many rocks in the Inyanga Mountains.",
            "The amazing individual creativity of present-day Zimbabwean artists. "
        ],
        "article": "Shona Sculpture In Zimbabwe, the Shona people were traditionally known as “the People of the Mist”, since they inhabited the mistshrouded Inyanga Mountains, from whose stone their descendants have been creating extraordinary sculpture over the past 55 years or so. The Shona are the oldest and also the largest ethnic group in the country, and the legendary guardians of King Solomon’s mines. Despite the fact that many of them are now urbanised, the essence of their misty land and its spiritual world is deep within their psyche and still relevant. In fact, the belief is that each rock on the Inyanga slopes, wonderfully varied in colour, contains a spirit unique to that stone. As an exhibition review in a London newspaper put it recently: “Shona sculpture is an art movement which emerged in Zimbabwe in the 1960s and has now been hailed on the international art scene. It is sculpture of world quality, extracting the individual spirit of the stone.” Potent traditions of spiritual connection and myth may well link contemporary Zimbabwean sculpture to steatite carvings, particularly of birds, created at Great Zimbabwe, the imposing city structure built of stone circa 1500 AD, and surrounded by massive walls. From long past, the Shona have been a gentle, deeply reflective people, mystically inclined and armed with enduring patience and resourcefulness. Even in warfare, they relied on traditional religious systems for power, defeating Portuguese invaders in 1600. The key to Shona life is the family – women, men and their children, as well as their ancestors. In the past a member of the family was a muvesi or carver, empowered by a Mudzimu, a special ancestral spirit who manifested in dreams. The muvesi created household items with great artistry in wood and clay, though not usually stone, as a way of recounting what he had seen in a dream. This heritage is embedded in the work of modern Shona sculptors, though theirs is clearly not for functional purposes. The present art movement surged into public consciousness in the early 1960s with the encouragement of Frank McEwen, the first director of the National Gallery in Harare. He quickly realised the inherent sculptural brilliance of the Shona, enabling sculptors to form a workshop at the gallery, helping to sell their work there, and fairly soon to exhibit it abroad. Many exhibitions at global venues have followed, as far afield as Japan. In 1966, a second European, Tom Blomefield, made available land on which he had been farming, which had a vast deposit of serpentine stone. This enabled a group of sculptural stars to shine. Initially, Blomefield encouraged them by supplying them with tools, shelter and food. All the sculpture produced was transported 100 miles to Harare where it was successfully sold, enabling the artists to become independent. Now, a second generation of urban artists has gained recognition, based in Harare or establishing themselves in Europe. Most Shona artists have no formal art training, but were taught by relatives or a master sculptor. Brothers Gedion and Collen were instructed by their father, Claud Nyanhongo. Gedion says: “I started making sculptures before I knew my name.” He has now achieved international success, with studios in Zimbabwe and the USA. His work explores social issues such as unemployment, as well as human love and the spiritual power it can provide. “I want to make people happy and to promote peace,” he adds. Juliet Highet, New African",
        "id": 37,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "Which of the following statements is most in line with the text?",
        "options": [
            "Modern Shona sculpture seems to be related to other artistic expressions from older times. ",
            "Sculptural ornamentation was a prominent feature of Great Zimbabwe. ",
            "Traditional images of mysterious beasts are a permanent source of inspiration for Shona sculptors. ",
            "The mythical city of Great Zimbabwe was full of impressive bird statues. "
        ],
        "article": "Shona Sculpture In Zimbabwe, the Shona people were traditionally known as “the People of the Mist”, since they inhabited the mistshrouded Inyanga Mountains, from whose stone their descendants have been creating extraordinary sculpture over the past 55 years or so. The Shona are the oldest and also the largest ethnic group in the country, and the legendary guardians of King Solomon’s mines. Despite the fact that many of them are now urbanised, the essence of their misty land and its spiritual world is deep within their psyche and still relevant. In fact, the belief is that each rock on the Inyanga slopes, wonderfully varied in colour, contains a spirit unique to that stone. As an exhibition review in a London newspaper put it recently: “Shona sculpture is an art movement which emerged in Zimbabwe in the 1960s and has now been hailed on the international art scene. It is sculpture of world quality, extracting the individual spirit of the stone.” Potent traditions of spiritual connection and myth may well link contemporary Zimbabwean sculpture to steatite carvings, particularly of birds, created at Great Zimbabwe, the imposing city structure built of stone circa 1500 AD, and surrounded by massive walls. From long past, the Shona have been a gentle, deeply reflective people, mystically inclined and armed with enduring patience and resourcefulness. Even in warfare, they relied on traditional religious systems for power, defeating Portuguese invaders in 1600. The key to Shona life is the family – women, men and their children, as well as their ancestors. In the past a member of the family was a muvesi or carver, empowered by a Mudzimu, a special ancestral spirit who manifested in dreams. The muvesi created household items with great artistry in wood and clay, though not usually stone, as a way of recounting what he had seen in a dream. This heritage is embedded in the work of modern Shona sculptors, though theirs is clearly not for functional purposes. The present art movement surged into public consciousness in the early 1960s with the encouragement of Frank McEwen, the first director of the National Gallery in Harare. He quickly realised the inherent sculptural brilliance of the Shona, enabling sculptors to form a workshop at the gallery, helping to sell their work there, and fairly soon to exhibit it abroad. Many exhibitions at global venues have followed, as far afield as Japan. In 1966, a second European, Tom Blomefield, made available land on which he had been farming, which had a vast deposit of serpentine stone. This enabled a group of sculptural stars to shine. Initially, Blomefield encouraged them by supplying them with tools, shelter and food. All the sculpture produced was transported 100 miles to Harare where it was successfully sold, enabling the artists to become independent. Now, a second generation of urban artists has gained recognition, based in Harare or establishing themselves in Europe. Most Shona artists have no formal art training, but were taught by relatives or a master sculptor. Brothers Gedion and Collen were instructed by their father, Claud Nyanhongo. Gedion says: “I started making sculptures before I knew my name.” He has now achieved international success, with studios in Zimbabwe and the USA. His work explores social issues such as unemployment, as well as human love and the spiritual power it can provide. “I want to make people happy and to promote peace,” he adds. Juliet Highet, New African",
        "id": 38,
        "answer": "A",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What are we told about the muvesis in comparison with present-day Shona artists?",
        "options": [
            "Today’s Shona sculpture has further developed the practical aims of the muvesis.",
            "The dream element of today’s Shona sculpture was largely missing from the objects created by the muvesis.",
            "Basically the same materials were used by the muvesis as by recent Shona sculptors.",
            "The artistic work of the muvesis was more close ly related to everyday life than that of modern Shona sculptors"
        ],
        "article": "Shona Sculpture In Zimbabwe, the Shona people were traditionally known as “the People of the Mist”, since they inhabited the mistshrouded Inyanga Mountains, from whose stone their descendants have been creating extraordinary sculpture over the past 55 years or so. The Shona are the oldest and also the largest ethnic group in the country, and the legendary guardians of King Solomon’s mines. Despite the fact that many of them are now urbanised, the essence of their misty land and its spiritual world is deep within their psyche and still relevant. In fact, the belief is that each rock on the Inyanga slopes, wonderfully varied in colour, contains a spirit unique to that stone. As an exhibition review in a London newspaper put it recently: “Shona sculpture is an art movement which emerged in Zimbabwe in the 1960s and has now been hailed on the international art scene. It is sculpture of world quality, extracting the individual spirit of the stone.” Potent traditions of spiritual connection and myth may well link contemporary Zimbabwean sculpture to steatite carvings, particularly of birds, created at Great Zimbabwe, the imposing city structure built of stone circa 1500 AD, and surrounded by massive walls. From long past, the Shona have been a gentle, deeply reflective people, mystically inclined and armed with enduring patience and resourcefulness. Even in warfare, they relied on traditional religious systems for power, defeating Portuguese invaders in 1600. The key to Shona life is the family – women, men and their children, as well as their ancestors. In the past a member of the family was a muvesi or carver, empowered by a Mudzimu, a special ancestral spirit who manifested in dreams. The muvesi created household items with great artistry in wood and clay, though not usually stone, as a way of recounting what he had seen in a dream. This heritage is embedded in the work of modern Shona sculptors, though theirs is clearly not for functional purposes. The present art movement surged into public consciousness in the early 1960s with the encouragement of Frank McEwen, the first director of the National Gallery in Harare. He quickly realised the inherent sculptural brilliance of the Shona, enabling sculptors to form a workshop at the gallery, helping to sell their work there, and fairly soon to exhibit it abroad. Many exhibitions at global venues have followed, as far afield as Japan. In 1966, a second European, Tom Blomefield, made available land on which he had been farming, which had a vast deposit of serpentine stone. This enabled a group of sculptural stars to shine. Initially, Blomefield encouraged them by supplying them with tools, shelter and food. All the sculpture produced was transported 100 miles to Harare where it was successfully sold, enabling the artists to become independent. Now, a second generation of urban artists has gained recognition, based in Harare or establishing themselves in Europe. Most Shona artists have no formal art training, but were taught by relatives or a master sculptor. Brothers Gedion and Collen were instructed by their father, Claud Nyanhongo. Gedion says: “I started making sculptures before I knew my name.” He has now achieved international success, with studios in Zimbabwe and the USA. His work explores social issues such as unemployment, as well as human love and the spiritual power it can provide. “I want to make people happy and to promote peace,” he adds. Juliet Highet, New African",
        "id": 39,
        "answer": "D",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What are we told about Tom Blomefield’s relationship to certain Shona artists?",
        "options": [
            "He made a good profit out of the high-quality sculpture produced on his land.",
            "His ideas about stone sculpture lay behind much of their later success.",
            "He provided all kinds of basic material support for their artistic work.",
            "His connections with the art world were key to their international careers."
        ],
        "article": "Shona Sculpture In Zimbabwe, the Shona people were traditionally known as “the People of the Mist”, since they inhabited the mistshrouded Inyanga Mountains, from whose stone their descendants have been creating extraordinary sculpture over the past 55 years or so. The Shona are the oldest and also the largest ethnic group in the country, and the legendary guardians of King Solomon’s mines. Despite the fact that many of them are now urbanised, the essence of their misty land and its spiritual world is deep within their psyche and still relevant. In fact, the belief is that each rock on the Inyanga slopes, wonderfully varied in colour, contains a spirit unique to that stone. As an exhibition review in a London newspaper put it recently: “Shona sculpture is an art movement which emerged in Zimbabwe in the 1960s and has now been hailed on the international art scene. It is sculpture of world quality, extracting the individual spirit of the stone.” Potent traditions of spiritual connection and myth may well link contemporary Zimbabwean sculpture to steatite carvings, particularly of birds, created at Great Zimbabwe, the imposing city structure built of stone circa 1500 AD, and surrounded by massive walls. From long past, the Shona have been a gentle, deeply reflective people, mystically inclined and armed with enduring patience and resourcefulness. Even in warfare, they relied on traditional religious systems for power, defeating Portuguese invaders in 1600. The key to Shona life is the family – women, men and their children, as well as their ancestors. In the past a member of the family was a muvesi or carver, empowered by a Mudzimu, a special ancestral spirit who manifested in dreams. The muvesi created household items with great artistry in wood and clay, though not usually stone, as a way of recounting what he had seen in a dream. This heritage is embedded in the work of modern Shona sculptors, though theirs is clearly not for functional purposes. The present art movement surged into public consciousness in the early 1960s with the encouragement of Frank McEwen, the first director of the National Gallery in Harare. He quickly realised the inherent sculptural brilliance of the Shona, enabling sculptors to form a workshop at the gallery, helping to sell their work there, and fairly soon to exhibit it abroad. Many exhibitions at global venues have followed, as far afield as Japan. In 1966, a second European, Tom Blomefield, made available land on which he had been farming, which had a vast deposit of serpentine stone. This enabled a group of sculptural stars to shine. Initially, Blomefield encouraged them by supplying them with tools, shelter and food. All the sculpture produced was transported 100 miles to Harare where it was successfully sold, enabling the artists to become independent. Now, a second generation of urban artists has gained recognition, based in Harare or establishing themselves in Europe. Most Shona artists have no formal art training, but were taught by relatives or a master sculptor. Brothers Gedion and Collen were instructed by their father, Claud Nyanhongo. Gedion says: “I started making sculptures before I knew my name.” He has now achieved international success, with studios in Zimbabwe and the USA. His work explores social issues such as unemployment, as well as human love and the spiritual power it can provide. “I want to make people happy and to promote peace,” he adds. Juliet Highet, New African",
        "id": 40,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2016"
    },
    {
        "question": "What was shown in Prof. Mischel’s original “marshmallow test”?",
        "options": [
            "Young children had more difficulty waiting for a reward than more mature ones.",
            "The children’s results were basically determined by the test situation at hand.",
            "The age of the children involved was largely irrelevant for the outcome of the study.",
            "Children in general proved less able than adults to resist immediate temptation."
        ],
        "article": "The great Russian writer Leo Tolstoy was a man of ferocious complexity and contradiction, of a scope that Walter Mischel could appreciate. Mischel, a professor of psychology at Columbia University, is a towering figure in personality research, whose first popular book, The Marshmallow Test, summarizes his decades of work on self-control. It is also thanks to Mischel that we consider the degree to which situation dictates behavior, and the fact that willpower – and much more – is only as predictable as the context in which it operates. His so-called “marshmallow test,” first conducted at Stanford University in the late 1960s, monitors the ability of a child to resist a treat when left alone with it. Mischel’s subjects understood that they could eat the treat immediately if they rang a bell to summon the researcher, but they’d receive two treats if they could await the researcher’s return. Four-year-old paragons of executive function waited up to 15 minutes; school-age kids with “high delay skills” waited so long that Mischel himself could no longer stand to watch their heroic juvenile abnegation. The test is noteworthy because long-term follow-up studies show that a child’s ability to delay reward correlates with academic success and adult income, as well as with affective powers such as the ability to tolerate stress and rejection. The “cooling” mental transformations that help a kid wait agonizing minutes to consume a marshmallow – imagining the candy is a cloud, putting a frame around it to make it abstract, etc. – are the bedrock processes that allow that same person to stay future-focused enough to earn a college degree or to reliably invest in a long-term pension plan. Those with the ability to delay have a well-oiled prefrontal cortex that successfully regulates their abstract thinking as well as impulse control. Mainstream interest in Mischel’s work reflects our culture’s obsession with executive function or its lack (ADHD). We’re in the midst of a cognitive gold rush on self-control and grit, perhaps the two most valorized skills of our young century. Ironically, Mischel’s studies of personality, which are legendary among psychologists but largely unknown to the public, suggest that there are natural checks on self-regulation. Self-control, like most behaviors, is radically contingent. Character and willpower bend depending on the environment and the individual’s level of motivation. Leo Tolstoy was a maelstrom of lapses in his personal life. He fought incessantly with his wife. He was critical and demanding of his family even as he preached a love of humanity. This may surprise some people, but it would not surprise Mischel. He argues that the key to exercising self-control and to understanding why people appear to behave erratically resides in the slimmest of phrases: if–then. For example, the degree of conscientiousness we bring to a task at our job does not predict how conscientious we’ll be about bill paying or interacting with loved ones when we arrive home. However, it is predictive of how conscientious we’ll continue to be in work-related endeavors. Mischel proposes that personality inscribes itself in if–then behavioral patterns, which “characterize most people when their behavior is closely examined.” Further, “the behavioral signature of personality specifies what the individual does predictably if particular situational triggers occur. These behavioral triggers have been found with adults as well as children and for everything from conscientiousness and sociability to anxiety and stress.” Perhaps an omniscient biographer could map the intricate if–then patterns in Tolstoy’s life, as scattered as constellations in the night sky but ultimately just as predictable. We are, each of us, that complicated. Mischel has said that he is most interested in the subjects who “failed” the marshmallow test but went on to succeed in life. There’s no way to know whether Tolstoy would fit the bill, but if so, Mischel would have an idea why.",
        "id": 36,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "Why, in particular, is the marshmallow test of wider psychological interest?",
        "options": [
            "When applied to adults, its results tend to be more or less identical to those of young children.",
            "It has suggested some unexpected perspectives on basic brain dysfunction.",
            "It has demonstrated that the same kind of under­ lying reasoning explains a person’s behavior regardless of age.",
            "Recent research has indicated its relevance for being able to wait without stress."
        ],
        "article": "The great Russian writer Leo Tolstoy was a man of ferocious complexity and contradiction, of a scope that Walter Mischel could appreciate. Mischel, a professor of psychology at Columbia University, is a towering figure in personality research, whose first popular book, The Marshmallow Test, summarizes his decades of work on self-control. It is also thanks to Mischel that we consider the degree to which situation dictates behavior, and the fact that willpower – and much more – is only as predictable as the context in which it operates. His so-called “marshmallow test,” first conducted at Stanford University in the late 1960s, monitors the ability of a child to resist a treat when left alone with it. Mischel’s subjects understood that they could eat the treat immediately if they rang a bell to summon the researcher, but they’d receive two treats if they could await the researcher’s return. Four-year-old paragons of executive function waited up to 15 minutes; school-age kids with “high delay skills” waited so long that Mischel himself could no longer stand to watch their heroic juvenile abnegation. The test is noteworthy because long-term follow-up studies show that a child’s ability to delay reward correlates with academic success and adult income, as well as with affective powers such as the ability to tolerate stress and rejection. The “cooling” mental transformations that help a kid wait agonizing minutes to consume a marshmallow – imagining the candy is a cloud, putting a frame around it to make it abstract, etc. – are the bedrock processes that allow that same person to stay future-focused enough to earn a college degree or to reliably invest in a long-term pension plan. Those with the ability to delay have a well-oiled prefrontal cortex that successfully regulates their abstract thinking as well as impulse control. Mainstream interest in Mischel’s work reflects our culture’s obsession with executive function or its lack (ADHD). We’re in the midst of a cognitive gold rush on self-control and grit, perhaps the two most valorized skills of our young century. Ironically, Mischel’s studies of personality, which are legendary among psychologists but largely unknown to the public, suggest that there are natural checks on self-regulation. Self-control, like most behaviors, is radically contingent. Character and willpower bend depending on the environment and the individual’s level of motivation. Leo Tolstoy was a maelstrom of lapses in his personal life. He fought incessantly with his wife. He was critical and demanding of his family even as he preached a love of humanity. This may surprise some people, but it would not surprise Mischel. He argues that the key to exercising self-control and to understanding why people appear to behave erratically resides in the slimmest of phrases: if–then. For example, the degree of conscientiousness we bring to a task at our job does not predict how conscientious we’ll be about bill paying or interacting with loved ones when we arrive home. However, it is predictive of how conscientious we’ll continue to be in work-related endeavors. Mischel proposes that personality inscribes itself in if–then behavioral patterns, which “characterize most people when their behavior is closely examined.” Further, “the behavioral signature of personality specifies what the individual does predictably if particular situational triggers occur. These behavioral triggers have been found with adults as well as children and for everything from conscientiousness and sociability to anxiety and stress.” Perhaps an omniscient biographer could map the intricate if–then patterns in Tolstoy’s life, as scattered as constellations in the night sky but ultimately just as predictable. We are, each of us, that complicated. Mischel has said that he is most interested in the subjects who “failed” the marshmallow test but went on to succeed in life. There’s no way to know whether Tolstoy would fit the bill, but if so, Mischel would have an idea why.",
        "id": 37,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "Why is Leo Tolstoy mentioned?",
        "options": [
            "As a good example of the way many writers’ minds seem to function.",
            "As an illustration of the conflict between person al ideals and actual behavior.",
            "As an argument that self­control is not a well­ defined psychological concept.",
            "As a suitable case for treatment by behavioral therapy."
        ],
        "article": "The great Russian writer Leo Tolstoy was a man of ferocious complexity and contradiction, of a scope that Walter Mischel could appreciate. Mischel, a professor of psychology at Columbia University, is a towering figure in personality research, whose first popular book, The Marshmallow Test, summarizes his decades of work on self-control. It is also thanks to Mischel that we consider the degree to which situation dictates behavior, and the fact that willpower – and much more – is only as predictable as the context in which it operates. His so-called “marshmallow test,” first conducted at Stanford University in the late 1960s, monitors the ability of a child to resist a treat when left alone with it. Mischel’s subjects understood that they could eat the treat immediately if they rang a bell to summon the researcher, but they’d receive two treats if they could await the researcher’s return. Four-year-old paragons of executive function waited up to 15 minutes; school-age kids with “high delay skills” waited so long that Mischel himself could no longer stand to watch their heroic juvenile abnegation. The test is noteworthy because long-term follow-up studies show that a child’s ability to delay reward correlates with academic success and adult income, as well as with affective powers such as the ability to tolerate stress and rejection. The “cooling” mental transformations that help a kid wait agonizing minutes to consume a marshmallow – imagining the candy is a cloud, putting a frame around it to make it abstract, etc. – are the bedrock processes that allow that same person to stay future-focused enough to earn a college degree or to reliably invest in a long-term pension plan. Those with the ability to delay have a well-oiled prefrontal cortex that successfully regulates their abstract thinking as well as impulse control. Mainstream interest in Mischel’s work reflects our culture’s obsession with executive function or its lack (ADHD). We’re in the midst of a cognitive gold rush on self-control and grit, perhaps the two most valorized skills of our young century. Ironically, Mischel’s studies of personality, which are legendary among psychologists but largely unknown to the public, suggest that there are natural checks on self-regulation. Self-control, like most behaviors, is radically contingent. Character and willpower bend depending on the environment and the individual’s level of motivation. Leo Tolstoy was a maelstrom of lapses in his personal life. He fought incessantly with his wife. He was critical and demanding of his family even as he preached a love of humanity. This may surprise some people, but it would not surprise Mischel. He argues that the key to exercising self-control and to understanding why people appear to behave erratically resides in the slimmest of phrases: if–then. For example, the degree of conscientiousness we bring to a task at our job does not predict how conscientious we’ll be about bill paying or interacting with loved ones when we arrive home. However, it is predictive of how conscientious we’ll continue to be in work-related endeavors. Mischel proposes that personality inscribes itself in if–then behavioral patterns, which “characterize most people when their behavior is closely examined.” Further, “the behavioral signature of personality specifies what the individual does predictably if particular situational triggers occur. These behavioral triggers have been found with adults as well as children and for everything from conscientiousness and sociability to anxiety and stress.” Perhaps an omniscient biographer could map the intricate if–then patterns in Tolstoy’s life, as scattered as constellations in the night sky but ultimately just as predictable. We are, each of us, that complicated. Mischel has said that he is most interested in the subjects who “failed” the marshmallow test but went on to succeed in life. There’s no way to know whether Tolstoy would fit the bill, but if so, Mischel would have an idea why.",
        "id": 38,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What is the basic point of Prof. Mischel’s “if– then” argument?",
        "options": [
            "There is no link between personality profiles and observable behavior.",
            "Both child and adult personalities are guided by little­known behavioral norms.",
            "Personality may give rise to a certain behavior only under specific conditions.",
            "Neither personality nor behavior can explain the mystery of human action."
        ],
        "article": "The great Russian writer Leo Tolstoy was a man of ferocious complexity and contradiction, of a scope that Walter Mischel could appreciate. Mischel, a professor of psychology at Columbia University, is a towering figure in personality research, whose first popular book, The Marshmallow Test, summarizes his decades of work on self-control. It is also thanks to Mischel that we consider the degree to which situation dictates behavior, and the fact that willpower – and much more – is only as predictable as the context in which it operates. His so-called “marshmallow test,” first conducted at Stanford University in the late 1960s, monitors the ability of a child to resist a treat when left alone with it. Mischel’s subjects understood that they could eat the treat immediately if they rang a bell to summon the researcher, but they’d receive two treats if they could await the researcher’s return. Four-year-old paragons of executive function waited up to 15 minutes; school-age kids with “high delay skills” waited so long that Mischel himself could no longer stand to watch their heroic juvenile abnegation. The test is noteworthy because long-term follow-up studies show that a child’s ability to delay reward correlates with academic success and adult income, as well as with affective powers such as the ability to tolerate stress and rejection. The “cooling” mental transformations that help a kid wait agonizing minutes to consume a marshmallow – imagining the candy is a cloud, putting a frame around it to make it abstract, etc. – are the bedrock processes that allow that same person to stay future-focused enough to earn a college degree or to reliably invest in a long-term pension plan. Those with the ability to delay have a well-oiled prefrontal cortex that successfully regulates their abstract thinking as well as impulse control. Mainstream interest in Mischel’s work reflects our culture’s obsession with executive function or its lack (ADHD). We’re in the midst of a cognitive gold rush on self-control and grit, perhaps the two most valorized skills of our young century. Ironically, Mischel’s studies of personality, which are legendary among psychologists but largely unknown to the public, suggest that there are natural checks on self-regulation. Self-control, like most behaviors, is radically contingent. Character and willpower bend depending on the environment and the individual’s level of motivation. Leo Tolstoy was a maelstrom of lapses in his personal life. He fought incessantly with his wife. He was critical and demanding of his family even as he preached a love of humanity. This may surprise some people, but it would not surprise Mischel. He argues that the key to exercising self-control and to understanding why people appear to behave erratically resides in the slimmest of phrases: if–then. For example, the degree of conscientiousness we bring to a task at our job does not predict how conscientious we’ll be about bill paying or interacting with loved ones when we arrive home. However, it is predictive of how conscientious we’ll continue to be in work-related endeavors. Mischel proposes that personality inscribes itself in if–then behavioral patterns, which “characterize most people when their behavior is closely examined.” Further, “the behavioral signature of personality specifies what the individual does predictably if particular situational triggers occur. These behavioral triggers have been found with adults as well as children and for everything from conscientiousness and sociability to anxiety and stress.” Perhaps an omniscient biographer could map the intricate if–then patterns in Tolstoy’s life, as scattered as constellations in the night sky but ultimately just as predictable. We are, each of us, that complicated. Mischel has said that he is most interested in the subjects who “failed” the marshmallow test but went on to succeed in life. There’s no way to know whether Tolstoy would fit the bill, but if so, Mischel would have an idea why.",
        "id": 39,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What can be concluded about the marshmallow test from the closing paragraph?",
        "options": [
            "its potential for improvement will probably add to its current status.",
            "It may be seen as less convincing than some other personality tests.",
            "Its power of prediction is even more remarkable than first assumed.",
            "It gives an opportunity to study some less typical personality profiles."
        ],
        "article": "The great Russian writer Leo Tolstoy was a man of ferocious complexity and contradiction, of a scope that Walter Mischel could appreciate. Mischel, a professor of psychology at Columbia University, is a towering figure in personality research, whose first popular book, The Marshmallow Test, summarizes his decades of work on self-control. It is also thanks to Mischel that we consider the degree to which situation dictates behavior, and the fact that willpower – and much more – is only as predictable as the context in which it operates. His so-called “marshmallow test,” first conducted at Stanford University in the late 1960s, monitors the ability of a child to resist a treat when left alone with it. Mischel’s subjects understood that they could eat the treat immediately if they rang a bell to summon the researcher, but they’d receive two treats if they could await the researcher’s return. Four-year-old paragons of executive function waited up to 15 minutes; school-age kids with “high delay skills” waited so long that Mischel himself could no longer stand to watch their heroic juvenile abnegation. The test is noteworthy because long-term follow-up studies show that a child’s ability to delay reward correlates with academic success and adult income, as well as with affective powers such as the ability to tolerate stress and rejection. The “cooling” mental transformations that help a kid wait agonizing minutes to consume a marshmallow – imagining the candy is a cloud, putting a frame around it to make it abstract, etc. – are the bedrock processes that allow that same person to stay future-focused enough to earn a college degree or to reliably invest in a long-term pension plan. Those with the ability to delay have a well-oiled prefrontal cortex that successfully regulates their abstract thinking as well as impulse control. Mainstream interest in Mischel’s work reflects our culture’s obsession with executive function or its lack (ADHD). We’re in the midst of a cognitive gold rush on self-control and grit, perhaps the two most valorized skills of our young century. Ironically, Mischel’s studies of personality, which are legendary among psychologists but largely unknown to the public, suggest that there are natural checks on self-regulation. Self-control, like most behaviors, is radically contingent. Character and willpower bend depending on the environment and the individual’s level of motivation. Leo Tolstoy was a maelstrom of lapses in his personal life. He fought incessantly with his wife. He was critical and demanding of his family even as he preached a love of humanity. This may surprise some people, but it would not surprise Mischel. He argues that the key to exercising self-control and to understanding why people appear to behave erratically resides in the slimmest of phrases: if–then. For example, the degree of conscientiousness we bring to a task at our job does not predict how conscientious we’ll be about bill paying or interacting with loved ones when we arrive home. However, it is predictive of how conscientious we’ll continue to be in work-related endeavors. Mischel proposes that personality inscribes itself in if–then behavioral patterns, which “characterize most people when their behavior is closely examined.” Further, “the behavioral signature of personality specifies what the individual does predictably if particular situational triggers occur. These behavioral triggers have been found with adults as well as children and for everything from conscientiousness and sociability to anxiety and stress.” Perhaps an omniscient biographer could map the intricate if–then patterns in Tolstoy’s life, as scattered as constellations in the night sky but ultimately just as predictable. We are, each of us, that complicated. Mischel has said that he is most interested in the subjects who “failed” the marshmallow test but went on to succeed in life. There’s no way to know whether Tolstoy would fit the bill, but if so, Mischel would have an idea why.",
        "id": 40,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What is the main point here?",
        "options": [
            "There is an urgent need to reform higher educa­ tion in many countries.",
            "More money for the education of young children will pay off later on.",
            "The number of well educated, skilled workers seems to be increasing.",
            "More money to be spent equally on all kinds of education is necessary."
        ],
        "article": "Education is key to increased economic equality. To get workers’ paychecks up, they have to possess better skills, yet many societies are not doing enough to improve schools and teachers and bolster job training. Enhancing access to good education isn’t just a matter of more money, however, but also how we spend that money. In a recent report, the Organization for Economic Cooperation and Development (OECD) recommended boosting investment in the early years of education, like preschool, since that improves a child’s academic performance over his or her entire career.",
        "id": 31,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What is the main point in this text?",
        "options": [
            "Immortality related dilemmas may appear whether eternal life is achieved or not.",
            "It is questionable whether human beings will become immortal.",
            "Philosophers quarrel about whether immortality would be a positive or a negative thing.",
            "Predicting whether humans will ever achieve immortality is impossible."
        ],
        "article": "Some scientists believe that one day technology will make it possible to achieve immortality by uploading our neural connections into robots’ bodies; others believe this is impossible. Regardless, legitimate philosophers are engaged in a debate over how such an eventuality would change our humanity. Their dialogue is important because even if the final goal falls short, human augmentation and improvements may raise similar issues. ",
        "id": 32,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What is the main point in the opening paragraph?",
        "options": [
            "The origins of moral religions have long been considered a mystery.",
            "Moral values provide a cornerstone for all reli­ gious beliefs.",
            "There are now competing views on the emer­ gence of moral religions.",
            "Morality can be seen as a side effect of religious thinking."
        ],
        "article": "About 2,500 years ago, something changed the way humans think. Within the span of two centuries, in three separate regions of Eurasia, spiritual movements emerged that would give rise to the world’s major moral religions, those preaching some combination of compassion, humility and asceticism. Scholars often attribute the rise of these moral religions – Buddhism, Judaism, Islam, Hinduism and Christianity included – to population growth, seeing morality as a necessary social stabilizer in increasingly large and volatile human communities. Yet findings from a recent study published in Current Biology point to a different factor: rising affluence. The authors investigated variables relating to political complexity and living standards. Affluence emerged as a major force in the rise of moral religion – in particular, access to energy. Across cultures, moral religions abruptly emerged when members of a population could reliably source 20,000 calories of energy a day, including food (for humans and livestock), fuel and raw materials. “This number appears to correspond with a certain peace of mind,” says lead author Nicolas Baumard, a research scientist at École Normale Supérieure in Paris. “Having a roof over your head, not feeling like the world is full of predators and enemies, knowing that you’ll have enough to eat tomorrow.” As Baumard points out, psychology research shows that affluence appears to influence our motivations and reward circuitry away from short-term gain to also considering the benefits of longterm strategy. In other words, with a steady energy supply, we had more time to cooperate, cultivate skills and consider consequences. Affluence also allowed more time for existential pondering: maybe we have some greater moral responsibility; perhaps life has a purpose. Baumard acknowledges that moral and ascetic qualities probably existed in humans before the major religions emphasized them. Other experts believe that the paper may not consider these inherent qualities seriously enough. Barbara King, an anthropologist at the college of William & Mary, argues that the study exaggerates the sharp transition to the moral belief systems. She suggests that a more gradual transition may have taken place – one that was perhaps nudged over the line by a reliable calorie count. “Anthropologists and psychologists have found deep roots of compassion and morality in other primates,” King explains. “I don’t see any reason to assume that cosmological morality and compassion were not important to earlier hunter-gatherer groups.” Bernard J. Crespi, an evolutionary biologist at Simon Fraser University in British Columbia, also cautions against Baumard’s claim: “The main idea in the article is fascinating, but the causal link between increasing affluence and religion remains to be established. Our work actually suggests that the authors might have their causality reversed – that religion itself drives increases in affluence via its effects on increased cooperation.” Still, Baumard’s findings point to a strong association between affluence and the emergence of moral religion, specifically. Plenty of ancient societies cooperated and had religious beliefs – the Maya, Sumerians and Egyptians among them. For the most part, however, none of these societies’ belief systems emphasized morality, or material and visceral restraint. And according to Baumard, members of these societies never had access to more than 15,000 calories a day. Whether cause or effect, morality, it seems, takes energy.",
        "id": 33,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What is said about the new study mentioned?",
        "options": [
            "Its main argument is based on the conflict between wealth and morality.",
            "It stresses the relevance of material conditions for the appearance of modern religions.",
            "Its reference to population statistics has sur­ prised many researchers.",
            "It is openly critical of previous attempts to explain the beginnings of modern religions."
        ],
        "article": "About 2,500 years ago, something changed the way humans think. Within the span of two centuries, in three separate regions of Eurasia, spiritual movements emerged that would give rise to the world’s major moral religions, those preaching some combination of compassion, humility and asceticism. Scholars often attribute the rise of these moral religions – Buddhism, Judaism, Islam, Hinduism and Christianity included – to population growth, seeing morality as a necessary social stabilizer in increasingly large and volatile human communities. Yet findings from a recent study published in Current Biology point to a different factor: rising affluence. The authors investigated variables relating to political complexity and living standards. Affluence emerged as a major force in the rise of moral religion – in particular, access to energy. Across cultures, moral religions abruptly emerged when members of a population could reliably source 20,000 calories of energy a day, including food (for humans and livestock), fuel and raw materials. “This number appears to correspond with a certain peace of mind,” says lead author Nicolas Baumard, a research scientist at École Normale Supérieure in Paris. “Having a roof over your head, not feeling like the world is full of predators and enemies, knowing that you’ll have enough to eat tomorrow.” As Baumard points out, psychology research shows that affluence appears to influence our motivations and reward circuitry away from short-term gain to also considering the benefits of longterm strategy. In other words, with a steady energy supply, we had more time to cooperate, cultivate skills and consider consequences. Affluence also allowed more time for existential pondering: maybe we have some greater moral responsibility; perhaps life has a purpose. Baumard acknowledges that moral and ascetic qualities probably existed in humans before the major religions emphasized them. Other experts believe that the paper may not consider these inherent qualities seriously enough. Barbara King, an anthropologist at the college of William & Mary, argues that the study exaggerates the sharp transition to the moral belief systems. She suggests that a more gradual transition may have taken place – one that was perhaps nudged over the line by a reliable calorie count. “Anthropologists and psychologists have found deep roots of compassion and morality in other primates,” King explains. “I don’t see any reason to assume that cosmological morality and compassion were not important to earlier hunter-gatherer groups.” Bernard J. Crespi, an evolutionary biologist at Simon Fraser University in British Columbia, also cautions against Baumard’s claim: “The main idea in the article is fascinating, but the causal link between increasing affluence and religion remains to be established. Our work actually suggests that the authors might have their causality reversed – that religion itself drives increases in affluence via its effects on increased cooperation.” Still, Baumard’s findings point to a strong association between affluence and the emergence of moral religion, specifically. Plenty of ancient societies cooperated and had religious beliefs – the Maya, Sumerians and Egyptians among them. For the most part, however, none of these societies’ belief systems emphasized morality, or material and visceral restraint. And according to Baumard, members of these societies never had access to more than 15,000 calories a day. Whether cause or effect, morality, it seems, takes energy.",
        "id": 34,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "How does Barbara King regard Nicolas Baumard’s research? According to King, …",
        "options": [
            " Baumard is unaware of moral thinking prior to modern religions. ",
            " Baumard’s calorie statistics do not stand up to close scrutiny.",
            " Baumard’s explanation is supported by other branches of research.",
            " Baumard overstates the abruptness of the appearance of morality. "
        ],
        "article": "About 2,500 years ago, something changed the way humans think. Within the span of two centuries, in three separate regions of Eurasia, spiritual movements emerged that would give rise to the world’s major moral religions, those preaching some combination of compassion, humility and asceticism. Scholars often attribute the rise of these moral religions – Buddhism, Judaism, Islam, Hinduism and Christianity included – to population growth, seeing morality as a necessary social stabilizer in increasingly large and volatile human communities. Yet findings from a recent study published in Current Biology point to a different factor: rising affluence. The authors investigated variables relating to political complexity and living standards. Affluence emerged as a major force in the rise of moral religion – in particular, access to energy. Across cultures, moral religions abruptly emerged when members of a population could reliably source 20,000 calories of energy a day, including food (for humans and livestock), fuel and raw materials. “This number appears to correspond with a certain peace of mind,” says lead author Nicolas Baumard, a research scientist at École Normale Supérieure in Paris. “Having a roof over your head, not feeling like the world is full of predators and enemies, knowing that you’ll have enough to eat tomorrow.” As Baumard points out, psychology research shows that affluence appears to influence our motivations and reward circuitry away from short-term gain to also considering the benefits of longterm strategy. In other words, with a steady energy supply, we had more time to cooperate, cultivate skills and consider consequences. Affluence also allowed more time for existential pondering: maybe we have some greater moral responsibility; perhaps life has a purpose. Baumard acknowledges that moral and ascetic qualities probably existed in humans before the major religions emphasized them. Other experts believe that the paper may not consider these inherent qualities seriously enough. Barbara King, an anthropologist at the college of William & Mary, argues that the study exaggerates the sharp transition to the moral belief systems. She suggests that a more gradual transition may have taken place – one that was perhaps nudged over the line by a reliable calorie count. “Anthropologists and psychologists have found deep roots of compassion and morality in other primates,” King explains. “I don’t see any reason to assume that cosmological morality and compassion were not important to earlier hunter-gatherer groups.” Bernard J. Crespi, an evolutionary biologist at Simon Fraser University in British Columbia, also cautions against Baumard’s claim: “The main idea in the article is fascinating, but the causal link between increasing affluence and religion remains to be established. Our work actually suggests that the authors might have their causality reversed – that religion itself drives increases in affluence via its effects on increased cooperation.” Still, Baumard’s findings point to a strong association between affluence and the emergence of moral religion, specifically. Plenty of ancient societies cooperated and had religious beliefs – the Maya, Sumerians and Egyptians among them. For the most part, however, none of these societies’ belief systems emphasized morality, or material and visceral restraint. And according to Baumard, members of these societies never had access to more than 15,000 calories a day. Whether cause or effect, morality, it seems, takes energy.",
        "id": 35,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "How does Bernard J. Crespi view Nicolas Baumard’s findings?",
        "options": [
            "Crespi thinks Baumard’s evidence for affluence giving rise to religion is insufficient.",
            "Crespi accepts Baumard’s account with only minor reservations.",
            "Crespi agrees that Baumard’s point about the role of affluence is of crucial importance.",
            "Crespi considers Baumard’s statistical data interesting but irrelevant."
        ],
        "article": "About 2,500 years ago, something changed the way humans think. Within the span of two centuries, in three separate regions of Eurasia, spiritual movements emerged that would give rise to the world’s major moral religions, those preaching some combination of compassion, humility and asceticism. Scholars often attribute the rise of these moral religions – Buddhism, Judaism, Islam, Hinduism and Christianity included – to population growth, seeing morality as a necessary social stabilizer in increasingly large and volatile human communities. Yet findings from a recent study published in Current Biology point to a different factor: rising affluence. The authors investigated variables relating to political complexity and living standards. Affluence emerged as a major force in the rise of moral religion – in particular, access to energy. Across cultures, moral religions abruptly emerged when members of a population could reliably source 20,000 calories of energy a day, including food (for humans and livestock), fuel and raw materials. “This number appears to correspond with a certain peace of mind,” says lead author Nicolas Baumard, a research scientist at École Normale Supérieure in Paris. “Having a roof over your head, not feeling like the world is full of predators and enemies, knowing that you’ll have enough to eat tomorrow.” As Baumard points out, psychology research shows that affluence appears to influence our motivations and reward circuitry away from short-term gain to also considering the benefits of longterm strategy. In other words, with a steady energy supply, we had more time to cooperate, cultivate skills and consider consequences. Affluence also allowed more time for existential pondering: maybe we have some greater moral responsibility; perhaps life has a purpose. Baumard acknowledges that moral and ascetic qualities probably existed in humans before the major religions emphasized them. Other experts believe that the paper may not consider these inherent qualities seriously enough. Barbara King, an anthropologist at the college of William & Mary, argues that the study exaggerates the sharp transition to the moral belief systems. She suggests that a more gradual transition may have taken place – one that was perhaps nudged over the line by a reliable calorie count. “Anthropologists and psychologists have found deep roots of compassion and morality in other primates,” King explains. “I don’t see any reason to assume that cosmological morality and compassion were not important to earlier hunter-gatherer groups.” Bernard J. Crespi, an evolutionary biologist at Simon Fraser University in British Columbia, also cautions against Baumard’s claim: “The main idea in the article is fascinating, but the causal link between increasing affluence and religion remains to be established. Our work actually suggests that the authors might have their causality reversed – that religion itself drives increases in affluence via its effects on increased cooperation.” Still, Baumard’s findings point to a strong association between affluence and the emergence of moral religion, specifically. Plenty of ancient societies cooperated and had religious beliefs – the Maya, Sumerians and Egyptians among them. For the most part, however, none of these societies’ belief systems emphasized morality, or material and visceral restraint. And according to Baumard, members of these societies never had access to more than 15,000 calories a day. Whether cause or effect, morality, it seems, takes energy.",
        "id": 36,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What is the writer’s overall attitude to Nicolas Baumard’s research?",
        "options": [
            "He is on the whole positive but has some serious objections.",
            "He is less convinced by its main argument than Bernard J. Crespi.",
            "He is clearly impressed but avoids taking a stand on a key issue.",
            "He is more sceptical about its conclusions than Barbara King."
        ],
        "article": "About 2,500 years ago, something changed the way humans think. Within the span of two centuries, in three separate regions of Eurasia, spiritual movements emerged that would give rise to the world’s major moral religions, those preaching some combination of compassion, humility and asceticism. Scholars often attribute the rise of these moral religions – Buddhism, Judaism, Islam, Hinduism and Christianity included – to population growth, seeing morality as a necessary social stabilizer in increasingly large and volatile human communities. Yet findings from a recent study published in Current Biology point to a different factor: rising affluence. The authors investigated variables relating to political complexity and living standards. Affluence emerged as a major force in the rise of moral religion – in particular, access to energy. Across cultures, moral religions abruptly emerged when members of a population could reliably source 20,000 calories of energy a day, including food (for humans and livestock), fuel and raw materials. “This number appears to correspond with a certain peace of mind,” says lead author Nicolas Baumard, a research scientist at École Normale Supérieure in Paris. “Having a roof over your head, not feeling like the world is full of predators and enemies, knowing that you’ll have enough to eat tomorrow.” As Baumard points out, psychology research shows that affluence appears to influence our motivations and reward circuitry away from short-term gain to also considering the benefits of longterm strategy. In other words, with a steady energy supply, we had more time to cooperate, cultivate skills and consider consequences. Affluence also allowed more time for existential pondering: maybe we have some greater moral responsibility; perhaps life has a purpose. Baumard acknowledges that moral and ascetic qualities probably existed in humans before the major religions emphasized them. Other experts believe that the paper may not consider these inherent qualities seriously enough. Barbara King, an anthropologist at the college of William & Mary, argues that the study exaggerates the sharp transition to the moral belief systems. She suggests that a more gradual transition may have taken place – one that was perhaps nudged over the line by a reliable calorie count. “Anthropologists and psychologists have found deep roots of compassion and morality in other primates,” King explains. “I don’t see any reason to assume that cosmological morality and compassion were not important to earlier hunter-gatherer groups.” Bernard J. Crespi, an evolutionary biologist at Simon Fraser University in British Columbia, also cautions against Baumard’s claim: “The main idea in the article is fascinating, but the causal link between increasing affluence and religion remains to be established. Our work actually suggests that the authors might have their causality reversed – that religion itself drives increases in affluence via its effects on increased cooperation.” Still, Baumard’s findings point to a strong association between affluence and the emergence of moral religion, specifically. Plenty of ancient societies cooperated and had religious beliefs – the Maya, Sumerians and Egyptians among them. For the most part, however, none of these societies’ belief systems emphasized morality, or material and visceral restraint. And according to Baumard, members of these societies never had access to more than 15,000 calories a day. Whether cause or effect, morality, it seems, takes energy.",
        "id": 37,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What is the main point here in relation to autistic people?",
        "options": [
            "They tend to express emotion more through their voices rather than by their faces.",
            "Their difficulties in distinguishing feelings may relate to what can be seen.",
            "They are generally better than non­autistic people at determining people’s emotions.",
            "Their visual deficits are less difficult to treat than their corresponding aural deficits."
        ],
        "article": "People with autism are often unable to judge when a face conveys emotions such as happiness or sadness, and many researchers take this as evidence that autism involves serious deficits in processing social information. Yet the voice, too, can provide emotional cues, and several recent studies indicate that when listening to voices, people with autism can actually recognize feelings and other traits of humanness as well as – or even better than – neurotypical people do. The studies suggest that at least for some subgroups of autistic people in certain situations, deficits in identifying emotions could be confined primarily to vision. ‘This is great news from a treatment perspective,’ says researcher Kevin Pelphrey at George Washington University. ‘It is much easier to help someone overcome an inability to read emotion from faces than it would be to treat a fundamental lack of understanding of emotion from all modalities.’",
        "id": 38,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "Which of the following phrases best characterizes China’s overall progress, as described in the text?",
        "options": [
            "One dimensional.",
            "Extremely varied.",
            "Environment friendly.",
            "Basically unethical."
        ],
        "article": "Once again, as a thousand years ago, China leads the world in many ways. For example, they are global leaders in genetic engineering, the Chinese government directing billions of dollars towards research into modifying genes of crops, animals and even humans. China also leads the world in mobile banking, spending on luxury goods, high-speed rail, car manufacture, foreign trade, the export of machinery and electronic products – and in the size of its art market. It has the planet’s largest number of broadband subscribers, and is ahead in both solar and wind industry, as well as in the use of methanol as an alternative fuel. It also leads the world in the growth of millionaires – and in poverty reduction.",
        "id": 39,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What is the main point here?",
        "options": [
            "The recognition of new disorders is often slowed down by bureaucratic obstacles.",
            "Attempts to spread the word about lesser-known disorders may be due to commercial interests.",
            "Restless legs syndrome is much less widespread than some studies have claimed.",
            "Production of medications is often delayed owing to lack of economic incentives."
        ],
        "article": "Glaxo Smith Kline made restless legs syndrome – a disorder characterized in part by the urge to move one’s legs at night – a household term in 2003, when it ran a vigorous awareness campaign for the disorder. The pharmaceutical company began by issuing press releases about presentations given at the 2003 annual meeting for the American Academy of Neurology, a professional society representing neurologists and neuroscientists. Two months later it distributed another press release, for a company-funded, as yet unpublished study, entitled “New survey reveals common yet under-recognized disorder – restless legs syndrome – is keeping Americans awake at night.” The press releases led to a slew of media coverage of the disorder; according to an analysis in 2006, one fifth of all articles written about it referred readers to the nonprofit Restless Legs Syndrome Foundation for more information, an organization that is heavily subsidized by Glaxo Smith Kline.",
        "id": 40,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2019"
    },
    {
        "question": "What is the main point here?",
        "options": [
            "Age-related changes in personality have previously been overstated. ",
            "Researchers have finally been able to understand how age shapes our personality.",
            "Age-related diseases affecting one’s personality appear to be on the increase.",
            "Researchers have found separate personality patterns among people under and over 30."
        ],
        "article": "Aging The process of aging remains as enigmatic as it is inevitable. And what knowledge we have gained over the years can sometimes be easily overturned. “For example, we used to believe that people became grumpy as they got older. But an analysis of long-term data from the Baltimore Longitudinal Study on Aging discovered that an adult’s personality generally doesn’t change much after age 30,\" says researcher Luigi Ferrucci. “In fact, people who are happy when they’re younger are likelier to be the same when they’re much older. \"It turns out “grumpy old men\" – or women, for that matter – may be gruff not because of age, but because they’re set in a personality shaped by their youth during times when war, poverty and other harsh experiences were more common. \"It also suggests that significant changes in personality may not be due to aging,\" says Ferrucci, \"but could be a sign of disease or dementia instead.\"",
        "id": 31,
        "answer": "A",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What is said here concerning executions in Britain?",
        "options": [
            "The death penalty was not abolished until the mid-19th century.",
            "Public executions stopped when the Victorians found less primitive forms of amusement.",
            "There is no consensus about the reason why public hangings finally ended. ",
            "Even upper-class people found pleasure watching public executions."
        ],
        "article": "Executions In past centuries, public executions of criminals had several purposes. They were a deterrent, a vengeful enactment of moral justice and a morbid form of entertainment. In the 19th century, however, many western nations began moving their gallows behind grey prison walls. Why this decision was taken is hotly debated, but it wasn’t due to dwindling public interest. On the contrary, crowds in Victorian Britain were often rowdy mobs; in 1868, the last man to be publicly executed in Britain was booed by 2,000 people as he swung from the gallows at Newgate. For campaigners like Charles Dickens, such furore was uncivilised and cruel, and his protests likely contributed to the change in the law.",
        "id": 32,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What is the main argument in the opening paragraph?",
        "options": [
            "People may differ a great deal concerning the amount of money they donate.",
            "Different churches favour different ways of collecting money from people.",
            "People tend to give away more money if they know they are being watched. ",
            "Social factors are relevant regarding people’s willingness to donate money."
        ],
        "article": "Doing Good When I was young, I would frequent two Catholic churches. One took its weekly collection by passing around a cloth bag, while at the other the congregation placed their offerings on a tray. I saw no reason for the variation, but an older churchgoer explained that it made a big difference to what people gave. People would put coins into a bag because others could only hear and not see that they had donated, whereas they would be more inclined to put notes onto a tray as a public display of their generosity. For an innocent young boy, it was a startling introduction to our endemic cynicism about charitable giving. You are seen as comically naïve if you believe that people give up their time or money purely out of the goodness of their own hearts. ‘What’s in it for them?’ is often the automatic response to signs of conspicuous altruism. Indeed, this reaction is no longer seen as cynical at all, but simply realistic. Good causes are increasingly trying to convince us that there’s at least as much in charity for us as there is for those we’re supposed to be helping. Even apparently heroically selfless lives can be reinterpreted as selfish quests for meaning, purpose or prestige. For instance, why are those who devote themselves to raising funds to help fight a disease so often people who have lost members of their own family to it? What are they fighting most, the disease or their own demons? In a world where pure goodness looks like fairytale nonsense, ‘do-gooders’ are brought crashing from their pedestals. Mother Teresa, once a byword for selflessness, has been described by Christopher Hitchens as a ‘friend of poverty rather than a friend of the poor’, who enjoyed her fame but did little to actually improve the lives of the poor of Calcutta. Are we right to be so suspicious of apparent acts of kindness? Cynicism about charity becomes inevitable once you demand that genuine altruism is never tainted by self-interest. But why should we ever think that in the first place? Consider two people who give the same amount to charity: one who gets pleasure from helping others, and one who begrudgingly gives out of duty. Which is the more morally praiseworthy? If charity shouldn’t benefit the benefactor, then we are forced to accept that the reluctant donor is superior. But surely it is the mark of the truly kind that doing good feels good and is something they want to do. There’s a kind of puritanism in the idea that charity must hurt to really count. The whole point of helping others is to make life better. But your life matters, too, so how can making it worse be better? From an objective point of view, altruism that helps both the giver and the recipient does more good than altruism that leaves the altruist worse off. Nor should we worry too much about motivation. People’s motives are invariably mixed, and if self-interest is among them, we shouldn’t worry, just as long as what they do genuinely helps others, too. Indeed, evolutionary psychologists would argue that the origins of the charitable impulse lie in the morally blind workings of natural selection. Put simply, groups of organisms that never co-operate or help each other are less successful at reproducing than those who do. The most successful replicators are those who help each other out, unless their apparent kindness is abused. So there is no contradiction or hypocrisy in giving that benefits the benefactor. Problems only arise if apparent altruism helps only the giver and perhaps even harms the recipient. And if you do good primarily to make yourself feel better, you can lose sight of whether ‘doing good’ really helps others at all. This is the danger with the generally welcome growth of ethical consumerism. Concern for the environment can easily become a form of eco-narcissism, where the main priority is to keep ourselves pure. But how altruistic is it to eat locally grown organic produce while refusing to spend money on food from countries that need our trade? How entitled are you to feel good about fitting a few low-energy light bulbs if you fly even once a year? And might your refusal to eat gene-modified foods be hampering a technology that could make feeding the world’s poor easier? The purpose of charity is to make life easier for everyone. Sacrificing some of your own self-interests for a substantial benefit to others is certainly laudable, and something too few of us are willing to do. But if we can help others while also benefiting ourselves, that is even better. When charity really can be a win-win deal, we would be foolish not to take it. Julian Baggini, Psychologies Magazine",
        "id": 33,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What are we told about goodness of heart as a human property?",
        "options": [
            "It seems to be viewed with a great deal of scepticism nowadays. ",
            "It has been proved to be an illusion that does not pass the test of reality.",
            "It can be seen as wishful thinking dressed up as religious dogma.",
            "It is a noble instinct that is becoming increasingly rare these days."
        ],
        "article": "Doing Good When I was young, I would frequent two Catholic churches. One took its weekly collection by passing around a cloth bag, while at the other the congregation placed their offerings on a tray. I saw no reason for the variation, but an older churchgoer explained that it made a big difference to what people gave. People would put coins into a bag because others could only hear and not see that they had donated, whereas they would be more inclined to put notes onto a tray as a public display of their generosity. For an innocent young boy, it was a startling introduction to our endemic cynicism about charitable giving. You are seen as comically naïve if you believe that people give up their time or money purely out of the goodness of their own hearts. ‘What’s in it for them?’ is often the automatic response to signs of conspicuous altruism. Indeed, this reaction is no longer seen as cynical at all, but simply realistic. Good causes are increasingly trying to convince us that there’s at least as much in charity for us as there is for those we’re supposed to be helping. Even apparently heroically selfless lives can be reinterpreted as selfish quests for meaning, purpose or prestige. For instance, why are those who devote themselves to raising funds to help fight a disease so often people who have lost members of their own family to it? What are they fighting most, the disease or their own demons? In a world where pure goodness looks like fairytale nonsense, ‘do-gooders’ are brought crashing from their pedestals. Mother Teresa, once a byword for selflessness, has been described by Christopher Hitchens as a ‘friend of poverty rather than a friend of the poor’, who enjoyed her fame but did little to actually improve the lives of the poor of Calcutta. Are we right to be so suspicious of apparent acts of kindness? Cynicism about charity becomes inevitable once you demand that genuine altruism is never tainted by self-interest. But why should we ever think that in the first place? Consider two people who give the same amount to charity: one who gets pleasure from helping others, and one who begrudgingly gives out of duty. Which is the more morally praiseworthy? If charity shouldn’t benefit the benefactor, then we are forced to accept that the reluctant donor is superior. But surely it is the mark of the truly kind that doing good feels good and is something they want to do. There’s a kind of puritanism in the idea that charity must hurt to really count. The whole point of helping others is to make life better. But your life matters, too, so how can making it worse be better? From an objective point of view, altruism that helps both the giver and the recipient does more good than altruism that leaves the altruist worse off. Nor should we worry too much about motivation. People’s motives are invariably mixed, and if self-interest is among them, we shouldn’t worry, just as long as what they do genuinely helps others, too. Indeed, evolutionary psychologists would argue that the origins of the charitable impulse lie in the morally blind workings of natural selection. Put simply, groups of organisms that never co-operate or help each other are less successful at reproducing than those who do. The most successful replicators are those who help each other out, unless their apparent kindness is abused. So there is no contradiction or hypocrisy in giving that benefits the benefactor. Problems only arise if apparent altruism helps only the giver and perhaps even harms the recipient. And if you do good primarily to make yourself feel better, you can lose sight of whether ‘doing good’ really helps others at all. This is the danger with the generally welcome growth of ethical consumerism. Concern for the environment can easily become a form of eco-narcissism, where the main priority is to keep ourselves pure. But how altruistic is it to eat locally grown organic produce while refusing to spend money on food from countries that need our trade? How entitled are you to feel good about fitting a few low-energy light bulbs if you fly even once a year? And might your refusal to eat gene-modified foods be hampering a technology that could make feeding the world’s poor easier? The purpose of charity is to make life easier for everyone. Sacrificing some of your own self-interests for a substantial benefit to others is certainly laudable, and something too few of us are willing to do. But if we can help others while also benefiting ourselves, that is even better. When charity really can be a win-win deal, we would be foolish not to take it. Julian Baggini, Psychologies Magazine",
        "id": 34,
        "answer": "A",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What, according to the writer, is especially problematic about discussions of charity?",
        "options": [
            "The notion that giving away money is an acceptable way of boosting one’s moral ego. ",
            "The fact that only wealthy people can afford to buy a clear conscience.",
            "The idea that giving money to those in need is politically unacceptable.",
            "The argument that only completely unselfish donations are truly ethical. "
        ],
        "article": "Doing Good When I was young, I would frequent two Catholic churches. One took its weekly collection by passing around a cloth bag, while at the other the congregation placed their offerings on a tray. I saw no reason for the variation, but an older churchgoer explained that it made a big difference to what people gave. People would put coins into a bag because others could only hear and not see that they had donated, whereas they would be more inclined to put notes onto a tray as a public display of their generosity. For an innocent young boy, it was a startling introduction to our endemic cynicism about charitable giving. You are seen as comically naïve if you believe that people give up their time or money purely out of the goodness of their own hearts. ‘What’s in it for them?’ is often the automatic response to signs of conspicuous altruism. Indeed, this reaction is no longer seen as cynical at all, but simply realistic. Good causes are increasingly trying to convince us that there’s at least as much in charity for us as there is for those we’re supposed to be helping. Even apparently heroically selfless lives can be reinterpreted as selfish quests for meaning, purpose or prestige. For instance, why are those who devote themselves to raising funds to help fight a disease so often people who have lost members of their own family to it? What are they fighting most, the disease or their own demons? In a world where pure goodness looks like fairytale nonsense, ‘do-gooders’ are brought crashing from their pedestals. Mother Teresa, once a byword for selflessness, has been described by Christopher Hitchens as a ‘friend of poverty rather than a friend of the poor’, who enjoyed her fame but did little to actually improve the lives of the poor of Calcutta. Are we right to be so suspicious of apparent acts of kindness? Cynicism about charity becomes inevitable once you demand that genuine altruism is never tainted by self-interest. But why should we ever think that in the first place? Consider two people who give the same amount to charity: one who gets pleasure from helping others, and one who begrudgingly gives out of duty. Which is the more morally praiseworthy? If charity shouldn’t benefit the benefactor, then we are forced to accept that the reluctant donor is superior. But surely it is the mark of the truly kind that doing good feels good and is something they want to do. There’s a kind of puritanism in the idea that charity must hurt to really count. The whole point of helping others is to make life better. But your life matters, too, so how can making it worse be better? From an objective point of view, altruism that helps both the giver and the recipient does more good than altruism that leaves the altruist worse off. Nor should we worry too much about motivation. People’s motives are invariably mixed, and if self-interest is among them, we shouldn’t worry, just as long as what they do genuinely helps others, too. Indeed, evolutionary psychologists would argue that the origins of the charitable impulse lie in the morally blind workings of natural selection. Put simply, groups of organisms that never co-operate or help each other are less successful at reproducing than those who do. The most successful replicators are those who help each other out, unless their apparent kindness is abused. So there is no contradiction or hypocrisy in giving that benefits the benefactor. Problems only arise if apparent altruism helps only the giver and perhaps even harms the recipient. And if you do good primarily to make yourself feel better, you can lose sight of whether ‘doing good’ really helps others at all. This is the danger with the generally welcome growth of ethical consumerism. Concern for the environment can easily become a form of eco-narcissism, where the main priority is to keep ourselves pure. But how altruistic is it to eat locally grown organic produce while refusing to spend money on food from countries that need our trade? How entitled are you to feel good about fitting a few low-energy light bulbs if you fly even once a year? And might your refusal to eat gene-modified foods be hampering a technology that could make feeding the world’s poor easier? The purpose of charity is to make life easier for everyone. Sacrificing some of your own self-interests for a substantial benefit to others is certainly laudable, and something too few of us are willing to do. But if we can help others while also benefiting ourselves, that is even better. When charity really can be a win-win deal, we would be foolish not to take it. Julian Baggini, Psychologies Magazine",
        "id": 35,
        "answer": "D",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "How can the author’s main view of charity best be summarized?",
        "options": [
            "Some of its psychological consequences are highly questionable.",
            "What counts is its effects, not the causes underlying donors’ will to help. ",
            "Its results so far demonstrate its severe practical limitations. ",
            "Regardless of its benefits, recipients risk getting long-term dependent on it. "
        ],
        "article": "Doing Good When I was young, I would frequent two Catholic churches. One took its weekly collection by passing around a cloth bag, while at the other the congregation placed their offerings on a tray. I saw no reason for the variation, but an older churchgoer explained that it made a big difference to what people gave. People would put coins into a bag because others could only hear and not see that they had donated, whereas they would be more inclined to put notes onto a tray as a public display of their generosity. For an innocent young boy, it was a startling introduction to our endemic cynicism about charitable giving. You are seen as comically naïve if you believe that people give up their time or money purely out of the goodness of their own hearts. ‘What’s in it for them?’ is often the automatic response to signs of conspicuous altruism. Indeed, this reaction is no longer seen as cynical at all, but simply realistic. Good causes are increasingly trying to convince us that there’s at least as much in charity for us as there is for those we’re supposed to be helping. Even apparently heroically selfless lives can be reinterpreted as selfish quests for meaning, purpose or prestige. For instance, why are those who devote themselves to raising funds to help fight a disease so often people who have lost members of their own family to it? What are they fighting most, the disease or their own demons? In a world where pure goodness looks like fairytale nonsense, ‘do-gooders’ are brought crashing from their pedestals. Mother Teresa, once a byword for selflessness, has been described by Christopher Hitchens as a ‘friend of poverty rather than a friend of the poor’, who enjoyed her fame but did little to actually improve the lives of the poor of Calcutta. Are we right to be so suspicious of apparent acts of kindness? Cynicism about charity becomes inevitable once you demand that genuine altruism is never tainted by self-interest. But why should we ever think that in the first place? Consider two people who give the same amount to charity: one who gets pleasure from helping others, and one who begrudgingly gives out of duty. Which is the more morally praiseworthy? If charity shouldn’t benefit the benefactor, then we are forced to accept that the reluctant donor is superior. But surely it is the mark of the truly kind that doing good feels good and is something they want to do. There’s a kind of puritanism in the idea that charity must hurt to really count. The whole point of helping others is to make life better. But your life matters, too, so how can making it worse be better? From an objective point of view, altruism that helps both the giver and the recipient does more good than altruism that leaves the altruist worse off. Nor should we worry too much about motivation. People’s motives are invariably mixed, and if self-interest is among them, we shouldn’t worry, just as long as what they do genuinely helps others, too. Indeed, evolutionary psychologists would argue that the origins of the charitable impulse lie in the morally blind workings of natural selection. Put simply, groups of organisms that never co-operate or help each other are less successful at reproducing than those who do. The most successful replicators are those who help each other out, unless their apparent kindness is abused. So there is no contradiction or hypocrisy in giving that benefits the benefactor. Problems only arise if apparent altruism helps only the giver and perhaps even harms the recipient. And if you do good primarily to make yourself feel better, you can lose sight of whether ‘doing good’ really helps others at all. This is the danger with the generally welcome growth of ethical consumerism. Concern for the environment can easily become a form of eco-narcissism, where the main priority is to keep ourselves pure. But how altruistic is it to eat locally grown organic produce while refusing to spend money on food from countries that need our trade? How entitled are you to feel good about fitting a few low-energy light bulbs if you fly even once a year? And might your refusal to eat gene-modified foods be hampering a technology that could make feeding the world’s poor easier? The purpose of charity is to make life easier for everyone. Sacrificing some of your own self-interests for a substantial benefit to others is certainly laudable, and something too few of us are willing to do. But if we can help others while also benefiting ourselves, that is even better. When charity really can be a win-win deal, we would be foolish not to take it. Julian Baggini, Psychologies Magazine",
        "id": 36,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "Which of the following words most clearly reflects the specific risk discussed by the author in relation to “ethical consumerism\"?",
        "options": [
            "Self-denial",
            "Self-protection",
            "Self-satisfaction",
            "Self-consciousness"
        ],
        "article": "Doing Good When I was young, I would frequent two Catholic churches. One took its weekly collection by passing around a cloth bag, while at the other the congregation placed their offerings on a tray. I saw no reason for the variation, but an older churchgoer explained that it made a big difference to what people gave. People would put coins into a bag because others could only hear and not see that they had donated, whereas they would be more inclined to put notes onto a tray as a public display of their generosity. For an innocent young boy, it was a startling introduction to our endemic cynicism about charitable giving. You are seen as comically naïve if you believe that people give up their time or money purely out of the goodness of their own hearts. ‘What’s in it for them?’ is often the automatic response to signs of conspicuous altruism. Indeed, this reaction is no longer seen as cynical at all, but simply realistic. Good causes are increasingly trying to convince us that there’s at least as much in charity for us as there is for those we’re supposed to be helping. Even apparently heroically selfless lives can be reinterpreted as selfish quests for meaning, purpose or prestige. For instance, why are those who devote themselves to raising funds to help fight a disease so often people who have lost members of their own family to it? What are they fighting most, the disease or their own demons? In a world where pure goodness looks like fairytale nonsense, ‘do-gooders’ are brought crashing from their pedestals. Mother Teresa, once a byword for selflessness, has been described by Christopher Hitchens as a ‘friend of poverty rather than a friend of the poor’, who enjoyed her fame but did little to actually improve the lives of the poor of Calcutta. Are we right to be so suspicious of apparent acts of kindness? Cynicism about charity becomes inevitable once you demand that genuine altruism is never tainted by self-interest. But why should we ever think that in the first place? Consider two people who give the same amount to charity: one who gets pleasure from helping others, and one who begrudgingly gives out of duty. Which is the more morally praiseworthy? If charity shouldn’t benefit the benefactor, then we are forced to accept that the reluctant donor is superior. But surely it is the mark of the truly kind that doing good feels good and is something they want to do. There’s a kind of puritanism in the idea that charity must hurt to really count. The whole point of helping others is to make life better. But your life matters, too, so how can making it worse be better? From an objective point of view, altruism that helps both the giver and the recipient does more good than altruism that leaves the altruist worse off. Nor should we worry too much about motivation. People’s motives are invariably mixed, and if self-interest is among them, we shouldn’t worry, just as long as what they do genuinely helps others, too. Indeed, evolutionary psychologists would argue that the origins of the charitable impulse lie in the morally blind workings of natural selection. Put simply, groups of organisms that never co-operate or help each other are less successful at reproducing than those who do. The most successful replicators are those who help each other out, unless their apparent kindness is abused. So there is no contradiction or hypocrisy in giving that benefits the benefactor. Problems only arise if apparent altruism helps only the giver and perhaps even harms the recipient. And if you do good primarily to make yourself feel better, you can lose sight of whether ‘doing good’ really helps others at all. This is the danger with the generally welcome growth of ethical consumerism. Concern for the environment can easily become a form of eco-narcissism, where the main priority is to keep ourselves pure. But how altruistic is it to eat locally grown organic produce while refusing to spend money on food from countries that need our trade? How entitled are you to feel good about fitting a few low-energy light bulbs if you fly even once a year? And might your refusal to eat gene-modified foods be hampering a technology that could make feeding the world’s poor easier? The purpose of charity is to make life easier for everyone. Sacrificing some of your own self-interests for a substantial benefit to others is certainly laudable, and something too few of us are willing to do. But if we can help others while also benefiting ourselves, that is even better. When charity really can be a win-win deal, we would be foolish not to take it. Julian Baggini, Psychologies Magazine",
        "id": 37,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What is said in this text with regard to the decline in infant mortality?",
        "options": [
            "It would have been less pronounced without the MDGs.",
            "The effect of the MDGs has not yet been clearly established. ",
            "The MDGs initiated the positive trend in the early 21st century.",
            "Only a minor role appears to have been played by the MDGs."
        ],
        "article": "Infant Mortality One of the happiest trends of the first years of this century has been a dramatic decline in infant mortality. In 2013, approximately 6.3m children under the age of five died around the world, down from 9.7m in 2000. Achieving a two-thirds decline from 1990 mortality rates (when 12.7m children aged five or under died) was one of the UN’s Millennium Development Goals (MDGs), a set of eight development targets for 2015 approved at a UN summit in 2000. Working out the extent to which the MDGs were responsible for this decline has become a big part of the debate over whether they were a success or not. The main difficulty is to guess what would have happened had the MDGs not been set; during the 1990s, the number of child deaths was already falling steeply",
        "id": 38,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What is said about king penguins?",
        "options": [
            "Their orange color is thought to function as a visual mating signal.",
            "They make use of color markings that cannot be perceived by humans. ",
            "They show greater within-species color variation than other species do.",
            "Their orange patches gradually fade after they have found a mating partner."
        ],
        "article": "King Penguins Tall in stature and long of beak, king penguins bear striking color patches along their necks, ear regions, and the sides of their beaks. In the visible light spectrum, these beak patches appear orange. Penguins are able to see into the ultraviolet region of the electromagnetic spectrum, however, and can detect additional patches of ultraviolet color along the lower bill that are imperceptible to human eyes. Scientists hypothesize that these beak markings signal sexual maturity and make penguins more attractive to mates. As a king penguin matures, the ultraviolet hue of the beak markings increases. In wild penguin studies, scientists have documented higher ultraviolet reflectance in recently formed male-female pairs. ",
        "id": 39,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What is claimed about the act of crying?",
        "options": [
            "Very little is known about why crying is a uniquely human activity.",
            "Tears of joy have a different emotional origin than tears of sadness.",
            "The characteristics of crying shift in predictable ways throughout life.",
            "Whether or not people cry is often determined by cultural expectations."
        ],
        "article": "Crying Crying encompasses two very different processes: vocal wailing and tearing. Human babies excel at the former, and for good reason – bawling is a very effective way of getting attention from caregivers. Around adolescence, we begin to cry less over physical pain and more over emotional pain. Many people also start to exhibit “moral crying\", in reaction to acts of bravery, self-sacrifice and altruism. Why we do this is still a mystery. Also mysterious is why, as we age, we increasingly shed tears over things that are positive. One theory is that tears of joy do not actually reflect happiness at all; events such as weddings and holidays are often bittersweet because they remind us of the passage of time and mortality. This may be why children usually do not cry out of happiness: they don’t yet make the associations with sacrifice, loss and impermanence.",
        "id": 40,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What can be concluded about Billie Holiday in the opening paragraph?",
        "options": [
            "Her lasting fame in jazz circles remains a mystery to this day.",
            "Her special way of singing sets her apart from other female performers.",
            "Her technical limitations were obviously a mat­ ter of pride to her.",
            "Her abilities as a jazz singer are severely ques­ tioned by Szwed. "
        ],
        "article": "Lady Sings the Blues A review of Billie Holiday: The Musician & the Myth by John Szwed A recording from the 1950s captures Billie Holiday in conversation with some fellow musicians at a rehearsal. ‘I’m telling you,’ she says, ‘me and my old voice, it just go up a little bit and come down a little bit. It’s not legit.’ And she was right. With a vocal range barely stretching beyond an octave, she really could only go up and down ‘a little bit’, and she wouldn’t have lasted a day at any of the conservatories that trained aspiring singers in the ‘legit’ techniques of classical or operatic performance. Since Holiday was incapable of the vocal gymnastics displayed by the likes of Sarah Vaughan and Ella Fitzgerald, her place in the first rank of jazz singers surely rests on other qualities. It’s in drawing those out that John Szwed’s book is at its most valuable. The familiar view has it that what imbued Holiday’s singing with such power and profundity, despite the obvious limitations of her voice, was a kind of personal authenticity. What Szwed calls the ‘myth’ of Billie Holiday – the pastiche of tragedy that built up around her during the course of her career and that has been recycled ever since her death in 1959, at the age of forty-four – makes the very real pain of her often tortured existence the key to understanding her art. She lived the emotions she sang of, it has been claimed, and her years of addiction, abuse and exploitation by lovers, record companies and the police infuse her singing with a raw honesty that speaks to us from the depth of her soul. The impulse to attribute her music’s power to its autobiographical veracity has been too much for most of her life’s chroniclers on page and screen to resist. Szwed, however, has the courage and imagination to question whether ‘real suffering’ is ‘necessary for great singing’ – even for great singing about suffering. A skilful singer can ‘simulate emotions by a catch in the throat, upper-register pyrotechnics, or vibrato’, he writes, and must be able to communicate a panoply of emotions by developing a ‘dramatic sensibility’. Singing, in other words, involves acting. And even if, as seems indisputable, Holiday did bring tremendous emotional intensity to bear on her singing, emotion does not automatically translate into great music. What can achieve that translation is a combination of taste and technique, two qualities that, Szwed argues, Holiday possessed in abundance. To think of Holiday as having ‘technique’ already takes us beyond the formula of technical limitations plus emotional sincerity that has governed so much discussion of her music, and Szwed pays Holiday a high compliment by focusing less on her authenticity than on her artistry. Her powers of rhythmic and melodic invention were legendary among the musicians she played with. Her spontaneous adaptations of well-known tunes, often necessitated by her need to compress a song’s range, were themselves improvisations of the highest standard, sometimes producing entirely new melodies. Then there was her way with rhythm. ‘Lagging’, or playing behind the beat, was already a common device of jazz musicians when Holiday began recording in the 1930s, but the exceptionally pronounced delay she often used took her early accompanists by surprise. No less innovative was her approach to accenting and phrasing lyrics, often placing the stress in unexpected places and, Szwed suggests, adding an almost abstract dimension to words by employing them not only in their representational sense but also, through accentuation and modification, as pure sound that conveyed its own meanings. Emotion was indeed a hallmark of her singing, but it was through these sharply honed techniques and imaginative leaps that emotion was wrought into beautiful music. This is a curious book, though. Billed as a biography on its dust jacket, in reality it’s anything but. Szwed feels obliged to tackle the ‘myth’ of Holiday in the first, and shorter, of the book’s two sections, but these chapters have a rather ragbag character, with little sense of narrative progression. It’s not at all clear why we’re given a vignette about the party antics of Woolworth heir Jimmy Donahue, or potted biographies of various other peripheral figures, when we’re told scarcely the basics about Holiday’s early years. It’s only when Szwed moves on to the ‘musician’ that the book comes into its own – though even then, what fascinates him seems to be less the musician than the voice. Daniel Matlin, Literary Review",
        "id": 36,
        "answer": "B",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "Which of the following statements is in agreement with Szwed’s views?",
        "options": [
            "Billie Holiday’s life had more than its fair share of misery. ",
            "The impact of Billie Holiday’s artistry derives from the life she led.",
            "The Billie Holiday legend bears little resemblance to her sad life.",
            "Billie Holiday’s musical success was based on fiction rather than fact."
        ],
        "article": "Lady Sings the Blues A review of Billie Holiday: The Musician & the Myth by John Szwed A recording from the 1950s captures Billie Holiday in conversation with some fellow musicians at a rehearsal. ‘I’m telling you,’ she says, ‘me and my old voice, it just go up a little bit and come down a little bit. It’s not legit.’ And she was right. With a vocal range barely stretching beyond an octave, she really could only go up and down ‘a little bit’, and she wouldn’t have lasted a day at any of the conservatories that trained aspiring singers in the ‘legit’ techniques of classical or operatic performance. Since Holiday was incapable of the vocal gymnastics displayed by the likes of Sarah Vaughan and Ella Fitzgerald, her place in the first rank of jazz singers surely rests on other qualities. It’s in drawing those out that John Szwed’s book is at its most valuable. The familiar view has it that what imbued Holiday’s singing with such power and profundity, despite the obvious limitations of her voice, was a kind of personal authenticity. What Szwed calls the ‘myth’ of Billie Holiday – the pastiche of tragedy that built up around her during the course of her career and that has been recycled ever since her death in 1959, at the age of forty-four – makes the very real pain of her often tortured existence the key to understanding her art. She lived the emotions she sang of, it has been claimed, and her years of addiction, abuse and exploitation by lovers, record companies and the police infuse her singing with a raw honesty that speaks to us from the depth of her soul. The impulse to attribute her music’s power to its autobiographical veracity has been too much for most of her life’s chroniclers on page and screen to resist. Szwed, however, has the courage and imagination to question whether ‘real suffering’ is ‘necessary for great singing’ – even for great singing about suffering. A skilful singer can ‘simulate emotions by a catch in the throat, upper-register pyrotechnics, or vibrato’, he writes, and must be able to communicate a panoply of emotions by developing a ‘dramatic sensibility’. Singing, in other words, involves acting. And even if, as seems indisputable, Holiday did bring tremendous emotional intensity to bear on her singing, emotion does not automatically translate into great music. What can achieve that translation is a combination of taste and technique, two qualities that, Szwed argues, Holiday possessed in abundance. To think of Holiday as having ‘technique’ already takes us beyond the formula of technical limitations plus emotional sincerity that has governed so much discussion of her music, and Szwed pays Holiday a high compliment by focusing less on her authenticity than on her artistry. Her powers of rhythmic and melodic invention were legendary among the musicians she played with. Her spontaneous adaptations of well-known tunes, often necessitated by her need to compress a song’s range, were themselves improvisations of the highest standard, sometimes producing entirely new melodies. Then there was her way with rhythm. ‘Lagging’, or playing behind the beat, was already a common device of jazz musicians when Holiday began recording in the 1930s, but the exceptionally pronounced delay she often used took her early accompanists by surprise. No less innovative was her approach to accenting and phrasing lyrics, often placing the stress in unexpected places and, Szwed suggests, adding an almost abstract dimension to words by employing them not only in their representational sense but also, through accentuation and modification, as pure sound that conveyed its own meanings. Emotion was indeed a hallmark of her singing, but it was through these sharply honed techniques and imaginative leaps that emotion was wrought into beautiful music. This is a curious book, though. Billed as a biography on its dust jacket, in reality it’s anything but. Szwed feels obliged to tackle the ‘myth’ of Holiday in the first, and shorter, of the book’s two sections, but these chapters have a rather ragbag character, with little sense of narrative progression. It’s not at all clear why we’re given a vignette about the party antics of Woolworth heir Jimmy Donahue, or potted biographies of various other peripheral figures, when we’re told scarcely the basics about Holiday’s early years. It’s only when Szwed moves on to the ‘musician’ that the book comes into its own – though even then, what fascinates him seems to be less the musician than the voice. Daniel Matlin, Literary Review",
        "id": 37,
        "answer": "A",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What is John Szwed’s opinion concerning Billie Holiday’s music?",
        "options": [
            "Its heartbreaking moods were mostly due to mere playacting.",
            "Her way of singing had its roots in her relations to other people.",
            "The personal qualities of her voice have often been overstated. ",
            "There need not be a clear link between her life and her singing. "
        ],
        "article": "Lady Sings the Blues A review of Billie Holiday: The Musician & the Myth by John Szwed A recording from the 1950s captures Billie Holiday in conversation with some fellow musicians at a rehearsal. ‘I’m telling you,’ she says, ‘me and my old voice, it just go up a little bit and come down a little bit. It’s not legit.’ And she was right. With a vocal range barely stretching beyond an octave, she really could only go up and down ‘a little bit’, and she wouldn’t have lasted a day at any of the conservatories that trained aspiring singers in the ‘legit’ techniques of classical or operatic performance. Since Holiday was incapable of the vocal gymnastics displayed by the likes of Sarah Vaughan and Ella Fitzgerald, her place in the first rank of jazz singers surely rests on other qualities. It’s in drawing those out that John Szwed’s book is at its most valuable. The familiar view has it that what imbued Holiday’s singing with such power and profundity, despite the obvious limitations of her voice, was a kind of personal authenticity. What Szwed calls the ‘myth’ of Billie Holiday – the pastiche of tragedy that built up around her during the course of her career and that has been recycled ever since her death in 1959, at the age of forty-four – makes the very real pain of her often tortured existence the key to understanding her art. She lived the emotions she sang of, it has been claimed, and her years of addiction, abuse and exploitation by lovers, record companies and the police infuse her singing with a raw honesty that speaks to us from the depth of her soul. The impulse to attribute her music’s power to its autobiographical veracity has been too much for most of her life’s chroniclers on page and screen to resist. Szwed, however, has the courage and imagination to question whether ‘real suffering’ is ‘necessary for great singing’ – even for great singing about suffering. A skilful singer can ‘simulate emotions by a catch in the throat, upper-register pyrotechnics, or vibrato’, he writes, and must be able to communicate a panoply of emotions by developing a ‘dramatic sensibility’. Singing, in other words, involves acting. And even if, as seems indisputable, Holiday did bring tremendous emotional intensity to bear on her singing, emotion does not automatically translate into great music. What can achieve that translation is a combination of taste and technique, two qualities that, Szwed argues, Holiday possessed in abundance. To think of Holiday as having ‘technique’ already takes us beyond the formula of technical limitations plus emotional sincerity that has governed so much discussion of her music, and Szwed pays Holiday a high compliment by focusing less on her authenticity than on her artistry. Her powers of rhythmic and melodic invention were legendary among the musicians she played with. Her spontaneous adaptations of well-known tunes, often necessitated by her need to compress a song’s range, were themselves improvisations of the highest standard, sometimes producing entirely new melodies. Then there was her way with rhythm. ‘Lagging’, or playing behind the beat, was already a common device of jazz musicians when Holiday began recording in the 1930s, but the exceptionally pronounced delay she often used took her early accompanists by surprise. No less innovative was her approach to accenting and phrasing lyrics, often placing the stress in unexpected places and, Szwed suggests, adding an almost abstract dimension to words by employing them not only in their representational sense but also, through accentuation and modification, as pure sound that conveyed its own meanings. Emotion was indeed a hallmark of her singing, but it was through these sharply honed techniques and imaginative leaps that emotion was wrought into beautiful music. This is a curious book, though. Billed as a biography on its dust jacket, in reality it’s anything but. Szwed feels obliged to tackle the ‘myth’ of Holiday in the first, and shorter, of the book’s two sections, but these chapters have a rather ragbag character, with little sense of narrative progression. It’s not at all clear why we’re given a vignette about the party antics of Woolworth heir Jimmy Donahue, or potted biographies of various other peripheral figures, when we’re told scarcely the basics about Holiday’s early years. It’s only when Szwed moves on to the ‘musician’ that the book comes into its own – though even then, what fascinates him seems to be less the musician than the voice. Daniel Matlin, Literary Review",
        "id": 38,
        "answer": "D",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What are we told in connection with Billie Holiday’s treatment of lyrics?",
        "options": [
            "The words mattered much less to her than the music.",
            "She used to add her own words to the songs she sang.",
            "She tended to pronounce words in unconven­ tional ways. ",
            "It was often hard to hear what words she was singing."
        ],
        "article": "Lady Sings the Blues A review of Billie Holiday: The Musician & the Myth by John Szwed A recording from the 1950s captures Billie Holiday in conversation with some fellow musicians at a rehearsal. ‘I’m telling you,’ she says, ‘me and my old voice, it just go up a little bit and come down a little bit. It’s not legit.’ And she was right. With a vocal range barely stretching beyond an octave, she really could only go up and down ‘a little bit’, and she wouldn’t have lasted a day at any of the conservatories that trained aspiring singers in the ‘legit’ techniques of classical or operatic performance. Since Holiday was incapable of the vocal gymnastics displayed by the likes of Sarah Vaughan and Ella Fitzgerald, her place in the first rank of jazz singers surely rests on other qualities. It’s in drawing those out that John Szwed’s book is at its most valuable. The familiar view has it that what imbued Holiday’s singing with such power and profundity, despite the obvious limitations of her voice, was a kind of personal authenticity. What Szwed calls the ‘myth’ of Billie Holiday – the pastiche of tragedy that built up around her during the course of her career and that has been recycled ever since her death in 1959, at the age of forty-four – makes the very real pain of her often tortured existence the key to understanding her art. She lived the emotions she sang of, it has been claimed, and her years of addiction, abuse and exploitation by lovers, record companies and the police infuse her singing with a raw honesty that speaks to us from the depth of her soul. The impulse to attribute her music’s power to its autobiographical veracity has been too much for most of her life’s chroniclers on page and screen to resist. Szwed, however, has the courage and imagination to question whether ‘real suffering’ is ‘necessary for great singing’ – even for great singing about suffering. A skilful singer can ‘simulate emotions by a catch in the throat, upper-register pyrotechnics, or vibrato’, he writes, and must be able to communicate a panoply of emotions by developing a ‘dramatic sensibility’. Singing, in other words, involves acting. And even if, as seems indisputable, Holiday did bring tremendous emotional intensity to bear on her singing, emotion does not automatically translate into great music. What can achieve that translation is a combination of taste and technique, two qualities that, Szwed argues, Holiday possessed in abundance. To think of Holiday as having ‘technique’ already takes us beyond the formula of technical limitations plus emotional sincerity that has governed so much discussion of her music, and Szwed pays Holiday a high compliment by focusing less on her authenticity than on her artistry. Her powers of rhythmic and melodic invention were legendary among the musicians she played with. Her spontaneous adaptations of well-known tunes, often necessitated by her need to compress a song’s range, were themselves improvisations of the highest standard, sometimes producing entirely new melodies. Then there was her way with rhythm. ‘Lagging’, or playing behind the beat, was already a common device of jazz musicians when Holiday began recording in the 1930s, but the exceptionally pronounced delay she often used took her early accompanists by surprise. No less innovative was her approach to accenting and phrasing lyrics, often placing the stress in unexpected places and, Szwed suggests, adding an almost abstract dimension to words by employing them not only in their representational sense but also, through accentuation and modification, as pure sound that conveyed its own meanings. Emotion was indeed a hallmark of her singing, but it was through these sharply honed techniques and imaginative leaps that emotion was wrought into beautiful music. This is a curious book, though. Billed as a biography on its dust jacket, in reality it’s anything but. Szwed feels obliged to tackle the ‘myth’ of Holiday in the first, and shorter, of the book’s two sections, but these chapters have a rather ragbag character, with little sense of narrative progression. It’s not at all clear why we’re given a vignette about the party antics of Woolworth heir Jimmy Donahue, or potted biographies of various other peripheral figures, when we’re told scarcely the basics about Holiday’s early years. It’s only when Szwed moves on to the ‘musician’ that the book comes into its own – though even then, what fascinates him seems to be less the musician than the voice. Daniel Matlin, Literary Review",
        "id": 39,
        "answer": "C",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "Which of the following statements is most in line with the reviewer’s reaction to John Szwed’s book?",
        "options": [
            "It is marketed to readers under a glaringly false label.",
            "It presents a number of new facts about Billie Holiday’s career.",
            "It relies too heavily on previous biographies of Billie Holiday.",
            "It is based on solid research but somewhat academic."
        ],
        "article": "Lady Sings the Blues A review of Billie Holiday: The Musician & the Myth by John Szwed A recording from the 1950s captures Billie Holiday in conversation with some fellow musicians at a rehearsal. ‘I’m telling you,’ she says, ‘me and my old voice, it just go up a little bit and come down a little bit. It’s not legit.’ And she was right. With a vocal range barely stretching beyond an octave, she really could only go up and down ‘a little bit’, and she wouldn’t have lasted a day at any of the conservatories that trained aspiring singers in the ‘legit’ techniques of classical or operatic performance. Since Holiday was incapable of the vocal gymnastics displayed by the likes of Sarah Vaughan and Ella Fitzgerald, her place in the first rank of jazz singers surely rests on other qualities. It’s in drawing those out that John Szwed’s book is at its most valuable. The familiar view has it that what imbued Holiday’s singing with such power and profundity, despite the obvious limitations of her voice, was a kind of personal authenticity. What Szwed calls the ‘myth’ of Billie Holiday – the pastiche of tragedy that built up around her during the course of her career and that has been recycled ever since her death in 1959, at the age of forty-four – makes the very real pain of her often tortured existence the key to understanding her art. She lived the emotions she sang of, it has been claimed, and her years of addiction, abuse and exploitation by lovers, record companies and the police infuse her singing with a raw honesty that speaks to us from the depth of her soul. The impulse to attribute her music’s power to its autobiographical veracity has been too much for most of her life’s chroniclers on page and screen to resist. Szwed, however, has the courage and imagination to question whether ‘real suffering’ is ‘necessary for great singing’ – even for great singing about suffering. A skilful singer can ‘simulate emotions by a catch in the throat, upper-register pyrotechnics, or vibrato’, he writes, and must be able to communicate a panoply of emotions by developing a ‘dramatic sensibility’. Singing, in other words, involves acting. And even if, as seems indisputable, Holiday did bring tremendous emotional intensity to bear on her singing, emotion does not automatically translate into great music. What can achieve that translation is a combination of taste and technique, two qualities that, Szwed argues, Holiday possessed in abundance. To think of Holiday as having ‘technique’ already takes us beyond the formula of technical limitations plus emotional sincerity that has governed so much discussion of her music, and Szwed pays Holiday a high compliment by focusing less on her authenticity than on her artistry. Her powers of rhythmic and melodic invention were legendary among the musicians she played with. Her spontaneous adaptations of well-known tunes, often necessitated by her need to compress a song’s range, were themselves improvisations of the highest standard, sometimes producing entirely new melodies. Then there was her way with rhythm. ‘Lagging’, or playing behind the beat, was already a common device of jazz musicians when Holiday began recording in the 1930s, but the exceptionally pronounced delay she often used took her early accompanists by surprise. No less innovative was her approach to accenting and phrasing lyrics, often placing the stress in unexpected places and, Szwed suggests, adding an almost abstract dimension to words by employing them not only in their representational sense but also, through accentuation and modification, as pure sound that conveyed its own meanings. Emotion was indeed a hallmark of her singing, but it was through these sharply honed techniques and imaginative leaps that emotion was wrought into beautiful music. This is a curious book, though. Billed as a biography on its dust jacket, in reality it’s anything but. Szwed feels obliged to tackle the ‘myth’ of Holiday in the first, and shorter, of the book’s two sections, but these chapters have a rather ragbag character, with little sense of narrative progression. It’s not at all clear why we’re given a vignette about the party antics of Woolworth heir Jimmy Donahue, or potted biographies of various other peripheral figures, when we’re told scarcely the basics about Holiday’s early years. It’s only when Szwed moves on to the ‘musician’ that the book comes into its own – though even then, what fascinates him seems to be less the musician than the voice. Daniel Matlin, Literary Review",
        "id": 40,
        "answer": "A",
        "type": "en_rc",
        "exam": "vt_2019"
    },
    {
        "question": "What is claimed here?",
        "options": [
            "Royal courts have generally mirrored the role of women in society.",
            "There was a time when queens were widely con­ sidered more powerful than their husbands.",
            "Royal courts have often allowed for a fair degree of equality between men and women.",
            "Many kings seem to have actively promoted equality between the sexes."
        ],
        "article": "At Court Historically, courts have tended to subvert boundaries between the sexes. Because of a European consort’s role in assuring the succession and enhancing dynastic prestige, her household and apartments could rival in size and splendour those of the monarch. Sometimes she controlled her own finances. A court was therefore the only arena where women could compete with men, on near equal terms, for power and influence. In a global, non-ideological, gender-conscious age, the appeal of court history is growing. The days when professors could claim ‘only the condition of the working-class matters’, or ‘don’t touch that royal stuff, it may damage your career’, are over.",
        "id": 31,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What may be concluded from the text?",
        "options": [
            "Courts have kept their place as an important subject for historical research.",
            "Attitudes to court history have changed a great deal in recent times. ",
            "new generation of feminist scholars have trans formed research on court history.",
            "Working­class history has much in common with the history of royalty. "
        ],
        "article": "At Court Historically, courts have tended to subvert boundaries between the sexes. Because of a European consort’s role in assuring the succession and enhancing dynastic prestige, her household and apartments could rival in size and splendour those of the monarch. Sometimes she controlled her own finances. A court was therefore the only arena where women could compete with men, on near equal terms, for power and influence. In a global, non-ideological, gender-conscious age, the appeal of court history is growing. The days when professors could claim ‘only the condition of the working-class matters’, or ‘don’t touch that royal stuff, it may damage your career’, are over.",
        "id": 32,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is meant by the statement that “the rules of social time constitute a ‘silent language’” in cultures of the world?",
        "options": [
            "They are principles transferred between cultures across language barriers. ",
            "They are much more similar between cultures than languages are. ",
            "They are unwritten laws determining accepted or preferred behavior. ",
            "They are very stable and change more slowly than languages do. "
        ],
        "article": "Clocking Cultures The study of time and society can be divided into the pragmatic and the cosmological. On the practical side, in the 1950s anthropologist Edward T. Hall, Jr., wrote that the rules of social time constitute a “silent language” for a given culture. The rules might not always be made explicit, he stated, but “they exist in the air... They are either familiar and comfortable or unfamiliar and wrong.” Most cultures around the world now have clocks and calendars, uniting the majority of the globe in the same general rhythm of time. But that doesn’t mean we all march to the same beat. Some people feel so rushed by the pace of modern life that they are fighting back with “slow food,” while in other societies, people feel little pressure to “manage” their time. “One of the beauties of studying time is that it’s a wonderful window on culture,” says Robert V. Levine, a social psychologist at California State University, Fresno. “You get answers on what cultures value and believe in. You get a really good idea of what’s important to people.” Levine and his colleagues have conducted so-called pace-of-life studies in 31 countries. In A Geography of Time, first published in 1997, Levine describes how he ranked the countries by using three measures: walking speed on urban sidewalks, how quickly postal clerks could fulfill a request for a common stamp, and the accuracy of public clocks. Kevin K. Birth, an anthropologist at Queens College, has examined time perceptions in Trinidad. Birth’s 1999 book, Any Time Is Trinidad Time: Social Meanings and Temporal Consciousness, refers to a commonly used phrase to excuse lateness. In that country, Birth observes, “if you have a meeting at 6:00 at night, people show up at 6:45 or 7:00 and say, ‘Any time is Trinidad time.’” When it comes to business, however, that loose approach to timeliness works only for the people with power. A boss can show up late and toss off “any time is Trinidad time”, but underlings are expected to be more punctual. For them, the saying goes, “time is time.” Birth adds that the tie between power and waiting time is true for many other cultures as well. The nebulous nature of time can make it difficult for anthropologists and social psychologists to study. “You can’t simply go into a society, walk up to some poor soul and say, ‘Tell me about your notions of time’,” Birth says. “People don’t really have an answer to that. You have to come up with other ways to find out.” Birth attempted to get at how Trinidadians value time by exploring how closely their society links time and money. He surveyed rural residents and found that farmers – whose days are dictated by natural events, such as sunrise – did not recognize the phrases “time is money,” “budget your time” or “time management,” even though they had satellite TV and were familiar with Western popular culture. But tailors in the same areas were aware of such notions. Birth concluded that wage work altered the tailors’ views of time. “The ideas of associating time with money are not found globally,” he says, “but are attached to your job and the people you work with.” How people deal with time on a day-to-day basis often has nothing to do with how they conceive of time as an abstract entity. “There’s often a disjunction between how a culture views the mythology of time and how people think about time in their daily lives,” Birth asserts. “We don’t think of Stephen Hawking’s theories as we go about our daily lives.” Ziauddin Sardar, a British Muslim author and critic, has written about time and Islamic cultures. Muslims “always carry the past with them,” claims Sardar, and asserts that the West has “colonized” time by spreading the expectation that life should become better as time passes: “If you colonize time, you also colonize the future. If you think of time as an arrow, of course you think of the future as progress, going in one direction. But different people may desire different futures.” Scientific American",
        "id": 33,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is said in the text regarding the expression “time is time” in Trinidad?",
        "options": [
            "It stresses the irrelevance in Trinidad of the expression “time is money”. ",
            "It is used by business leaders to make sure that meetings always begin on time. ",
            "It is used by Trinidadians to express a relaxed view of time and life in general. ",
            "It illustrates the conventions applying to people in low­status positions. "
        ],
        "article": "Clocking Cultures The study of time and society can be divided into the pragmatic and the cosmological. On the practical side, in the 1950s anthropologist Edward T. Hall, Jr., wrote that the rules of social time constitute a “silent language” for a given culture. The rules might not always be made explicit, he stated, but “they exist in the air... They are either familiar and comfortable or unfamiliar and wrong.” Most cultures around the world now have clocks and calendars, uniting the majority of the globe in the same general rhythm of time. But that doesn’t mean we all march to the same beat. Some people feel so rushed by the pace of modern life that they are fighting back with “slow food,” while in other societies, people feel little pressure to “manage” their time. “One of the beauties of studying time is that it’s a wonderful window on culture,” says Robert V. Levine, a social psychologist at California State University, Fresno. “You get answers on what cultures value and believe in. You get a really good idea of what’s important to people.” Levine and his colleagues have conducted so-called pace-of-life studies in 31 countries. In A Geography of Time, first published in 1997, Levine describes how he ranked the countries by using three measures: walking speed on urban sidewalks, how quickly postal clerks could fulfill a request for a common stamp, and the accuracy of public clocks. Kevin K. Birth, an anthropologist at Queens College, has examined time perceptions in Trinidad. Birth’s 1999 book, Any Time Is Trinidad Time: Social Meanings and Temporal Consciousness, refers to a commonly used phrase to excuse lateness. In that country, Birth observes, “if you have a meeting at 6:00 at night, people show up at 6:45 or 7:00 and say, ‘Any time is Trinidad time.’” When it comes to business, however, that loose approach to timeliness works only for the people with power. A boss can show up late and toss off “any time is Trinidad time”, but underlings are expected to be more punctual. For them, the saying goes, “time is time.” Birth adds that the tie between power and waiting time is true for many other cultures as well. The nebulous nature of time can make it difficult for anthropologists and social psychologists to study. “You can’t simply go into a society, walk up to some poor soul and say, ‘Tell me about your notions of time’,” Birth says. “People don’t really have an answer to that. You have to come up with other ways to find out.” Birth attempted to get at how Trinidadians value time by exploring how closely their society links time and money. He surveyed rural residents and found that farmers – whose days are dictated by natural events, such as sunrise – did not recognize the phrases “time is money,” “budget your time” or “time management,” even though they had satellite TV and were familiar with Western popular culture. But tailors in the same areas were aware of such notions. Birth concluded that wage work altered the tailors’ views of time. “The ideas of associating time with money are not found globally,” he says, “but are attached to your job and the people you work with.” How people deal with time on a day-to-day basis often has nothing to do with how they conceive of time as an abstract entity. “There’s often a disjunction between how a culture views the mythology of time and how people think about time in their daily lives,” Birth asserts. “We don’t think of Stephen Hawking’s theories as we go about our daily lives.” Ziauddin Sardar, a British Muslim author and critic, has written about time and Islamic cultures. Muslims “always carry the past with them,” claims Sardar, and asserts that the West has “colonized” time by spreading the expectation that life should become better as time passes: “If you colonize time, you also colonize the future. If you think of time as an arrow, of course you think of the future as progress, going in one direction. But different people may desire different futures.” Scientific American",
        "id": 34,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is argued to be problematic for researchers when asking people about their ideas of time?",
        "options": [
            "People are generally not aware of their own perceptions of time. ",
            "large number of people must be interviewed to get reliable results. ",
            "Questions relevant in one culture are often irrelevant in another. ",
            "The researchers often do not have sufficient background knowledge."
        ],
        "article": "Clocking Cultures The study of time and society can be divided into the pragmatic and the cosmological. On the practical side, in the 1950s anthropologist Edward T. Hall, Jr., wrote that the rules of social time constitute a “silent language” for a given culture. The rules might not always be made explicit, he stated, but “they exist in the air... They are either familiar and comfortable or unfamiliar and wrong.” Most cultures around the world now have clocks and calendars, uniting the majority of the globe in the same general rhythm of time. But that doesn’t mean we all march to the same beat. Some people feel so rushed by the pace of modern life that they are fighting back with “slow food,” while in other societies, people feel little pressure to “manage” their time. “One of the beauties of studying time is that it’s a wonderful window on culture,” says Robert V. Levine, a social psychologist at California State University, Fresno. “You get answers on what cultures value and believe in. You get a really good idea of what’s important to people.” Levine and his colleagues have conducted so-called pace-of-life studies in 31 countries. In A Geography of Time, first published in 1997, Levine describes how he ranked the countries by using three measures: walking speed on urban sidewalks, how quickly postal clerks could fulfill a request for a common stamp, and the accuracy of public clocks. Kevin K. Birth, an anthropologist at Queens College, has examined time perceptions in Trinidad. Birth’s 1999 book, Any Time Is Trinidad Time: Social Meanings and Temporal Consciousness, refers to a commonly used phrase to excuse lateness. In that country, Birth observes, “if you have a meeting at 6:00 at night, people show up at 6:45 or 7:00 and say, ‘Any time is Trinidad time.’” When it comes to business, however, that loose approach to timeliness works only for the people with power. A boss can show up late and toss off “any time is Trinidad time”, but underlings are expected to be more punctual. For them, the saying goes, “time is time.” Birth adds that the tie between power and waiting time is true for many other cultures as well. The nebulous nature of time can make it difficult for anthropologists and social psychologists to study. “You can’t simply go into a society, walk up to some poor soul and say, ‘Tell me about your notions of time’,” Birth says. “People don’t really have an answer to that. You have to come up with other ways to find out.” Birth attempted to get at how Trinidadians value time by exploring how closely their society links time and money. He surveyed rural residents and found that farmers – whose days are dictated by natural events, such as sunrise – did not recognize the phrases “time is money,” “budget your time” or “time management,” even though they had satellite TV and were familiar with Western popular culture. But tailors in the same areas were aware of such notions. Birth concluded that wage work altered the tailors’ views of time. “The ideas of associating time with money are not found globally,” he says, “but are attached to your job and the people you work with.” How people deal with time on a day-to-day basis often has nothing to do with how they conceive of time as an abstract entity. “There’s often a disjunction between how a culture views the mythology of time and how people think about time in their daily lives,” Birth asserts. “We don’t think of Stephen Hawking’s theories as we go about our daily lives.” Ziauddin Sardar, a British Muslim author and critic, has written about time and Islamic cultures. Muslims “always carry the past with them,” claims Sardar, and asserts that the West has “colonized” time by spreading the expectation that life should become better as time passes: “If you colonize time, you also colonize the future. If you think of time as an arrow, of course you think of the future as progress, going in one direction. But different people may desire different futures.” Scientific American",
        "id": 35,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is said in relation to the concept underlying the expression “time is money”?",
        "options": [
            "It is most evident in developed European countries and the U.S. ",
            "It is linked more to occupations than a specific culture. ",
            "It is generally not found among island people such as Trinidadians. ",
            "It is often spread to non­Western cultures through Western TV­shows. "
        ],
        "article": "Clocking Cultures The study of time and society can be divided into the pragmatic and the cosmological. On the practical side, in the 1950s anthropologist Edward T. Hall, Jr., wrote that the rules of social time constitute a “silent language” for a given culture. The rules might not always be made explicit, he stated, but “they exist in the air... They are either familiar and comfortable or unfamiliar and wrong.” Most cultures around the world now have clocks and calendars, uniting the majority of the globe in the same general rhythm of time. But that doesn’t mean we all march to the same beat. Some people feel so rushed by the pace of modern life that they are fighting back with “slow food,” while in other societies, people feel little pressure to “manage” their time. “One of the beauties of studying time is that it’s a wonderful window on culture,” says Robert V. Levine, a social psychologist at California State University, Fresno. “You get answers on what cultures value and believe in. You get a really good idea of what’s important to people.” Levine and his colleagues have conducted so-called pace-of-life studies in 31 countries. In A Geography of Time, first published in 1997, Levine describes how he ranked the countries by using three measures: walking speed on urban sidewalks, how quickly postal clerks could fulfill a request for a common stamp, and the accuracy of public clocks. Kevin K. Birth, an anthropologist at Queens College, has examined time perceptions in Trinidad. Birth’s 1999 book, Any Time Is Trinidad Time: Social Meanings and Temporal Consciousness, refers to a commonly used phrase to excuse lateness. In that country, Birth observes, “if you have a meeting at 6:00 at night, people show up at 6:45 or 7:00 and say, ‘Any time is Trinidad time.’” When it comes to business, however, that loose approach to timeliness works only for the people with power. A boss can show up late and toss off “any time is Trinidad time”, but underlings are expected to be more punctual. For them, the saying goes, “time is time.” Birth adds that the tie between power and waiting time is true for many other cultures as well. The nebulous nature of time can make it difficult for anthropologists and social psychologists to study. “You can’t simply go into a society, walk up to some poor soul and say, ‘Tell me about your notions of time’,” Birth says. “People don’t really have an answer to that. You have to come up with other ways to find out.” Birth attempted to get at how Trinidadians value time by exploring how closely their society links time and money. He surveyed rural residents and found that farmers – whose days are dictated by natural events, such as sunrise – did not recognize the phrases “time is money,” “budget your time” or “time management,” even though they had satellite TV and were familiar with Western popular culture. But tailors in the same areas were aware of such notions. Birth concluded that wage work altered the tailors’ views of time. “The ideas of associating time with money are not found globally,” he says, “but are attached to your job and the people you work with.” How people deal with time on a day-to-day basis often has nothing to do with how they conceive of time as an abstract entity. “There’s often a disjunction between how a culture views the mythology of time and how people think about time in their daily lives,” Birth asserts. “We don’t think of Stephen Hawking’s theories as we go about our daily lives.” Ziauddin Sardar, a British Muslim author and critic, has written about time and Islamic cultures. Muslims “always carry the past with them,” claims Sardar, and asserts that the West has “colonized” time by spreading the expectation that life should become better as time passes: “If you colonize time, you also colonize the future. If you think of time as an arrow, of course you think of the future as progress, going in one direction. But different people may desire different futures.” Scientific American",
        "id": 36,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "How can Ziauddin Sardar’s opinion best be summarized?",
        "options": [
            "The perception of time may affect people’s expectations of societies’ development. ",
            "The variation in how time is perceived is just as great as the number of human cultures. ",
            "The perception of time in a society directly influences the possibilities for democracy. ",
            "The linear view of time was an important contributing factor behind Western colonialism. "
        ],
        "article": "Clocking Cultures The study of time and society can be divided into the pragmatic and the cosmological. On the practical side, in the 1950s anthropologist Edward T. Hall, Jr., wrote that the rules of social time constitute a “silent language” for a given culture. The rules might not always be made explicit, he stated, but “they exist in the air... They are either familiar and comfortable or unfamiliar and wrong.” Most cultures around the world now have clocks and calendars, uniting the majority of the globe in the same general rhythm of time. But that doesn’t mean we all march to the same beat. Some people feel so rushed by the pace of modern life that they are fighting back with “slow food,” while in other societies, people feel little pressure to “manage” their time. “One of the beauties of studying time is that it’s a wonderful window on culture,” says Robert V. Levine, a social psychologist at California State University, Fresno. “You get answers on what cultures value and believe in. You get a really good idea of what’s important to people.” Levine and his colleagues have conducted so-called pace-of-life studies in 31 countries. In A Geography of Time, first published in 1997, Levine describes how he ranked the countries by using three measures: walking speed on urban sidewalks, how quickly postal clerks could fulfill a request for a common stamp, and the accuracy of public clocks. Kevin K. Birth, an anthropologist at Queens College, has examined time perceptions in Trinidad. Birth’s 1999 book, Any Time Is Trinidad Time: Social Meanings and Temporal Consciousness, refers to a commonly used phrase to excuse lateness. In that country, Birth observes, “if you have a meeting at 6:00 at night, people show up at 6:45 or 7:00 and say, ‘Any time is Trinidad time.’” When it comes to business, however, that loose approach to timeliness works only for the people with power. A boss can show up late and toss off “any time is Trinidad time”, but underlings are expected to be more punctual. For them, the saying goes, “time is time.” Birth adds that the tie between power and waiting time is true for many other cultures as well. The nebulous nature of time can make it difficult for anthropologists and social psychologists to study. “You can’t simply go into a society, walk up to some poor soul and say, ‘Tell me about your notions of time’,” Birth says. “People don’t really have an answer to that. You have to come up with other ways to find out.” Birth attempted to get at how Trinidadians value time by exploring how closely their society links time and money. He surveyed rural residents and found that farmers – whose days are dictated by natural events, such as sunrise – did not recognize the phrases “time is money,” “budget your time” or “time management,” even though they had satellite TV and were familiar with Western popular culture. But tailors in the same areas were aware of such notions. Birth concluded that wage work altered the tailors’ views of time. “The ideas of associating time with money are not found globally,” he says, “but are attached to your job and the people you work with.” How people deal with time on a day-to-day basis often has nothing to do with how they conceive of time as an abstract entity. “There’s often a disjunction between how a culture views the mythology of time and how people think about time in their daily lives,” Birth asserts. “We don’t think of Stephen Hawking’s theories as we go about our daily lives.” Ziauddin Sardar, a British Muslim author and critic, has written about time and Islamic cultures. Muslims “always carry the past with them,” claims Sardar, and asserts that the West has “colonized” time by spreading the expectation that life should become better as time passes: “If you colonize time, you also colonize the future. If you think of time as an arrow, of course you think of the future as progress, going in one direction. But different people may desire different futures.” Scientific American",
        "id": 37,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is the writer’s main point here concerning political correctness?",
        "options": [
            "It derives from the ethical principle of not offending people. ",
            "It should be treated with a certain degree of scepticism. ",
            "It has always been a controversial notion at universities in the U.S.",
            "It can only be defended where there is no freedom of speech. "
        ],
        "article": "Political Correctness The new wave of political correctness on American campuses, including “trigger warnings” to help students avoid having to read about topics that may upset them, is born, essentially, of a noble idea. Minority students, facing bullying or belittlement, argue for the need to protect themselves, to create a safe space. But in creating that space, these advocates risk walling themselves off from the unexpected, albeit sometimes ugly, reality of engaging in pitched debate with people with whom they do not see eye to eye. They are rejecting the sometimes crushing but always formative experience of discovering that you disagree, deeply and fundamentally, with a friend, and then deciding to stay friends anyway. It is a crucial experience for anyone living in a pluralistic democracy.",
        "id": 38,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is this text mainly about?",
        "options": [
            "Land reform ",
            "Slavery",
            "Ethnic cleansing",
            "Colonialism Political Correctness"
        ],
        "article": "The Berlin Conference In many parts of Africa, the atrocity inflicted upon the people by the Berlin Conference of 1884–85, which balkanised the continent, can be seen starkly in how homogeneous ethnic groups were carved up and shoved into two, three, and sometimes even more countries just by the stroke of a European pen. It is something that Africans have had to live with since the day the Berlin Conference ended and the Scramble for Africa began in earnest.",
        "id": 39,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is the main point here with regard to bonobos?",
        "options": [
            "In comparison with most other animals, their sounds have more varied meanings.",
            "They are heavily focused on avoiding conflicts and fights within the group.",
            "Just like humans, they often engage in acts of unprovoked violence.",
            "They appear to have a well­developed sense of justice."
        ],
        "article": "Bonobos He started it! Bonobos shriek when attacked. But they produce different sounds depending on whether this happens with or without provocation. This distinction adds evidence to the idea that they sense when others have treated them unfairly. A bystander sometimes intervenes, supporting or consoling the attacked ape. This could mean that bonobos also take into account the fairness of how others are treated, something so far thought to be limited to humans. When one bonobo attacks another, there is sometimes an obvious reason, such as fighting over food. On other occasions it appears unprovoked, and in those cases the scream is longer, more high-pitched and harshersounding. “We think these screams are actually eliciting interventions and help from third parties,” says researcher Zanna Clay. “It probably also inhibits the aggressor from doing it again.",
        "id": 40,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is said concerning Joseph Lintner?",
        "options": [
            "He was the first person known to make a connection between smell and attraction.",
            "He suggested how the knowledge of odorant molecules could be put to practical use. ",
            "He managed to give a rough description of the chemical properties involved in moth attraction. ",
            "He made ancient ideas concerning smell and attraction available to the broader scientific "
        ],
        "article": "Pheromones Powerful messages can be delivered by smell, a fact known long before the means by which they did so was discovered. The ancient Greeks knew that secretions from a female dog in heat were attractive to male dogs. In his beekeepers’ manual The Feminine Monarchie, published in 1623, Charles Butler warned that an injured bee’s “ranke smell” would attract other angry bees to sting. In the late 19th century, New York’s first state entomologist, Joseph Lintner, noted the crowds gathered on the sidewalk outside, looking at the spectacle of 50 wild silk moth males attracted to a virgin female moth in his office window. Lintner correctly observed that the attraction was probably a chemical, detected by the large, elaborate antennae of the males. He predicted that such an irresistible and far- reaching force could be used to control agricultural pest moths, if only chemists could identify and synthesize these powerful molecules, now known as pheromones. Lintner’s prediction has been generously fulfilled. But because the quantity of pheromone produced by each animal is so small, it was almost 70 years before the first pheromone could be chemically identified, in 1959, by the Nobel Prize-winning German chemist Adolf Butenandt and his large team. In the half century since then, as the technology for isolating and identifying trace amounts of compounds has become more refined, pheromones have been found in almost every kind of animal, in squid, lobsters, ants, fish, salamanders, and mice, to name just a few. Although pheromones are important in many species for finding mates for sex, they can have a wide variety of other functions, such as one produced by mother rabbits that prompts suckling by their pups. In many social insects, such as ants, bees, and wasps, almost every part of colony behavior is mediated by pheromones, from queen signals affecting worker reproduction within the colony to Butler’s “ranke smell” alarm pheromone that activates colony defense against enemies. Pheromones are chemical signals that have evolved for communication between members of the same species. A pheromone signal elicits a specific reaction in the receiver, for example, a stereotyped behavior (releaser effect) or a developmental process (primer effect). Some pheromones can have both effects. Most pheromones are detected by the sense of smell. However, not all smells are pheromones. Mammals, including humans, also give off a cloud of molecules that represent our unique individual “smell” or chemical profile. To definitively demonstrate that a pheromone exists, one must design a repeatable experiment, a bioassay, that shows that a smell molecule (odorant) causes a particular effect on the receiver, for example inducing a behavior. It is highly likely that humans, like other mammals, have pheromones. However, despite the many sites on the Internet offering to sell “pheromones” to make one sexually irresistible, no human pheromones have been chemically identified to date. Sadly, there is no evidence for the widely published claims that the molecules androstadienone and estratetraenol are human pheromones of any kind. Another misuse of the term pheromone is in so-called pheromone parties, a trend started by an artist in Brooklyn in 2010. At these events, singles sniff numbered T-shirts that have been worn by others at the party to see if they like the smell and photograph themselves with the shirt in a numbered bag to facilitate meeting up later. Although these get-togethers might sound like fun, they are misnamed, because the smells on the shirts are unique to each individual, not species-wide pheromones that trigger the same behavior in every person. The idea behind these parties is based on the surprising observation that mice preferred to mate with mice that were immunologically different from themselves in the major histocompatibility complex (MHC), a difference that can be detected by smell. When a similar experiment was tried with Swiss students sniffing T-shirts, it was discovered that women similarly found the smell of men immunologically different from themselves more attractive. However, subsequent replications of the experiment have not shown the effect consistently and, disappointingly, studies of large-scale human genome data have been equally inconclusive. Clearly, the idea of choosing a potential partner by smell has popular appeal even though the evidence is not there yet. The biggest challenge for work on human pheromones will be the identification of behaviors and physiological responses that will make robust bioassays. Because of our cultural complexity and diversity, influences on human behaviors are notoriously difficult to study. American Scientist",
        "id": 36,
        "answer": "B",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is claimed about pheromones in nature?",
        "options": [
            "By far their most common use is for finding a mating partner.",
            "True examples of them have both a releaser effect and a primer effect.",
            "Insect communities are often strongly dependent on them. ",
            "Most of their functions are still relatively poorly understood."
        ],
        "article": "Pheromones Powerful messages can be delivered by smell, a fact known long before the means by which they did so was discovered. The ancient Greeks knew that secretions from a female dog in heat were attractive to male dogs. In his beekeepers’ manual The Feminine Monarchie, published in 1623, Charles Butler warned that an injured bee’s “ranke smell” would attract other angry bees to sting. In the late 19th century, New York’s first state entomologist, Joseph Lintner, noted the crowds gathered on the sidewalk outside, looking at the spectacle of 50 wild silk moth males attracted to a virgin female moth in his office window. Lintner correctly observed that the attraction was probably a chemical, detected by the large, elaborate antennae of the males. He predicted that such an irresistible and far- reaching force could be used to control agricultural pest moths, if only chemists could identify and synthesize these powerful molecules, now known as pheromones. Lintner’s prediction has been generously fulfilled. But because the quantity of pheromone produced by each animal is so small, it was almost 70 years before the first pheromone could be chemically identified, in 1959, by the Nobel Prize-winning German chemist Adolf Butenandt and his large team. In the half century since then, as the technology for isolating and identifying trace amounts of compounds has become more refined, pheromones have been found in almost every kind of animal, in squid, lobsters, ants, fish, salamanders, and mice, to name just a few. Although pheromones are important in many species for finding mates for sex, they can have a wide variety of other functions, such as one produced by mother rabbits that prompts suckling by their pups. In many social insects, such as ants, bees, and wasps, almost every part of colony behavior is mediated by pheromones, from queen signals affecting worker reproduction within the colony to Butler’s “ranke smell” alarm pheromone that activates colony defense against enemies. Pheromones are chemical signals that have evolved for communication between members of the same species. A pheromone signal elicits a specific reaction in the receiver, for example, a stereotyped behavior (releaser effect) or a developmental process (primer effect). Some pheromones can have both effects. Most pheromones are detected by the sense of smell. However, not all smells are pheromones. Mammals, including humans, also give off a cloud of molecules that represent our unique individual “smell” or chemical profile. To definitively demonstrate that a pheromone exists, one must design a repeatable experiment, a bioassay, that shows that a smell molecule (odorant) causes a particular effect on the receiver, for example inducing a behavior. It is highly likely that humans, like other mammals, have pheromones. However, despite the many sites on the Internet offering to sell “pheromones” to make one sexually irresistible, no human pheromones have been chemically identified to date. Sadly, there is no evidence for the widely published claims that the molecules androstadienone and estratetraenol are human pheromones of any kind. Another misuse of the term pheromone is in so-called pheromone parties, a trend started by an artist in Brooklyn in 2010. At these events, singles sniff numbered T-shirts that have been worn by others at the party to see if they like the smell and photograph themselves with the shirt in a numbered bag to facilitate meeting up later. Although these get-togethers might sound like fun, they are misnamed, because the smells on the shirts are unique to each individual, not species-wide pheromones that trigger the same behavior in every person. The idea behind these parties is based on the surprising observation that mice preferred to mate with mice that were immunologically different from themselves in the major histocompatibility complex (MHC), a difference that can be detected by smell. When a similar experiment was tried with Swiss students sniffing T-shirts, it was discovered that women similarly found the smell of men immunologically different from themselves more attractive. However, subsequent replications of the experiment have not shown the effect consistently and, disappointingly, studies of large-scale human genome data have been equally inconclusive. Clearly, the idea of choosing a potential partner by smell has popular appeal even though the evidence is not there yet. The biggest challenge for work on human pheromones will be the identification of behaviors and physiological responses that will make robust bioassays. Because of our cultural complexity and diversity, influences on human behaviors are notoriously difficult to study. American Scientist",
        "id": 37,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What is argued about the molecules androstadienone and estratetraenol?",
        "options": [
            "They are the most likely examples of human pheromones.",
            "They are some of the strongest pheromones found in nature. ",
            "They are pheromones in some mammals but not in humans.",
            "They are chemicals which do not seem to be pheromones. "
        ],
        "article": "Pheromones Powerful messages can be delivered by smell, a fact known long before the means by which they did so was discovered. The ancient Greeks knew that secretions from a female dog in heat were attractive to male dogs. In his beekeepers’ manual The Feminine Monarchie, published in 1623, Charles Butler warned that an injured bee’s “ranke smell” would attract other angry bees to sting. In the late 19th century, New York’s first state entomologist, Joseph Lintner, noted the crowds gathered on the sidewalk outside, looking at the spectacle of 50 wild silk moth males attracted to a virgin female moth in his office window. Lintner correctly observed that the attraction was probably a chemical, detected by the large, elaborate antennae of the males. He predicted that such an irresistible and far- reaching force could be used to control agricultural pest moths, if only chemists could identify and synthesize these powerful molecules, now known as pheromones. Lintner’s prediction has been generously fulfilled. But because the quantity of pheromone produced by each animal is so small, it was almost 70 years before the first pheromone could be chemically identified, in 1959, by the Nobel Prize-winning German chemist Adolf Butenandt and his large team. In the half century since then, as the technology for isolating and identifying trace amounts of compounds has become more refined, pheromones have been found in almost every kind of animal, in squid, lobsters, ants, fish, salamanders, and mice, to name just a few. Although pheromones are important in many species for finding mates for sex, they can have a wide variety of other functions, such as one produced by mother rabbits that prompts suckling by their pups. In many social insects, such as ants, bees, and wasps, almost every part of colony behavior is mediated by pheromones, from queen signals affecting worker reproduction within the colony to Butler’s “ranke smell” alarm pheromone that activates colony defense against enemies. Pheromones are chemical signals that have evolved for communication between members of the same species. A pheromone signal elicits a specific reaction in the receiver, for example, a stereotyped behavior (releaser effect) or a developmental process (primer effect). Some pheromones can have both effects. Most pheromones are detected by the sense of smell. However, not all smells are pheromones. Mammals, including humans, also give off a cloud of molecules that represent our unique individual “smell” or chemical profile. To definitively demonstrate that a pheromone exists, one must design a repeatable experiment, a bioassay, that shows that a smell molecule (odorant) causes a particular effect on the receiver, for example inducing a behavior. It is highly likely that humans, like other mammals, have pheromones. However, despite the many sites on the Internet offering to sell “pheromones” to make one sexually irresistible, no human pheromones have been chemically identified to date. Sadly, there is no evidence for the widely published claims that the molecules androstadienone and estratetraenol are human pheromones of any kind. Another misuse of the term pheromone is in so-called pheromone parties, a trend started by an artist in Brooklyn in 2010. At these events, singles sniff numbered T-shirts that have been worn by others at the party to see if they like the smell and photograph themselves with the shirt in a numbered bag to facilitate meeting up later. Although these get-togethers might sound like fun, they are misnamed, because the smells on the shirts are unique to each individual, not species-wide pheromones that trigger the same behavior in every person. The idea behind these parties is based on the surprising observation that mice preferred to mate with mice that were immunologically different from themselves in the major histocompatibility complex (MHC), a difference that can be detected by smell. When a similar experiment was tried with Swiss students sniffing T-shirts, it was discovered that women similarly found the smell of men immunologically different from themselves more attractive. However, subsequent replications of the experiment have not shown the effect consistently and, disappointingly, studies of large-scale human genome data have been equally inconclusive. Clearly, the idea of choosing a potential partner by smell has popular appeal even though the evidence is not there yet. The biggest challenge for work on human pheromones will be the identification of behaviors and physiological responses that will make robust bioassays. Because of our cultural complexity and diversity, influences on human behaviors are notoriously difficult to study. American Scientist",
        "id": 38,
        "answer": "D",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What can be concluded from the studies of the “major histocompatibility complex” (MHC) in mice and humans?",
        "options": [
            "The findings indicate that mice and humans might have a pheromone in common.",
            "What seemed to work for mice clearly does not work with humans.",
            "It is possible that smell-based attraction works for both mice and humans. ",
            "The evidence shows some indications of a human pheromone."
        ],
        "article": "Pheromones Powerful messages can be delivered by smell, a fact known long before the means by which they did so was discovered. The ancient Greeks knew that secretions from a female dog in heat were attractive to male dogs. In his beekeepers’ manual The Feminine Monarchie, published in 1623, Charles Butler warned that an injured bee’s “ranke smell” would attract other angry bees to sting. In the late 19th century, New York’s first state entomologist, Joseph Lintner, noted the crowds gathered on the sidewalk outside, looking at the spectacle of 50 wild silk moth males attracted to a virgin female moth in his office window. Lintner correctly observed that the attraction was probably a chemical, detected by the large, elaborate antennae of the males. He predicted that such an irresistible and far- reaching force could be used to control agricultural pest moths, if only chemists could identify and synthesize these powerful molecules, now known as pheromones. Lintner’s prediction has been generously fulfilled. But because the quantity of pheromone produced by each animal is so small, it was almost 70 years before the first pheromone could be chemically identified, in 1959, by the Nobel Prize-winning German chemist Adolf Butenandt and his large team. In the half century since then, as the technology for isolating and identifying trace amounts of compounds has become more refined, pheromones have been found in almost every kind of animal, in squid, lobsters, ants, fish, salamanders, and mice, to name just a few. Although pheromones are important in many species for finding mates for sex, they can have a wide variety of other functions, such as one produced by mother rabbits that prompts suckling by their pups. In many social insects, such as ants, bees, and wasps, almost every part of colony behavior is mediated by pheromones, from queen signals affecting worker reproduction within the colony to Butler’s “ranke smell” alarm pheromone that activates colony defense against enemies. Pheromones are chemical signals that have evolved for communication between members of the same species. A pheromone signal elicits a specific reaction in the receiver, for example, a stereotyped behavior (releaser effect) or a developmental process (primer effect). Some pheromones can have both effects. Most pheromones are detected by the sense of smell. However, not all smells are pheromones. Mammals, including humans, also give off a cloud of molecules that represent our unique individual “smell” or chemical profile. To definitively demonstrate that a pheromone exists, one must design a repeatable experiment, a bioassay, that shows that a smell molecule (odorant) causes a particular effect on the receiver, for example inducing a behavior. It is highly likely that humans, like other mammals, have pheromones. However, despite the many sites on the Internet offering to sell “pheromones” to make one sexually irresistible, no human pheromones have been chemically identified to date. Sadly, there is no evidence for the widely published claims that the molecules androstadienone and estratetraenol are human pheromones of any kind. Another misuse of the term pheromone is in so-called pheromone parties, a trend started by an artist in Brooklyn in 2010. At these events, singles sniff numbered T-shirts that have been worn by others at the party to see if they like the smell and photograph themselves with the shirt in a numbered bag to facilitate meeting up later. Although these get-togethers might sound like fun, they are misnamed, because the smells on the shirts are unique to each individual, not species-wide pheromones that trigger the same behavior in every person. The idea behind these parties is based on the surprising observation that mice preferred to mate with mice that were immunologically different from themselves in the major histocompatibility complex (MHC), a difference that can be detected by smell. When a similar experiment was tried with Swiss students sniffing T-shirts, it was discovered that women similarly found the smell of men immunologically different from themselves more attractive. However, subsequent replications of the experiment have not shown the effect consistently and, disappointingly, studies of large-scale human genome data have been equally inconclusive. Clearly, the idea of choosing a potential partner by smell has popular appeal even though the evidence is not there yet. The biggest challenge for work on human pheromones will be the identification of behaviors and physiological responses that will make robust bioassays. Because of our cultural complexity and diversity, influences on human behaviors are notoriously difficult to study. American Scientist",
        "id": 39,
        "answer": "C",
        "type": "en_rc",
        "exam": "ht_2018"
    },
    {
        "question": "What, according to the text, causes the relative difficulty of identifying human pheromones?",
        "options": [
            "The very nature of human societies makes pheromone-based behaviors hard to detect.",
            "Human pheromones tend to be chemically more complex than those of other species.",
            "Most of the more obvious human pheromones have already been discovered.",
            "Humans are likely to release fewer pheromones than most other species."
        ],
        "article": "Pheromones Powerful messages can be delivered by smell, a fact known long before the means by which they did so was discovered. The ancient Greeks knew that secretions from a female dog in heat were attractive to male dogs. In his beekeepers’ manual The Feminine Monarchie, published in 1623, Charles Butler warned that an injured bee’s “ranke smell” would attract other angry bees to sting. In the late 19th century, New York’s first state entomologist, Joseph Lintner, noted the crowds gathered on the sidewalk outside, looking at the spectacle of 50 wild silk moth males attracted to a virgin female moth in his office window. Lintner correctly observed that the attraction was probably a chemical, detected by the large, elaborate antennae of the males. He predicted that such an irresistible and far- reaching force could be used to control agricultural pest moths, if only chemists could identify and synthesize these powerful molecules, now known as pheromones. Lintner’s prediction has been generously fulfilled. But because the quantity of pheromone produced by each animal is so small, it was almost 70 years before the first pheromone could be chemically identified, in 1959, by the Nobel Prize-winning German chemist Adolf Butenandt and his large team. In the half century since then, as the technology for isolating and identifying trace amounts of compounds has become more refined, pheromones have been found in almost every kind of animal, in squid, lobsters, ants, fish, salamanders, and mice, to name just a few. Although pheromones are important in many species for finding mates for sex, they can have a wide variety of other functions, such as one produced by mother rabbits that prompts suckling by their pups. In many social insects, such as ants, bees, and wasps, almost every part of colony behavior is mediated by pheromones, from queen signals affecting worker reproduction within the colony to Butler’s “ranke smell” alarm pheromone that activates colony defense against enemies. Pheromones are chemical signals that have evolved for communication between members of the same species. A pheromone signal elicits a specific reaction in the receiver, for example, a stereotyped behavior (releaser effect) or a developmental process (primer effect). Some pheromones can have both effects. Most pheromones are detected by the sense of smell. However, not all smells are pheromones. Mammals, including humans, also give off a cloud of molecules that represent our unique individual “smell” or chemical profile. To definitively demonstrate that a pheromone exists, one must design a repeatable experiment, a bioassay, that shows that a smell molecule (odorant) causes a particular effect on the receiver, for example inducing a behavior. It is highly likely that humans, like other mammals, have pheromones. However, despite the many sites on the Internet offering to sell “pheromones” to make one sexually irresistible, no human pheromones have been chemically identified to date. Sadly, there is no evidence for the widely published claims that the molecules androstadienone and estratetraenol are human pheromones of any kind. Another misuse of the term pheromone is in so-called pheromone parties, a trend started by an artist in Brooklyn in 2010. At these events, singles sniff numbered T-shirts that have been worn by others at the party to see if they like the smell and photograph themselves with the shirt in a numbered bag to facilitate meeting up later. Although these get-togethers might sound like fun, they are misnamed, because the smells on the shirts are unique to each individual, not species-wide pheromones that trigger the same behavior in every person. The idea behind these parties is based on the surprising observation that mice preferred to mate with mice that were immunologically different from themselves in the major histocompatibility complex (MHC), a difference that can be detected by smell. When a similar experiment was tried with Swiss students sniffing T-shirts, it was discovered that women similarly found the smell of men immunologically different from themselves more attractive. However, subsequent replications of the experiment have not shown the effect consistently and, disappointingly, studies of large-scale human genome data have been equally inconclusive. Clearly, the idea of choosing a potential partner by smell has popular appeal even though the evidence is not there yet. The biggest challenge for work on human pheromones will be the identification of behaviors and physiological responses that will make robust bioassays. Because of our cultural complexity and diversity, influences on human behaviors are notoriously difficult to study. American Scientist",
        "id": 40,
        "answer": "A",
        "type": "en_rc",
        "exam": "ht_2018"
    }
]